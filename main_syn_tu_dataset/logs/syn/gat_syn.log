step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.1
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GAT
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5597
| Tree: House:140  , Cycle:1260 , Grids:1260 , Diams:1260  
| BA  : House:1260 , Cycle:139  , Grids:139  , Diams:139   
| All : House:1400 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:10.0%, Cycle:90.1%, Grids:90.1%, Diams:90.1%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:797
| Tree: House:20   , Cycle:180  , Grids:180  , Diams:180   
| BA  : House:180  , Cycle:19   , Grids:19   , Diams:19    
| All : House:200  , Cycle:199  , Grids:199  , Diams:199   
| BIAS: House:10.0%, Cycle:90.5%, Grids:90.5%, Diams:90.5%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.10] | Model:[GAT] Epoch:[1/100] Loss:[0.5989] Train:[79.29] val:[52.20] Test:[41.56] | Best Val:[52.20] Update Test:[41.56] at Epoch:[1] | lr:0.002000
BIAS:[0.10] | Model:[GAT] Epoch:[2/100] Loss:[0.3224] Train:[89.44] val:[91.72] Test:[69.50] | Best Val:[91.72] Update Test:[69.50] at Epoch:[2] | lr:0.001998
BIAS:[0.10] | Model:[GAT] Epoch:[3/100] Loss:[0.2844] Train:[90.12] val:[90.97] Test:[72.75] | Best Val:[91.72] Update Test:[69.50] at Epoch:[2] | lr:0.001996
BIAS:[0.10] | Model:[GAT] Epoch:[4/100] Loss:[0.2654] Train:[90.82] val:[92.85] Test:[73.56] | Best Val:[92.85] Update Test:[73.56] at Epoch:[4] | lr:0.001993
BIAS:[0.10] | Model:[GAT] Epoch:[5/100] Loss:[0.2317] Train:[92.19] val:[92.22] Test:[75.38] | Best Val:[92.85] Update Test:[73.56] at Epoch:[4] | lr:0.001988
BIAS:[0.10] | Model:[GAT] Epoch:[6/100] Loss:[0.2220] Train:[92.75] val:[92.47] Test:[75.31] | Best Val:[92.85] Update Test:[73.56] at Epoch:[4] | lr:0.001983
BIAS:[0.10] | Model:[GAT] Epoch:[7/100] Loss:[0.2190] Train:[92.71] val:[94.73] Test:[78.12] | Best Val:[94.73] Update Test:[78.12] at Epoch:[7] | lr:0.001977
BIAS:[0.10] | Model:[GAT] Epoch:[8/100] Loss:[0.2136] Train:[92.84] val:[93.48] Test:[74.38] | Best Val:[94.73] Update Test:[78.12] at Epoch:[7] | lr:0.001970
BIAS:[0.10] | Model:[GAT] Epoch:[9/100] Loss:[0.2131] Train:[92.69] val:[93.60] Test:[77.88] | Best Val:[94.73] Update Test:[78.12] at Epoch:[7] | lr:0.001962
BIAS:[0.10] | Model:[GAT] Epoch:[10/100] Loss:[0.2029] Train:[93.12] val:[94.35] Test:[74.25] | Best Val:[94.73] Update Test:[78.12] at Epoch:[7] | lr:0.001954
BIAS:[0.10] | Model:[GAT] Epoch:[11/100] Loss:[0.1974] Train:[93.35] val:[93.85] Test:[73.50] | Best Val:[94.73] Update Test:[78.12] at Epoch:[7] | lr:0.001944
BIAS:[0.10] | Model:[GAT] Epoch:[12/100] Loss:[0.1862] Train:[94.16] val:[92.97] Test:[82.19] | Best Val:[94.73] Update Test:[78.12] at Epoch:[7] | lr:0.001933
BIAS:[0.10] | Model:[GAT] Epoch:[13/100] Loss:[0.1885] Train:[94.09] val:[94.48] Test:[76.56] | Best Val:[94.73] Update Test:[78.12] at Epoch:[7] | lr:0.001922
BIAS:[0.10] | Model:[GAT] Epoch:[14/100] Loss:[0.1963] Train:[93.59] val:[95.61] Test:[83.00] | Best Val:[95.61] Update Test:[83.00] at Epoch:[14] | lr:0.001910
BIAS:[0.10] | Model:[GAT] Epoch:[15/100] Loss:[0.1842] Train:[93.89] val:[94.86] Test:[78.81] | Best Val:[95.61] Update Test:[83.00] at Epoch:[14] | lr:0.001896
BIAS:[0.10] | Model:[GAT] Epoch:[16/100] Loss:[0.1950] Train:[93.66] val:[93.48] Test:[78.50] | Best Val:[95.61] Update Test:[83.00] at Epoch:[14] | lr:0.001882
BIAS:[0.10] | Model:[GAT] Epoch:[17/100] Loss:[0.1883] Train:[94.12] val:[94.98] Test:[79.69] | Best Val:[95.61] Update Test:[83.00] at Epoch:[14] | lr:0.001868
BIAS:[0.10] | Model:[GAT] Epoch:[18/100] Loss:[0.1827] Train:[94.16] val:[95.36] Test:[81.06] | Best Val:[95.61] Update Test:[83.00] at Epoch:[14] | lr:0.001852
BIAS:[0.10] | Model:[GAT] Epoch:[19/100] Loss:[0.1754] Train:[93.96] val:[94.23] Test:[81.38] | Best Val:[95.61] Update Test:[83.00] at Epoch:[14] | lr:0.001836
BIAS:[0.10] | Model:[GAT] Epoch:[20/100] Loss:[0.1768] Train:[94.21] val:[94.86] Test:[82.44] | Best Val:[95.61] Update Test:[83.00] at Epoch:[14] | lr:0.001819
BIAS:[0.10] | Model:[GAT] Epoch:[21/100] Loss:[0.1644] Train:[94.30] val:[94.10] Test:[76.62] | Best Val:[95.61] Update Test:[83.00] at Epoch:[14] | lr:0.001801
BIAS:[0.10] | Model:[GAT] Epoch:[22/100] Loss:[0.1849] Train:[94.03] val:[95.73] Test:[82.56] | Best Val:[95.73] Update Test:[82.56] at Epoch:[22] | lr:0.001782
BIAS:[0.10] | Model:[GAT] Epoch:[23/100] Loss:[0.1689] Train:[94.39] val:[95.23] Test:[81.19] | Best Val:[95.73] Update Test:[82.56] at Epoch:[22] | lr:0.001763
BIAS:[0.10] | Model:[GAT] Epoch:[24/100] Loss:[0.1660] Train:[94.28] val:[94.86] Test:[78.62] | Best Val:[95.73] Update Test:[82.56] at Epoch:[22] | lr:0.001743
BIAS:[0.10] | Model:[GAT] Epoch:[25/100] Loss:[0.1618] Train:[94.73] val:[94.98] Test:[80.56] | Best Val:[95.73] Update Test:[82.56] at Epoch:[22] | lr:0.001722
BIAS:[0.10] | Model:[GAT] Epoch:[26/100] Loss:[0.1699] Train:[94.43] val:[93.98] Test:[79.88] | Best Val:[95.73] Update Test:[82.56] at Epoch:[22] | lr:0.001700
BIAS:[0.10] | Model:[GAT] Epoch:[27/100] Loss:[0.1655] Train:[94.44] val:[94.86] Test:[81.50] | Best Val:[95.73] Update Test:[82.56] at Epoch:[22] | lr:0.001678
BIAS:[0.10] | Model:[GAT] Epoch:[28/100] Loss:[0.1588] Train:[95.02] val:[94.98] Test:[82.75] | Best Val:[95.73] Update Test:[82.56] at Epoch:[22] | lr:0.001656
BIAS:[0.10] | Model:[GAT] Epoch:[29/100] Loss:[0.1541] Train:[94.96] val:[95.86] Test:[79.12] | Best Val:[95.86] Update Test:[79.12] at Epoch:[29] | lr:0.001632
BIAS:[0.10] | Model:[GAT] Epoch:[30/100] Loss:[0.1629] Train:[94.35] val:[95.48] Test:[83.75] | Best Val:[95.86] Update Test:[79.12] at Epoch:[29] | lr:0.001608
BIAS:[0.10] | Model:[GAT] Epoch:[31/100] Loss:[0.1650] Train:[94.57] val:[95.11] Test:[80.19] | Best Val:[95.86] Update Test:[79.12] at Epoch:[29] | lr:0.001584
BIAS:[0.10] | Model:[GAT] Epoch:[32/100] Loss:[0.1434] Train:[95.03] val:[95.61] Test:[79.19] | Best Val:[95.86] Update Test:[79.12] at Epoch:[29] | lr:0.001559
BIAS:[0.10] | Model:[GAT] Epoch:[33/100] Loss:[0.1523] Train:[94.87] val:[95.48] Test:[81.00] | Best Val:[95.86] Update Test:[79.12] at Epoch:[29] | lr:0.001534
BIAS:[0.10] | Model:[GAT] Epoch:[34/100] Loss:[0.1444] Train:[95.59] val:[96.24] Test:[84.25] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001508
BIAS:[0.10] | Model:[GAT] Epoch:[35/100] Loss:[0.1354] Train:[95.62] val:[95.48] Test:[81.75] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001481
BIAS:[0.10] | Model:[GAT] Epoch:[36/100] Loss:[0.1456] Train:[95.23] val:[95.48] Test:[82.81] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001454
BIAS:[0.10] | Model:[GAT] Epoch:[37/100] Loss:[0.1488] Train:[95.02] val:[95.61] Test:[81.44] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001427
BIAS:[0.10] | Model:[GAT] Epoch:[38/100] Loss:[0.1401] Train:[94.98] val:[96.11] Test:[83.12] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001400
BIAS:[0.10] | Model:[GAT] Epoch:[39/100] Loss:[0.1431] Train:[95.14] val:[95.86] Test:[84.19] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001372
BIAS:[0.10] | Model:[GAT] Epoch:[40/100] Loss:[0.1427] Train:[94.96] val:[95.86] Test:[81.81] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001344
BIAS:[0.10] | Model:[GAT] Epoch:[41/100] Loss:[0.1294] Train:[95.53] val:[95.86] Test:[84.06] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001315
BIAS:[0.10] | Model:[GAT] Epoch:[42/100] Loss:[0.1329] Train:[95.62] val:[95.73] Test:[79.50] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001286
BIAS:[0.10] | Model:[GAT] Epoch:[43/100] Loss:[0.1309] Train:[95.50] val:[95.61] Test:[82.12] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001257
BIAS:[0.10] | Model:[GAT] Epoch:[44/100] Loss:[0.1340] Train:[95.46] val:[95.23] Test:[81.81] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001228
BIAS:[0.10] | Model:[GAT] Epoch:[45/100] Loss:[0.1353] Train:[95.68] val:[96.24] Test:[84.38] | Best Val:[96.24] Update Test:[84.25] at Epoch:[34] | lr:0.001199
BIAS:[0.10] | Model:[GAT] Epoch:[46/100] Loss:[0.1349] Train:[95.48] val:[96.36] Test:[82.88] | Best Val:[96.36] Update Test:[82.88] at Epoch:[46] | lr:0.001169
BIAS:[0.10] | Model:[GAT] Epoch:[47/100] Loss:[0.1256] Train:[95.77] val:[95.86] Test:[83.56] | Best Val:[96.36] Update Test:[82.88] at Epoch:[46] | lr:0.001139
BIAS:[0.10] | Model:[GAT] Epoch:[48/100] Loss:[0.1298] Train:[95.68] val:[95.98] Test:[82.06] | Best Val:[96.36] Update Test:[82.88] at Epoch:[46] | lr:0.001110
BIAS:[0.10] | Model:[GAT] Epoch:[49/100] Loss:[0.1260] Train:[95.73] val:[95.61] Test:[83.12] | Best Val:[96.36] Update Test:[82.88] at Epoch:[46] | lr:0.001080
BIAS:[0.10] | Model:[GAT] Epoch:[50/100] Loss:[0.1219] Train:[95.84] val:[96.49] Test:[84.31] | Best Val:[96.49] Update Test:[84.31] at Epoch:[50] | lr:0.001050
BIAS:[0.10] | Model:[GAT] Epoch:[51/100] Loss:[0.1345] Train:[95.57] val:[96.36] Test:[83.62] | Best Val:[96.49] Update Test:[84.31] at Epoch:[50] | lr:0.001020
BIAS:[0.10] | Model:[GAT] Epoch:[52/100] Loss:[0.1299] Train:[95.55] val:[96.36] Test:[83.75] | Best Val:[96.49] Update Test:[84.31] at Epoch:[50] | lr:0.000990
BIAS:[0.10] | Model:[GAT] Epoch:[53/100] Loss:[0.1271] Train:[95.62] val:[95.36] Test:[84.00] | Best Val:[96.49] Update Test:[84.31] at Epoch:[50] | lr:0.000961
BIAS:[0.10] | Model:[GAT] Epoch:[54/100] Loss:[0.1244] Train:[95.80] val:[96.61] Test:[84.62] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000931
BIAS:[0.10] | Model:[GAT] Epoch:[55/100] Loss:[0.1243] Train:[95.69] val:[96.24] Test:[84.88] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000901
BIAS:[0.10] | Model:[GAT] Epoch:[56/100] Loss:[0.1173] Train:[95.89] val:[95.98] Test:[83.50] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000872
BIAS:[0.10] | Model:[GAT] Epoch:[57/100] Loss:[0.1202] Train:[95.80] val:[95.73] Test:[84.56] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000843
BIAS:[0.10] | Model:[GAT] Epoch:[58/100] Loss:[0.1122] Train:[96.30] val:[95.86] Test:[84.75] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000814
BIAS:[0.10] | Model:[GAT] Epoch:[59/100] Loss:[0.1189] Train:[96.18] val:[96.36] Test:[85.12] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000785
BIAS:[0.10] | Model:[GAT] Epoch:[60/100] Loss:[0.1072] Train:[96.11] val:[96.11] Test:[83.56] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000756
BIAS:[0.10] | Model:[GAT] Epoch:[61/100] Loss:[0.1045] Train:[96.34] val:[96.36] Test:[86.75] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000728
BIAS:[0.10] | Model:[GAT] Epoch:[62/100] Loss:[0.1101] Train:[96.14] val:[96.36] Test:[82.44] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000700
BIAS:[0.10] | Model:[GAT] Epoch:[63/100] Loss:[0.1061] Train:[96.14] val:[96.36] Test:[83.88] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000673
BIAS:[0.10] | Model:[GAT] Epoch:[64/100] Loss:[0.1076] Train:[96.44] val:[96.36] Test:[84.19] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000646
BIAS:[0.10] | Model:[GAT] Epoch:[65/100] Loss:[0.1047] Train:[96.52] val:[95.98] Test:[83.38] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000619
BIAS:[0.10] | Model:[GAT] Epoch:[66/100] Loss:[0.1067] Train:[96.27] val:[96.36] Test:[83.50] | Best Val:[96.61] Update Test:[84.62] at Epoch:[54] | lr:0.000592
BIAS:[0.10] | Model:[GAT] Epoch:[67/100] Loss:[0.1010] Train:[96.41] val:[96.86] Test:[84.38] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000566
BIAS:[0.10] | Model:[GAT] Epoch:[68/100] Loss:[0.1049] Train:[96.46] val:[96.24] Test:[83.81] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000541
BIAS:[0.10] | Model:[GAT] Epoch:[69/100] Loss:[0.1025] Train:[96.44] val:[96.49] Test:[84.62] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000516
BIAS:[0.10] | Model:[GAT] Epoch:[70/100] Loss:[0.1012] Train:[96.55] val:[96.24] Test:[83.38] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000492
BIAS:[0.10] | Model:[GAT] Epoch:[71/100] Loss:[0.0973] Train:[96.80] val:[96.61] Test:[85.12] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000468
BIAS:[0.10] | Model:[GAT] Epoch:[72/100] Loss:[0.1030] Train:[96.28] val:[96.49] Test:[84.75] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000444
BIAS:[0.10] | Model:[GAT] Epoch:[73/100] Loss:[0.0946] Train:[96.84] val:[96.61] Test:[84.94] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000422
BIAS:[0.10] | Model:[GAT] Epoch:[74/100] Loss:[0.0919] Train:[96.75] val:[96.61] Test:[83.94] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000400
BIAS:[0.10] | Model:[GAT] Epoch:[75/100] Loss:[0.0993] Train:[96.61] val:[96.74] Test:[85.31] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000378
BIAS:[0.10] | Model:[GAT] Epoch:[76/100] Loss:[0.0924] Train:[96.77] val:[96.36] Test:[83.81] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000357
BIAS:[0.10] | Model:[GAT] Epoch:[77/100] Loss:[0.0881] Train:[96.94] val:[96.24] Test:[84.75] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000337
BIAS:[0.10] | Model:[GAT] Epoch:[78/100] Loss:[0.1003] Train:[96.62] val:[96.36] Test:[85.62] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000318
BIAS:[0.10] | Model:[GAT] Epoch:[79/100] Loss:[0.0963] Train:[96.69] val:[96.49] Test:[84.69] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000299
BIAS:[0.10] | Model:[GAT] Epoch:[80/100] Loss:[0.0857] Train:[96.94] val:[96.61] Test:[85.44] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000281
BIAS:[0.10] | Model:[GAT] Epoch:[81/100] Loss:[0.0921] Train:[96.73] val:[96.86] Test:[84.75] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000264
BIAS:[0.10] | Model:[GAT] Epoch:[82/100] Loss:[0.0815] Train:[97.18] val:[96.61] Test:[84.75] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000248
BIAS:[0.10] | Model:[GAT] Epoch:[83/100] Loss:[0.0904] Train:[97.09] val:[96.36] Test:[83.75] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000232
BIAS:[0.10] | Model:[GAT] Epoch:[84/100] Loss:[0.0903] Train:[96.80] val:[96.61] Test:[84.81] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000218
BIAS:[0.10] | Model:[GAT] Epoch:[85/100] Loss:[0.0869] Train:[96.96] val:[96.74] Test:[85.12] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000204
BIAS:[0.10] | Model:[GAT] Epoch:[86/100] Loss:[0.0914] Train:[96.89] val:[96.74] Test:[85.06] | Best Val:[96.86] Update Test:[84.38] at Epoch:[67] | lr:0.000190
BIAS:[0.10] | Model:[GAT] Epoch:[87/100] Loss:[0.0826] Train:[97.16] val:[97.11] Test:[85.50] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000178
BIAS:[0.10] | Model:[GAT] Epoch:[88/100] Loss:[0.0884] Train:[97.03] val:[96.86] Test:[85.06] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000167
BIAS:[0.10] | Model:[GAT] Epoch:[89/100] Loss:[0.0813] Train:[97.25] val:[96.61] Test:[85.06] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000156
BIAS:[0.10] | Model:[GAT] Epoch:[90/100] Loss:[0.0786] Train:[97.07] val:[96.86] Test:[84.88] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000146
BIAS:[0.10] | Model:[GAT] Epoch:[91/100] Loss:[0.0827] Train:[97.19] val:[96.99] Test:[85.12] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000138
BIAS:[0.10] | Model:[GAT] Epoch:[92/100] Loss:[0.0809] Train:[97.28] val:[96.61] Test:[85.19] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000130
BIAS:[0.10] | Model:[GAT] Epoch:[93/100] Loss:[0.0802] Train:[97.12] val:[96.86] Test:[85.31] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000123
BIAS:[0.10] | Model:[GAT] Epoch:[94/100] Loss:[0.0853] Train:[97.00] val:[96.74] Test:[84.31] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000117
BIAS:[0.10] | Model:[GAT] Epoch:[95/100] Loss:[0.0789] Train:[97.28] val:[96.74] Test:[84.94] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000112
BIAS:[0.10] | Model:[GAT] Epoch:[96/100] Loss:[0.0850] Train:[97.12] val:[96.36] Test:[84.12] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000107
BIAS:[0.10] | Model:[GAT] Epoch:[97/100] Loss:[0.0766] Train:[97.53] val:[96.74] Test:[84.81] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000104
BIAS:[0.10] | Model:[GAT] Epoch:[98/100] Loss:[0.0809] Train:[97.36] val:[96.49] Test:[84.69] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000102
BIAS:[0.10] | Model:[GAT] Epoch:[99/100] Loss:[0.0767] Train:[97.28] val:[96.74] Test:[85.19] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000100
BIAS:[0.10] | Model:[GAT] Epoch:[100/100] Loss:[0.0803] Train:[97.41] val:[96.74] Test:[85.06] | Best Val:[97.11] Update Test:[85.50] at Epoch:[87] | lr:0.000100
syd: BIAS:[0.10] | Best Val acc:[97.11] Test acc:[85.50] at epoch:[87]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.1
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGAT
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5597
| Tree: House:140  , Cycle:1260 , Grids:1260 , Diams:1260  
| BA  : House:1260 , Cycle:139  , Grids:139  , Diams:139   
| All : House:1400 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:10.0%, Cycle:90.1%, Grids:90.1%, Diams:90.1%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:797
| Tree: House:20   , Cycle:180  , Grids:180  , Diams:180   
| BA  : House:180  , Cycle:19   , Grids:19   , Diams:19    
| All : House:200  , Cycle:199  , Grids:199  , Diams:199   
| BIAS: House:10.0%, Cycle:90.5%, Grids:90.5%, Diams:90.5%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.10] | Model:[CausalGAT] Epoch:[1/100] Loss:[0.8190=0.0187+0.5151+0.5891] Train:[81.95] val:[89.34] Test:[68.19] | Update Test:[co:64.75,c:24.19,o:68.19] at Epoch:[1] | lr:0.002000
BIAS:[0.10] | Model:[CausalGAT] Epoch:[2/100] Loss:[0.4173=0.0012+0.2703+0.2928] Train:[91.05] val:[91.22] Test:[76.62] | Update Test:[co:70.19,c:25.00,o:76.62] at Epoch:[2] | lr:0.001998
BIAS:[0.10] | Model:[CausalGAT] Epoch:[3/100] Loss:[0.3392=0.0006+0.2211+0.2357] Train:[92.84] val:[91.34] Test:[75.00] | Update Test:[co:76.81,c:25.37,o:75.00] at Epoch:[3] | lr:0.001996
BIAS:[0.10] | Model:[CausalGAT] Epoch:[4/100] Loss:[0.3317=0.0006+0.2161+0.2307] Train:[92.66] val:[92.35] Test:[78.75] | Update Test:[co:80.88,c:25.50,o:78.75] at Epoch:[4] | lr:0.001993
BIAS:[0.10] | Model:[CausalGAT] Epoch:[5/100] Loss:[0.3025=0.0005+0.1962+0.2122] Train:[93.44] val:[94.60] Test:[81.62] | Update Test:[co:78.25,c:24.44,o:81.62] at Epoch:[5] | lr:0.001988
BIAS:[0.10] | Model:[CausalGAT] Epoch:[6/100] Loss:[0.2977=0.0003+0.1937+0.2076] Train:[93.84] val:[90.59] Test:[77.19] | Update Test:[co:78.25,c:24.44,o:81.62] at Epoch:[5] | lr:0.001983
BIAS:[0.10] | Model:[CausalGAT] Epoch:[7/100] Loss:[0.2743=0.0004+0.1794+0.1895] Train:[94.14] val:[94.86] Test:[80.69] | Update Test:[co:75.31,c:25.12,o:80.69] at Epoch:[7] | lr:0.001977
BIAS:[0.10] | Model:[CausalGAT] Epoch:[8/100] Loss:[0.2679=0.0002+0.1756+0.1843] Train:[94.32] val:[95.36] Test:[84.50] | Update Test:[co:82.75,c:25.62,o:84.50] at Epoch:[8] | lr:0.001970
BIAS:[0.10] | Model:[CausalGAT] Epoch:[9/100] Loss:[0.2513=0.0002+0.1647+0.1729] Train:[94.66] val:[95.73] Test:[80.62] | Update Test:[co:79.75,c:27.00,o:80.62] at Epoch:[9] | lr:0.001962
BIAS:[0.10] | Model:[CausalGAT] Epoch:[10/100] Loss:[0.2451=0.0002+0.1594+0.1712] Train:[94.96] val:[95.23] Test:[80.88] | Update Test:[co:79.75,c:27.00,o:80.62] at Epoch:[9] | lr:0.001954
BIAS:[0.10] | Model:[CausalGAT] Epoch:[11/100] Loss:[0.2418=0.0002+0.1591+0.1652] Train:[94.62] val:[94.98] Test:[78.62] | Update Test:[co:79.75,c:27.00,o:80.62] at Epoch:[9] | lr:0.001944
BIAS:[0.10] | Model:[CausalGAT] Epoch:[12/100] Loss:[0.2473=0.0002+0.1622+0.1699] Train:[94.46] val:[95.23] Test:[81.75] | Update Test:[co:79.75,c:27.00,o:80.62] at Epoch:[9] | lr:0.001933
BIAS:[0.10] | Model:[CausalGAT] Epoch:[13/100] Loss:[0.2506=0.0002+0.1654+0.1703] Train:[94.50] val:[96.36] Test:[83.81] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001922
BIAS:[0.10] | Model:[CausalGAT] Epoch:[14/100] Loss:[0.2533=0.0002+0.1654+0.1755] Train:[94.50] val:[95.11] Test:[80.81] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001910
BIAS:[0.10] | Model:[CausalGAT] Epoch:[15/100] Loss:[0.2409=0.0001+0.1585+0.1647] Train:[94.71] val:[94.73] Test:[82.38] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001896
BIAS:[0.10] | Model:[CausalGAT] Epoch:[16/100] Loss:[0.2287=0.0002+0.1509+0.1555] Train:[94.93] val:[93.85] Test:[85.94] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001882
BIAS:[0.10] | Model:[CausalGAT] Epoch:[17/100] Loss:[0.2400=0.0002+0.1576+0.1648] Train:[94.84] val:[94.60] Test:[81.06] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001868
BIAS:[0.10] | Model:[CausalGAT] Epoch:[18/100] Loss:[0.2257=0.0002+0.1477+0.1557] Train:[95.03] val:[95.86] Test:[85.00] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001852
BIAS:[0.10] | Model:[CausalGAT] Epoch:[19/100] Loss:[0.2112=0.0001+0.1388+0.1448] Train:[95.41] val:[94.86] Test:[80.38] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001836
BIAS:[0.10] | Model:[CausalGAT] Epoch:[20/100] Loss:[0.2275=0.0001+0.1493+0.1562] Train:[95.09] val:[92.72] Test:[85.19] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001819
BIAS:[0.10] | Model:[CausalGAT] Epoch:[21/100] Loss:[0.2090=0.0001+0.1377+0.1424] Train:[95.25] val:[94.73] Test:[88.00] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001801
BIAS:[0.10] | Model:[CausalGAT] Epoch:[22/100] Loss:[0.2202=0.0001+0.1455+0.1494] Train:[95.14] val:[96.11] Test:[85.19] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001782
BIAS:[0.10] | Model:[CausalGAT] Epoch:[23/100] Loss:[0.1958=0.0001+0.1287+0.1341] Train:[95.50] val:[93.60] Test:[86.81] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001763
BIAS:[0.10] | Model:[CausalGAT] Epoch:[24/100] Loss:[0.2074=0.0002+0.1374+0.1399] Train:[95.46] val:[95.23] Test:[85.81] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001743
BIAS:[0.10] | Model:[CausalGAT] Epoch:[25/100] Loss:[0.2008=0.0002+0.1325+0.1364] Train:[95.52] val:[95.36] Test:[85.19] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001722
BIAS:[0.10] | Model:[CausalGAT] Epoch:[26/100] Loss:[0.1873=0.0001+0.1237+0.1271] Train:[96.11] val:[95.73] Test:[83.19] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001700
BIAS:[0.10] | Model:[CausalGAT] Epoch:[27/100] Loss:[0.1939=0.0002+0.1270+0.1336] Train:[95.50] val:[96.11] Test:[86.19] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001678
BIAS:[0.10] | Model:[CausalGAT] Epoch:[28/100] Loss:[0.1793=0.0001+0.1186+0.1214] Train:[96.03] val:[93.60] Test:[83.88] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001656
BIAS:[0.10] | Model:[CausalGAT] Epoch:[29/100] Loss:[0.1718=0.0001+0.1128+0.1179] Train:[96.12] val:[95.11] Test:[86.12] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001632
BIAS:[0.10] | Model:[CausalGAT] Epoch:[30/100] Loss:[0.1713=0.0001+0.1126+0.1173] Train:[96.23] val:[96.11] Test:[88.62] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001608
BIAS:[0.10] | Model:[CausalGAT] Epoch:[31/100] Loss:[0.1816=0.0001+0.1194+0.1242] Train:[95.93] val:[94.86] Test:[85.94] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001584
BIAS:[0.10] | Model:[CausalGAT] Epoch:[32/100] Loss:[0.1710=0.0001+0.1124+0.1173] Train:[96.03] val:[95.61] Test:[87.56] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001559
BIAS:[0.10] | Model:[CausalGAT] Epoch:[33/100] Loss:[0.1732=0.0001+0.1139+0.1185] Train:[96.12] val:[95.23] Test:[84.50] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001534
BIAS:[0.10] | Model:[CausalGAT] Epoch:[34/100] Loss:[0.1707=0.0001+0.1118+0.1177] Train:[95.94] val:[94.98] Test:[88.88] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001508
BIAS:[0.10] | Model:[CausalGAT] Epoch:[35/100] Loss:[0.1547=0.0001+0.1019+0.1055] Train:[96.71] val:[93.60] Test:[88.75] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001481
BIAS:[0.10] | Model:[CausalGAT] Epoch:[36/100] Loss:[0.1543=0.0001+0.1022+0.1041] Train:[96.46] val:[91.34] Test:[87.56] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001454
BIAS:[0.10] | Model:[CausalGAT] Epoch:[37/100] Loss:[0.1636=0.0001+0.1069+0.1133] Train:[96.27] val:[95.61] Test:[89.00] | Update Test:[co:80.00,c:24.94,o:83.81] at Epoch:[13] | lr:0.001427
BIAS:[0.10] | Model:[CausalGAT] Epoch:[38/100] Loss:[0.1659=0.0001+0.1074+0.1169] Train:[95.96] val:[96.61] Test:[87.88] | Update Test:[co:87.19,c:28.88,o:87.88] at Epoch:[38] | lr:0.001400
BIAS:[0.10] | Model:[CausalGAT] Epoch:[39/100] Loss:[0.1621=0.0001+0.1081+0.1079] Train:[96.25] val:[95.98] Test:[90.06] | Update Test:[co:87.19,c:28.88,o:87.88] at Epoch:[38] | lr:0.001372
BIAS:[0.10] | Model:[CausalGAT] Epoch:[40/100] Loss:[0.1453=0.0001+0.0958+0.0989] Train:[96.73] val:[95.86] Test:[90.75] | Update Test:[co:87.19,c:28.88,o:87.88] at Epoch:[38] | lr:0.001344
BIAS:[0.10] | Model:[CausalGAT] Epoch:[41/100] Loss:[0.1379=0.0001+0.0903+0.0952] Train:[97.11] val:[96.86] Test:[87.88] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001315
BIAS:[0.10] | Model:[CausalGAT] Epoch:[42/100] Loss:[0.1410=0.0001+0.0929+0.0962] Train:[97.00] val:[96.74] Test:[91.69] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001286
BIAS:[0.10] | Model:[CausalGAT] Epoch:[43/100] Loss:[0.1473=0.0001+0.0967+0.1011] Train:[96.71] val:[95.73] Test:[88.62] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001257
BIAS:[0.10] | Model:[CausalGAT] Epoch:[44/100] Loss:[0.1342=0.0001+0.0873+0.0938] Train:[97.18] val:[91.22] Test:[88.25] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001228
BIAS:[0.10] | Model:[CausalGAT] Epoch:[45/100] Loss:[0.1350=0.0001+0.0882+0.0936] Train:[96.84] val:[92.22] Test:[88.62] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001199
BIAS:[0.10] | Model:[CausalGAT] Epoch:[46/100] Loss:[0.1327=0.0001+0.0865+0.0923] Train:[96.86] val:[95.98] Test:[88.19] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001169
BIAS:[0.10] | Model:[CausalGAT] Epoch:[47/100] Loss:[0.1181=0.0000+0.0771+0.0820] Train:[97.03] val:[94.86] Test:[90.19] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001139
BIAS:[0.10] | Model:[CausalGAT] Epoch:[48/100] Loss:[0.1323=0.0001+0.0865+0.0916] Train:[97.02] val:[76.16] Test:[80.44] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001110
BIAS:[0.10] | Model:[CausalGAT] Epoch:[49/100] Loss:[0.1244=0.0001+0.0822+0.0844] Train:[97.09] val:[95.73] Test:[82.38] | Update Test:[co:88.31,c:27.00,o:87.88] at Epoch:[41] | lr:0.001080
BIAS:[0.10] | Model:[CausalGAT] Epoch:[50/100] Loss:[0.1220=0.0000+0.0795+0.0850] Train:[97.45] val:[98.24] Test:[93.06] | Update Test:[co:92.56,c:25.31,o:93.06] at Epoch:[50] | lr:0.001050
BIAS:[0.10] | Model:[CausalGAT] Epoch:[51/100] Loss:[0.1058=0.0000+0.0697+0.0721] Train:[97.16] val:[59.72] Test:[63.38] | Update Test:[co:92.56,c:25.31,o:93.06] at Epoch:[50] | lr:0.001020
BIAS:[0.10] | Model:[CausalGAT] Epoch:[52/100] Loss:[0.1105=0.0000+0.0717+0.0777] Train:[97.59] val:[96.36] Test:[90.50] | Update Test:[co:92.56,c:25.31,o:93.06] at Epoch:[50] | lr:0.000990
BIAS:[0.10] | Model:[CausalGAT] Epoch:[53/100] Loss:[0.1171=0.0001+0.0767+0.0807] Train:[97.23] val:[83.31] Test:[80.44] | Update Test:[co:92.56,c:25.31,o:93.06] at Epoch:[50] | lr:0.000961
BIAS:[0.10] | Model:[CausalGAT] Epoch:[54/100] Loss:[0.1025=0.0000+0.0670+0.0710] Train:[97.75] val:[97.11] Test:[91.94] | Update Test:[co:92.56,c:25.31,o:93.06] at Epoch:[50] | lr:0.000931
BIAS:[0.10] | Model:[CausalGAT] Epoch:[55/100] Loss:[0.0929=0.0001+0.0609+0.0639] Train:[97.84] val:[94.60] Test:[75.31] | Update Test:[co:92.56,c:25.31,o:93.06] at Epoch:[50] | lr:0.000901
BIAS:[0.10] | Model:[CausalGAT] Epoch:[56/100] Loss:[0.0910=0.0001+0.0590+0.0639] Train:[97.80] val:[96.86] Test:[89.06] | Update Test:[co:92.56,c:25.31,o:93.06] at Epoch:[50] | lr:0.000872
BIAS:[0.10] | Model:[CausalGAT] Epoch:[57/100] Loss:[0.0952=0.0000+0.0621+0.0663] Train:[97.80] val:[98.37] Test:[92.75] | Update Test:[co:92.94,c:25.50,o:92.75] at Epoch:[57] | lr:0.000843
BIAS:[0.10] | Model:[CausalGAT] Epoch:[58/100] Loss:[0.0821=0.0000+0.0537+0.0567] Train:[98.11] val:[98.49] Test:[92.88] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000814
BIAS:[0.10] | Model:[CausalGAT] Epoch:[59/100] Loss:[0.0892=0.0001+0.0569+0.0646] Train:[97.91] val:[94.23] Test:[90.38] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000785
BIAS:[0.10] | Model:[CausalGAT] Epoch:[60/100] Loss:[0.0876=0.0000+0.0560+0.0631] Train:[98.03] val:[83.44] Test:[84.25] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000756
BIAS:[0.10] | Model:[CausalGAT] Epoch:[61/100] Loss:[0.0806=0.0000+0.0526+0.0560] Train:[98.21] val:[97.24] Test:[88.19] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000728
BIAS:[0.10] | Model:[CausalGAT] Epoch:[62/100] Loss:[0.0782=0.0000+0.0510+0.0545] Train:[97.98] val:[97.87] Test:[90.69] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000700
BIAS:[0.10] | Model:[CausalGAT] Epoch:[63/100] Loss:[0.0693=0.0000+0.0451+0.0482] Train:[98.36] val:[96.61] Test:[87.88] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000673
BIAS:[0.10] | Model:[CausalGAT] Epoch:[64/100] Loss:[0.0730=0.0000+0.0476+0.0508] Train:[98.39] val:[93.10] Test:[88.69] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000646
BIAS:[0.10] | Model:[CausalGAT] Epoch:[65/100] Loss:[0.0669=0.0000+0.0434+0.0468] Train:[98.45] val:[83.19] Test:[83.12] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000619
BIAS:[0.10] | Model:[CausalGAT] Epoch:[66/100] Loss:[0.0751=0.0000+0.0487+0.0527] Train:[98.30] val:[95.36] Test:[78.12] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000592
BIAS:[0.10] | Model:[CausalGAT] Epoch:[67/100] Loss:[0.0639=0.0000+0.0411+0.0456] Train:[98.45] val:[94.73] Test:[77.19] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000566
BIAS:[0.10] | Model:[CausalGAT] Epoch:[68/100] Loss:[0.0740=0.0000+0.0483+0.0513] Train:[98.18] val:[98.49] Test:[94.00] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000541
BIAS:[0.10] | Model:[CausalGAT] Epoch:[69/100] Loss:[0.0586=0.0000+0.0382+0.0406] Train:[98.61] val:[97.49] Test:[89.69] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000516
BIAS:[0.10] | Model:[CausalGAT] Epoch:[70/100] Loss:[0.0605=0.0000+0.0394+0.0421] Train:[98.82] val:[96.74] Test:[87.44] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000492
BIAS:[0.10] | Model:[CausalGAT] Epoch:[71/100] Loss:[0.0623=0.0000+0.0404+0.0437] Train:[98.59] val:[97.87] Test:[92.62] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000468
BIAS:[0.10] | Model:[CausalGAT] Epoch:[72/100] Loss:[0.0523=0.0000+0.0340+0.0365] Train:[98.86] val:[97.37] Test:[90.81] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000444
BIAS:[0.10] | Model:[CausalGAT] Epoch:[73/100] Loss:[0.0507=0.0000+0.0335+0.0345] Train:[98.96] val:[94.73] Test:[77.75] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000422
BIAS:[0.10] | Model:[CausalGAT] Epoch:[74/100] Loss:[0.0584=0.0000+0.0382+0.0405] Train:[98.73] val:[98.12] Test:[93.62] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000400
BIAS:[0.10] | Model:[CausalGAT] Epoch:[75/100] Loss:[0.0516=0.0000+0.0330+0.0372] Train:[98.91] val:[98.49] Test:[93.06] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000378
BIAS:[0.10] | Model:[CausalGAT] Epoch:[76/100] Loss:[0.0510=0.0000+0.0332+0.0356] Train:[98.87] val:[96.49] Test:[87.06] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000357
BIAS:[0.10] | Model:[CausalGAT] Epoch:[77/100] Loss:[0.0458=0.0000+0.0298+0.0318] Train:[98.86] val:[97.74] Test:[91.31] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000337
BIAS:[0.10] | Model:[CausalGAT] Epoch:[78/100] Loss:[0.0486=0.0000+0.0321+0.0329] Train:[98.84] val:[97.49] Test:[90.12] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000318
BIAS:[0.10] | Model:[CausalGAT] Epoch:[79/100] Loss:[0.0494=0.0000+0.0323+0.0342] Train:[98.68] val:[97.24] Test:[90.69] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000299
BIAS:[0.10] | Model:[CausalGAT] Epoch:[80/100] Loss:[0.0500=0.0000+0.0322+0.0356] Train:[99.02] val:[96.86] Test:[90.00] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000281
BIAS:[0.10] | Model:[CausalGAT] Epoch:[81/100] Loss:[0.0529=0.0000+0.0343+0.0373] Train:[98.80] val:[96.86] Test:[87.38] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000264
BIAS:[0.10] | Model:[CausalGAT] Epoch:[82/100] Loss:[0.0500=0.0000+0.0330+0.0340] Train:[98.93] val:[98.49] Test:[91.06] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000248
BIAS:[0.10] | Model:[CausalGAT] Epoch:[83/100] Loss:[0.0430=0.0000+0.0275+0.0310] Train:[99.11] val:[96.49] Test:[85.81] | Update Test:[co:91.62,c:25.37,o:92.88] at Epoch:[58] | lr:0.000232
BIAS:[0.10] | Model:[CausalGAT] Epoch:[84/100] Loss:[0.0511=0.0000+0.0326+0.0370] Train:[98.64] val:[98.87] Test:[93.56] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000218
BIAS:[0.10] | Model:[CausalGAT] Epoch:[85/100] Loss:[0.0405=0.0000+0.0260+0.0290] Train:[99.09] val:[97.49] Test:[92.69] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000204
BIAS:[0.10] | Model:[CausalGAT] Epoch:[86/100] Loss:[0.0437=0.0000+0.0282+0.0309] Train:[99.00] val:[96.61] Test:[89.88] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000190
BIAS:[0.10] | Model:[CausalGAT] Epoch:[87/100] Loss:[0.0479=0.0000+0.0307+0.0344] Train:[99.00] val:[98.37] Test:[92.81] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000178
BIAS:[0.10] | Model:[CausalGAT] Epoch:[88/100] Loss:[0.0452=0.0000+0.0293+0.0317] Train:[98.95] val:[97.49] Test:[91.50] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000167
BIAS:[0.10] | Model:[CausalGAT] Epoch:[89/100] Loss:[0.0403=0.0000+0.0257+0.0294] Train:[99.04] val:[97.11] Test:[89.25] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000156
BIAS:[0.10] | Model:[CausalGAT] Epoch:[90/100] Loss:[0.0475=0.0000+0.0308+0.0334] Train:[98.89] val:[96.11] Test:[81.94] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000146
BIAS:[0.10] | Model:[CausalGAT] Epoch:[91/100] Loss:[0.0395=0.0000+0.0253+0.0284] Train:[99.14] val:[96.61] Test:[87.12] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000138
BIAS:[0.10] | Model:[CausalGAT] Epoch:[92/100] Loss:[0.0446=0.0000+0.0287+0.0317] Train:[99.04] val:[96.61] Test:[87.12] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000130
BIAS:[0.10] | Model:[CausalGAT] Epoch:[93/100] Loss:[0.0438=0.0000+0.0289+0.0297] Train:[98.96] val:[97.62] Test:[90.69] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000123
BIAS:[0.10] | Model:[CausalGAT] Epoch:[94/100] Loss:[0.0411=0.0000+0.0269+0.0283] Train:[99.02] val:[97.87] Test:[90.69] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000117
BIAS:[0.10] | Model:[CausalGAT] Epoch:[95/100] Loss:[0.0398=0.0000+0.0258+0.0280] Train:[99.18] val:[97.74] Test:[89.31] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000112
BIAS:[0.10] | Model:[CausalGAT] Epoch:[96/100] Loss:[0.0378=0.0000+0.0241+0.0274] Train:[99.25] val:[97.37] Test:[88.81] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000107
BIAS:[0.10] | Model:[CausalGAT] Epoch:[97/100] Loss:[0.0416=0.0000+0.0268+0.0296] Train:[99.18] val:[96.86] Test:[87.25] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000104
BIAS:[0.10] | Model:[CausalGAT] Epoch:[98/100] Loss:[0.0393=0.0000+0.0253+0.0281] Train:[99.21] val:[97.24] Test:[89.69] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000102
BIAS:[0.10] | Model:[CausalGAT] Epoch:[99/100] Loss:[0.0352=0.0000+0.0225+0.0254] Train:[99.30] val:[96.74] Test:[87.44] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000100
BIAS:[0.10] | Model:[CausalGAT] Epoch:[100/100] Loss:[0.0406=0.0000+0.0262+0.0287] Train:[99.02] val:[96.86] Test:[88.44] | Update Test:[co:93.75,c:26.06,o:93.56] at Epoch:[84] | lr:0.000100
syd: BIAS:[0.10] | Val acc:[96.86] Test acc:[co:93.75,c:26.06,o:93.56] at epoch:[84]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.3
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GAT
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:420  , Cycle:979  , Grids:979  , Diams:979   
| BA  : House:979  , Cycle:420  , Grids:420  , Diams:420   
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:30.0%, Cycle:70.0%, Grids:70.0%, Diams:70.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:60   , Cycle:140  , Grids:140  , Diams:140   
| BA  : House:140  , Cycle:60   , Grids:60   , Diams:60    
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:30.0%, Cycle:70.0%, Grids:70.0%, Diams:70.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.30] | Model:[GAT] Epoch:[1/100] Loss:[0.8598] Train:[65.01] val:[67.25] Test:[51.69] | Best Val:[67.25] Update Test:[51.69] at Epoch:[1] | lr:0.002000
BIAS:[0.30] | Model:[GAT] Epoch:[2/100] Loss:[0.5144] Train:[80.06] val:[83.12] Test:[74.88] | Best Val:[83.12] Update Test:[74.88] at Epoch:[2] | lr:0.001998
BIAS:[0.30] | Model:[GAT] Epoch:[3/100] Loss:[0.4392] Train:[82.88] val:[87.12] Test:[80.81] | Best Val:[87.12] Update Test:[80.81] at Epoch:[3] | lr:0.001996
BIAS:[0.30] | Model:[GAT] Epoch:[4/100] Loss:[0.3855] Train:[85.20] val:[88.38] Test:[82.94] | Best Val:[88.38] Update Test:[82.94] at Epoch:[4] | lr:0.001993
BIAS:[0.30] | Model:[GAT] Epoch:[5/100] Loss:[0.3745] Train:[86.01] val:[88.12] Test:[81.00] | Best Val:[88.38] Update Test:[82.94] at Epoch:[4] | lr:0.001988
BIAS:[0.30] | Model:[GAT] Epoch:[6/100] Loss:[0.3661] Train:[86.53] val:[88.12] Test:[79.56] | Best Val:[88.38] Update Test:[82.94] at Epoch:[4] | lr:0.001983
BIAS:[0.30] | Model:[GAT] Epoch:[7/100] Loss:[0.3478] Train:[86.45] val:[86.38] Test:[82.25] | Best Val:[88.38] Update Test:[82.94] at Epoch:[4] | lr:0.001977
BIAS:[0.30] | Model:[GAT] Epoch:[8/100] Loss:[0.3499] Train:[86.53] val:[89.25] Test:[84.38] | Best Val:[89.25] Update Test:[84.38] at Epoch:[8] | lr:0.001970
BIAS:[0.30] | Model:[GAT] Epoch:[9/100] Loss:[0.3307] Train:[87.58] val:[89.62] Test:[84.94] | Best Val:[89.62] Update Test:[84.94] at Epoch:[9] | lr:0.001962
BIAS:[0.30] | Model:[GAT] Epoch:[10/100] Loss:[0.3330] Train:[87.12] val:[89.62] Test:[84.31] | Best Val:[89.62] Update Test:[84.94] at Epoch:[9] | lr:0.001954
BIAS:[0.30] | Model:[GAT] Epoch:[11/100] Loss:[0.3152] Train:[87.54] val:[88.50] Test:[85.19] | Best Val:[89.62] Update Test:[84.94] at Epoch:[9] | lr:0.001944
BIAS:[0.30] | Model:[GAT] Epoch:[12/100] Loss:[0.3440] Train:[86.54] val:[90.38] Test:[84.12] | Best Val:[90.38] Update Test:[84.12] at Epoch:[12] | lr:0.001933
BIAS:[0.30] | Model:[GAT] Epoch:[13/100] Loss:[0.3152] Train:[88.42] val:[88.88] Test:[84.31] | Best Val:[90.38] Update Test:[84.12] at Epoch:[12] | lr:0.001922
BIAS:[0.30] | Model:[GAT] Epoch:[14/100] Loss:[0.3061] Train:[88.37] val:[88.25] Test:[82.62] | Best Val:[90.38] Update Test:[84.12] at Epoch:[12] | lr:0.001910
BIAS:[0.30] | Model:[GAT] Epoch:[15/100] Loss:[0.3074] Train:[88.17] val:[88.62] Test:[83.06] | Best Val:[90.38] Update Test:[84.12] at Epoch:[12] | lr:0.001896
BIAS:[0.30] | Model:[GAT] Epoch:[16/100] Loss:[0.3170] Train:[88.10] val:[88.88] Test:[83.38] | Best Val:[90.38] Update Test:[84.12] at Epoch:[12] | lr:0.001882
BIAS:[0.30] | Model:[GAT] Epoch:[17/100] Loss:[0.3024] Train:[88.63] val:[90.00] Test:[85.25] | Best Val:[90.38] Update Test:[84.12] at Epoch:[12] | lr:0.001868
BIAS:[0.30] | Model:[GAT] Epoch:[18/100] Loss:[0.3036] Train:[88.38] val:[88.12] Test:[84.69] | Best Val:[90.38] Update Test:[84.12] at Epoch:[12] | lr:0.001852
BIAS:[0.30] | Model:[GAT] Epoch:[19/100] Loss:[0.2952] Train:[88.90] val:[88.75] Test:[83.06] | Best Val:[90.38] Update Test:[84.12] at Epoch:[12] | lr:0.001836
BIAS:[0.30] | Model:[GAT] Epoch:[20/100] Loss:[0.2924] Train:[88.94] val:[90.88] Test:[85.25] | Best Val:[90.88] Update Test:[85.25] at Epoch:[20] | lr:0.001819
BIAS:[0.30] | Model:[GAT] Epoch:[21/100] Loss:[0.2920] Train:[88.92] val:[90.12] Test:[85.12] | Best Val:[90.88] Update Test:[85.25] at Epoch:[20] | lr:0.001801
BIAS:[0.30] | Model:[GAT] Epoch:[22/100] Loss:[0.2816] Train:[89.71] val:[90.75] Test:[87.00] | Best Val:[90.88] Update Test:[85.25] at Epoch:[20] | lr:0.001782
BIAS:[0.30] | Model:[GAT] Epoch:[23/100] Loss:[0.2716] Train:[89.67] val:[89.75] Test:[83.44] | Best Val:[90.88] Update Test:[85.25] at Epoch:[20] | lr:0.001763
BIAS:[0.30] | Model:[GAT] Epoch:[24/100] Loss:[0.2774] Train:[89.17] val:[90.50] Test:[85.38] | Best Val:[90.88] Update Test:[85.25] at Epoch:[20] | lr:0.001743
BIAS:[0.30] | Model:[GAT] Epoch:[25/100] Loss:[0.2855] Train:[89.46] val:[89.25] Test:[88.06] | Best Val:[90.88] Update Test:[85.25] at Epoch:[20] | lr:0.001722
BIAS:[0.30] | Model:[GAT] Epoch:[26/100] Loss:[0.2731] Train:[89.99] val:[91.50] Test:[86.06] | Best Val:[91.50] Update Test:[86.06] at Epoch:[26] | lr:0.001700
BIAS:[0.30] | Model:[GAT] Epoch:[27/100] Loss:[0.2680] Train:[89.96] val:[91.38] Test:[86.50] | Best Val:[91.50] Update Test:[86.06] at Epoch:[26] | lr:0.001678
BIAS:[0.30] | Model:[GAT] Epoch:[28/100] Loss:[0.2571] Train:[90.44] val:[91.75] Test:[87.81] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001656
BIAS:[0.30] | Model:[GAT] Epoch:[29/100] Loss:[0.2631] Train:[90.23] val:[90.88] Test:[86.50] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001632
BIAS:[0.30] | Model:[GAT] Epoch:[30/100] Loss:[0.2537] Train:[90.73] val:[90.88] Test:[85.06] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001608
BIAS:[0.30] | Model:[GAT] Epoch:[31/100] Loss:[0.2419] Train:[90.35] val:[91.38] Test:[86.06] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001584
BIAS:[0.30] | Model:[GAT] Epoch:[32/100] Loss:[0.2536] Train:[90.46] val:[91.38] Test:[85.56] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001559
BIAS:[0.30] | Model:[GAT] Epoch:[33/100] Loss:[0.2442] Train:[90.19] val:[90.50] Test:[85.56] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001534
BIAS:[0.30] | Model:[GAT] Epoch:[34/100] Loss:[0.2502] Train:[90.83] val:[90.88] Test:[85.31] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001508
BIAS:[0.30] | Model:[GAT] Epoch:[35/100] Loss:[0.2473] Train:[90.39] val:[90.25] Test:[86.81] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001481
BIAS:[0.30] | Model:[GAT] Epoch:[36/100] Loss:[0.2457] Train:[90.71] val:[89.88] Test:[86.56] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001454
BIAS:[0.30] | Model:[GAT] Epoch:[37/100] Loss:[0.2469] Train:[90.85] val:[91.62] Test:[87.00] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001427
BIAS:[0.30] | Model:[GAT] Epoch:[38/100] Loss:[0.2301] Train:[91.49] val:[91.62] Test:[88.69] | Best Val:[91.75] Update Test:[87.81] at Epoch:[28] | lr:0.001400
BIAS:[0.30] | Model:[GAT] Epoch:[39/100] Loss:[0.2358] Train:[90.94] val:[92.12] Test:[88.81] | Best Val:[92.12] Update Test:[88.81] at Epoch:[39] | lr:0.001372
BIAS:[0.30] | Model:[GAT] Epoch:[40/100] Loss:[0.2243] Train:[91.42] val:[91.88] Test:[88.12] | Best Val:[92.12] Update Test:[88.81] at Epoch:[39] | lr:0.001344
BIAS:[0.30] | Model:[GAT] Epoch:[41/100] Loss:[0.2322] Train:[91.19] val:[92.62] Test:[88.38] | Best Val:[92.62] Update Test:[88.38] at Epoch:[41] | lr:0.001315
BIAS:[0.30] | Model:[GAT] Epoch:[42/100] Loss:[0.2201] Train:[91.62] val:[92.50] Test:[87.94] | Best Val:[92.62] Update Test:[88.38] at Epoch:[41] | lr:0.001286
BIAS:[0.30] | Model:[GAT] Epoch:[43/100] Loss:[0.2294] Train:[91.32] val:[91.75] Test:[88.88] | Best Val:[92.62] Update Test:[88.38] at Epoch:[41] | lr:0.001257
BIAS:[0.30] | Model:[GAT] Epoch:[44/100] Loss:[0.2188] Train:[91.53] val:[91.62] Test:[87.25] | Best Val:[92.62] Update Test:[88.38] at Epoch:[41] | lr:0.001228
BIAS:[0.30] | Model:[GAT] Epoch:[45/100] Loss:[0.2179] Train:[91.83] val:[92.75] Test:[88.88] | Best Val:[92.75] Update Test:[88.88] at Epoch:[45] | lr:0.001199
BIAS:[0.30] | Model:[GAT] Epoch:[46/100] Loss:[0.2091] Train:[92.14] val:[93.25] Test:[89.06] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.001169
BIAS:[0.30] | Model:[GAT] Epoch:[47/100] Loss:[0.2178] Train:[91.69] val:[92.62] Test:[89.06] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.001139
BIAS:[0.30] | Model:[GAT] Epoch:[48/100] Loss:[0.2145] Train:[92.03] val:[92.12] Test:[87.44] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.001110
BIAS:[0.30] | Model:[GAT] Epoch:[49/100] Loss:[0.2135] Train:[92.23] val:[93.25] Test:[89.81] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.001080
BIAS:[0.30] | Model:[GAT] Epoch:[50/100] Loss:[0.2079] Train:[92.10] val:[92.88] Test:[89.50] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.001050
BIAS:[0.30] | Model:[GAT] Epoch:[51/100] Loss:[0.2032] Train:[92.57] val:[92.75] Test:[88.88] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.001020
BIAS:[0.30] | Model:[GAT] Epoch:[52/100] Loss:[0.2031] Train:[92.41] val:[93.00] Test:[88.25] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.000990
BIAS:[0.30] | Model:[GAT] Epoch:[53/100] Loss:[0.2126] Train:[92.07] val:[92.88] Test:[88.56] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.000961
BIAS:[0.30] | Model:[GAT] Epoch:[54/100] Loss:[0.2071] Train:[92.41] val:[92.12] Test:[87.50] | Best Val:[93.25] Update Test:[89.06] at Epoch:[46] | lr:0.000931
BIAS:[0.30] | Model:[GAT] Epoch:[55/100] Loss:[0.2043] Train:[92.21] val:[93.75] Test:[89.25] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000901
BIAS:[0.30] | Model:[GAT] Epoch:[56/100] Loss:[0.1928] Train:[92.51] val:[92.75] Test:[91.12] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000872
BIAS:[0.30] | Model:[GAT] Epoch:[57/100] Loss:[0.2025] Train:[92.41] val:[93.38] Test:[89.56] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000843
BIAS:[0.30] | Model:[GAT] Epoch:[58/100] Loss:[0.2051] Train:[92.32] val:[93.00] Test:[89.38] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000814
BIAS:[0.30] | Model:[GAT] Epoch:[59/100] Loss:[0.1933] Train:[92.55] val:[91.62] Test:[87.19] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000785
BIAS:[0.30] | Model:[GAT] Epoch:[60/100] Loss:[0.1946] Train:[92.42] val:[93.25] Test:[88.81] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000756
BIAS:[0.30] | Model:[GAT] Epoch:[61/100] Loss:[0.1860] Train:[93.25] val:[92.00] Test:[87.38] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000728
BIAS:[0.30] | Model:[GAT] Epoch:[62/100] Loss:[0.1924] Train:[93.07] val:[92.12] Test:[88.94] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000700
BIAS:[0.30] | Model:[GAT] Epoch:[63/100] Loss:[0.1952] Train:[92.74] val:[92.75] Test:[90.38] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000673
BIAS:[0.30] | Model:[GAT] Epoch:[64/100] Loss:[0.1841] Train:[93.17] val:[93.38] Test:[91.00] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000646
BIAS:[0.30] | Model:[GAT] Epoch:[65/100] Loss:[0.1904] Train:[93.03] val:[93.50] Test:[89.25] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000619
BIAS:[0.30] | Model:[GAT] Epoch:[66/100] Loss:[0.1871] Train:[93.12] val:[90.88] Test:[85.94] | Best Val:[93.75] Update Test:[89.25] at Epoch:[55] | lr:0.000592
BIAS:[0.30] | Model:[GAT] Epoch:[67/100] Loss:[0.1876] Train:[92.92] val:[93.88] Test:[89.62] | Best Val:[93.88] Update Test:[89.62] at Epoch:[67] | lr:0.000566
BIAS:[0.30] | Model:[GAT] Epoch:[68/100] Loss:[0.1840] Train:[93.28] val:[93.50] Test:[89.25] | Best Val:[93.88] Update Test:[89.62] at Epoch:[67] | lr:0.000541
BIAS:[0.30] | Model:[GAT] Epoch:[69/100] Loss:[0.1760] Train:[93.37] val:[93.12] Test:[88.75] | Best Val:[93.88] Update Test:[89.62] at Epoch:[67] | lr:0.000516
BIAS:[0.30] | Model:[GAT] Epoch:[70/100] Loss:[0.1697] Train:[93.73] val:[93.50] Test:[89.50] | Best Val:[93.88] Update Test:[89.62] at Epoch:[67] | lr:0.000492
BIAS:[0.30] | Model:[GAT] Epoch:[71/100] Loss:[0.1795] Train:[93.32] val:[93.38] Test:[89.19] | Best Val:[93.88] Update Test:[89.62] at Epoch:[67] | lr:0.000468
BIAS:[0.30] | Model:[GAT] Epoch:[72/100] Loss:[0.1777] Train:[93.60] val:[93.75] Test:[89.62] | Best Val:[93.88] Update Test:[89.62] at Epoch:[67] | lr:0.000444
BIAS:[0.30] | Model:[GAT] Epoch:[73/100] Loss:[0.1745] Train:[93.35] val:[93.50] Test:[89.19] | Best Val:[93.88] Update Test:[89.62] at Epoch:[67] | lr:0.000422
BIAS:[0.30] | Model:[GAT] Epoch:[74/100] Loss:[0.1822] Train:[92.94] val:[93.50] Test:[88.31] | Best Val:[93.88] Update Test:[89.62] at Epoch:[67] | lr:0.000400
BIAS:[0.30] | Model:[GAT] Epoch:[75/100] Loss:[0.1671] Train:[93.85] val:[94.00] Test:[90.06] | Best Val:[94.00] Update Test:[90.06] at Epoch:[75] | lr:0.000378
BIAS:[0.30] | Model:[GAT] Epoch:[76/100] Loss:[0.1690] Train:[93.67] val:[94.38] Test:[90.06] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000357
BIAS:[0.30] | Model:[GAT] Epoch:[77/100] Loss:[0.1683] Train:[93.67] val:[93.12] Test:[89.25] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000337
BIAS:[0.30] | Model:[GAT] Epoch:[78/100] Loss:[0.1694] Train:[93.78] val:[93.38] Test:[88.94] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000318
BIAS:[0.30] | Model:[GAT] Epoch:[79/100] Loss:[0.1789] Train:[93.16] val:[93.88] Test:[89.31] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000299
BIAS:[0.30] | Model:[GAT] Epoch:[80/100] Loss:[0.1664] Train:[93.58] val:[93.38] Test:[90.25] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000281
BIAS:[0.30] | Model:[GAT] Epoch:[81/100] Loss:[0.1627] Train:[94.37] val:[93.00] Test:[88.56] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000264
BIAS:[0.30] | Model:[GAT] Epoch:[82/100] Loss:[0.1672] Train:[93.73] val:[92.75] Test:[89.19] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000248
BIAS:[0.30] | Model:[GAT] Epoch:[83/100] Loss:[0.1629] Train:[93.80] val:[93.50] Test:[90.00] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000232
BIAS:[0.30] | Model:[GAT] Epoch:[84/100] Loss:[0.1550] Train:[94.09] val:[93.38] Test:[89.50] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000218
BIAS:[0.30] | Model:[GAT] Epoch:[85/100] Loss:[0.1610] Train:[94.26] val:[93.75] Test:[89.81] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000204
BIAS:[0.30] | Model:[GAT] Epoch:[86/100] Loss:[0.1617] Train:[93.76] val:[93.25] Test:[89.19] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000190
BIAS:[0.30] | Model:[GAT] Epoch:[87/100] Loss:[0.1616] Train:[94.23] val:[93.50] Test:[89.62] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000178
BIAS:[0.30] | Model:[GAT] Epoch:[88/100] Loss:[0.1524] Train:[94.59] val:[93.12] Test:[89.12] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000167
BIAS:[0.30] | Model:[GAT] Epoch:[89/100] Loss:[0.1625] Train:[93.87] val:[93.38] Test:[89.50] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000156
BIAS:[0.30] | Model:[GAT] Epoch:[90/100] Loss:[0.1572] Train:[94.16] val:[93.62] Test:[89.81] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000146
BIAS:[0.30] | Model:[GAT] Epoch:[91/100] Loss:[0.1573] Train:[94.03] val:[94.00] Test:[90.31] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000138
BIAS:[0.30] | Model:[GAT] Epoch:[92/100] Loss:[0.1521] Train:[94.30] val:[94.00] Test:[90.19] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000130
BIAS:[0.30] | Model:[GAT] Epoch:[93/100] Loss:[0.1542] Train:[94.25] val:[93.75] Test:[90.31] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000123
BIAS:[0.30] | Model:[GAT] Epoch:[94/100] Loss:[0.1481] Train:[94.64] val:[93.50] Test:[89.50] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000117
BIAS:[0.30] | Model:[GAT] Epoch:[95/100] Loss:[0.1525] Train:[94.44] val:[93.62] Test:[90.50] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000112
BIAS:[0.30] | Model:[GAT] Epoch:[96/100] Loss:[0.1543] Train:[93.92] val:[93.88] Test:[89.81] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000107
BIAS:[0.30] | Model:[GAT] Epoch:[97/100] Loss:[0.1487] Train:[94.62] val:[94.38] Test:[90.12] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000104
BIAS:[0.30] | Model:[GAT] Epoch:[98/100] Loss:[0.1478] Train:[94.28] val:[93.75] Test:[89.50] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000102
BIAS:[0.30] | Model:[GAT] Epoch:[99/100] Loss:[0.1425] Train:[94.71] val:[93.50] Test:[89.19] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000100
BIAS:[0.30] | Model:[GAT] Epoch:[100/100] Loss:[0.1505] Train:[94.44] val:[93.25] Test:[89.06] | Best Val:[94.38] Update Test:[90.06] at Epoch:[76] | lr:0.000100
syd: BIAS:[0.30] | Best Val acc:[94.38] Test acc:[90.06] at epoch:[76]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.3
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGAT
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:420  , Cycle:979  , Grids:979  , Diams:979   
| BA  : House:979  , Cycle:420  , Grids:420  , Diams:420   
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:30.0%, Cycle:70.0%, Grids:70.0%, Diams:70.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:60   , Cycle:140  , Grids:140  , Diams:140   
| BA  : House:140  , Cycle:60   , Grids:60   , Diams:60    
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:30.0%, Cycle:70.0%, Grids:70.0%, Diams:70.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.30] | Model:[CausalGAT] Epoch:[1/100] Loss:[1.0838=0.0194+0.6891+0.7701] Train:[73.36] val:[81.75] Test:[73.81] | Update Test:[co:60.81,c:25.37,o:73.81] at Epoch:[1] | lr:0.002000
BIAS:[0.30] | Model:[CausalGAT] Epoch:[2/100] Loss:[0.6476=0.0014+0.4208+0.4521] Train:[84.47] val:[88.50] Test:[82.56] | Update Test:[co:75.25,c:24.38,o:82.56] at Epoch:[2] | lr:0.001998
BIAS:[0.30] | Model:[CausalGAT] Epoch:[3/100] Loss:[0.5655=0.0009+0.3693+0.3915] Train:[86.15] val:[90.12] Test:[84.31] | Update Test:[co:82.62,c:26.56,o:84.31] at Epoch:[3] | lr:0.001996
BIAS:[0.30] | Model:[CausalGAT] Epoch:[4/100] Loss:[0.5136=0.0005+0.3355+0.3558] Train:[87.53] val:[90.25] Test:[84.94] | Update Test:[co:84.75,c:24.12,o:84.94] at Epoch:[4] | lr:0.001993
BIAS:[0.30] | Model:[CausalGAT] Epoch:[5/100] Loss:[0.5065=0.0005+0.3311+0.3504] Train:[88.01] val:[90.38] Test:[84.81] | Update Test:[co:84.75,c:25.25,o:84.81] at Epoch:[5] | lr:0.001988
BIAS:[0.30] | Model:[CausalGAT] Epoch:[6/100] Loss:[0.4944=0.0004+0.3255+0.3374] Train:[88.01] val:[86.50] Test:[82.25] | Update Test:[co:84.75,c:25.25,o:84.81] at Epoch:[5] | lr:0.001983
BIAS:[0.30] | Model:[CausalGAT] Epoch:[7/100] Loss:[0.4573=0.0003+0.2990+0.3162] Train:[88.58] val:[90.38] Test:[85.50] | Update Test:[co:84.75,c:25.25,o:84.81] at Epoch:[5] | lr:0.001977
BIAS:[0.30] | Model:[CausalGAT] Epoch:[8/100] Loss:[0.4333=0.0003+0.2852+0.2958] Train:[89.31] val:[92.00] Test:[89.50] | Update Test:[co:88.31,c:25.06,o:89.50] at Epoch:[8] | lr:0.001970
BIAS:[0.30] | Model:[CausalGAT] Epoch:[9/100] Loss:[0.4133=0.0002+0.2683+0.2897] Train:[89.89] val:[91.75] Test:[88.25] | Update Test:[co:88.31,c:25.06,o:89.50] at Epoch:[8] | lr:0.001962
BIAS:[0.30] | Model:[CausalGAT] Epoch:[10/100] Loss:[0.3980=0.0002+0.2590+0.2778] Train:[90.37] val:[91.38] Test:[88.06] | Update Test:[co:88.31,c:25.06,o:89.50] at Epoch:[8] | lr:0.001954
BIAS:[0.30] | Model:[CausalGAT] Epoch:[11/100] Loss:[0.3749=0.0003+0.2446+0.2602] Train:[90.65] val:[90.88] Test:[87.62] | Update Test:[co:88.31,c:25.06,o:89.50] at Epoch:[8] | lr:0.001944
BIAS:[0.30] | Model:[CausalGAT] Epoch:[12/100] Loss:[0.3935=0.0002+0.2571+0.2727] Train:[90.24] val:[90.50] Test:[88.75] | Update Test:[co:88.31,c:25.06,o:89.50] at Epoch:[8] | lr:0.001933
BIAS:[0.30] | Model:[CausalGAT] Epoch:[13/100] Loss:[0.3792=0.0001+0.2474+0.2634] Train:[90.71] val:[93.00] Test:[88.94] | Update Test:[co:88.69,c:21.62,o:88.94] at Epoch:[13] | lr:0.001922
BIAS:[0.30] | Model:[CausalGAT] Epoch:[14/100] Loss:[0.3630=0.0002+0.2379+0.2500] Train:[91.21] val:[91.88] Test:[88.50] | Update Test:[co:88.69,c:21.62,o:88.94] at Epoch:[13] | lr:0.001910
BIAS:[0.30] | Model:[CausalGAT] Epoch:[15/100] Loss:[0.3728=0.0002+0.2451+0.2553] Train:[90.55] val:[92.62] Test:[88.38] | Update Test:[co:88.69,c:21.62,o:88.94] at Epoch:[13] | lr:0.001896
BIAS:[0.30] | Model:[CausalGAT] Epoch:[16/100] Loss:[0.3655=0.0001+0.2413+0.2483] Train:[91.15] val:[92.50] Test:[88.81] | Update Test:[co:88.69,c:21.62,o:88.94] at Epoch:[13] | lr:0.001882
BIAS:[0.30] | Model:[CausalGAT] Epoch:[17/100] Loss:[0.3561=0.0001+0.2350+0.2420] Train:[91.62] val:[92.62] Test:[90.81] | Update Test:[co:88.69,c:21.62,o:88.94] at Epoch:[13] | lr:0.001868
BIAS:[0.30] | Model:[CausalGAT] Epoch:[18/100] Loss:[0.3504=0.0001+0.2314+0.2379] Train:[91.35] val:[90.62] Test:[88.69] | Update Test:[co:88.69,c:21.62,o:88.94] at Epoch:[13] | lr:0.001852
BIAS:[0.30] | Model:[CausalGAT] Epoch:[19/100] Loss:[0.3288=0.0001+0.2166+0.2245] Train:[92.35] val:[93.12] Test:[89.25] | Update Test:[co:89.25,c:26.06,o:89.25] at Epoch:[19] | lr:0.001836
BIAS:[0.30] | Model:[CausalGAT] Epoch:[20/100] Loss:[0.3005=0.0001+0.1975+0.2059] Train:[92.58] val:[94.12] Test:[90.75] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001819
BIAS:[0.30] | Model:[CausalGAT] Epoch:[21/100] Loss:[0.3162=0.0001+0.2073+0.2177] Train:[92.32] val:[89.00] Test:[87.06] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001801
BIAS:[0.30] | Model:[CausalGAT] Epoch:[22/100] Loss:[0.3369=0.0002+0.2231+0.2275] Train:[91.85] val:[92.62] Test:[90.44] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001782
BIAS:[0.30] | Model:[CausalGAT] Epoch:[23/100] Loss:[0.3139=0.0001+0.2070+0.2137] Train:[92.16] val:[91.12] Test:[89.31] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001763
BIAS:[0.30] | Model:[CausalGAT] Epoch:[24/100] Loss:[0.2942=0.0001+0.1930+0.2022] Train:[92.71] val:[92.25] Test:[90.06] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001743
BIAS:[0.30] | Model:[CausalGAT] Epoch:[25/100] Loss:[0.2961=0.0002+0.1943+0.2035] Train:[92.69] val:[94.12] Test:[92.50] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001722
BIAS:[0.30] | Model:[CausalGAT] Epoch:[26/100] Loss:[0.2935=0.0001+0.1936+0.1997] Train:[92.96] val:[94.00] Test:[90.75] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001700
BIAS:[0.30] | Model:[CausalGAT] Epoch:[27/100] Loss:[0.2778=0.0001+0.1834+0.1885] Train:[93.25] val:[90.38] Test:[87.94] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001678
BIAS:[0.30] | Model:[CausalGAT] Epoch:[28/100] Loss:[0.2964=0.0001+0.1956+0.2016] Train:[92.92] val:[92.38] Test:[90.81] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001656
BIAS:[0.30] | Model:[CausalGAT] Epoch:[29/100] Loss:[0.2648=0.0001+0.1743+0.1810] Train:[93.53] val:[91.88] Test:[87.62] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001632
BIAS:[0.30] | Model:[CausalGAT] Epoch:[30/100] Loss:[0.2669=0.0001+0.1761+0.1815] Train:[93.55] val:[92.00] Test:[89.75] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001608
BIAS:[0.30] | Model:[CausalGAT] Epoch:[31/100] Loss:[0.2782=0.0001+0.1846+0.1870] Train:[92.99] val:[93.12] Test:[91.88] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001584
BIAS:[0.30] | Model:[CausalGAT] Epoch:[32/100] Loss:[0.2592=0.0001+0.1710+0.1764] Train:[93.66] val:[93.00] Test:[89.06] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001559
BIAS:[0.30] | Model:[CausalGAT] Epoch:[33/100] Loss:[0.2561=0.0001+0.1680+0.1762] Train:[93.64] val:[93.88] Test:[91.12] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001534
BIAS:[0.30] | Model:[CausalGAT] Epoch:[34/100] Loss:[0.2537=0.0001+0.1673+0.1727] Train:[93.98] val:[91.62] Test:[89.69] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001508
BIAS:[0.30] | Model:[CausalGAT] Epoch:[35/100] Loss:[0.2495=0.0001+0.1645+0.1699] Train:[93.78] val:[93.88] Test:[91.56] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001481
BIAS:[0.30] | Model:[CausalGAT] Epoch:[36/100] Loss:[0.2533=0.0001+0.1659+0.1748] Train:[94.12] val:[91.50] Test:[89.81] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001454
BIAS:[0.30] | Model:[CausalGAT] Epoch:[37/100] Loss:[0.2414=0.0001+0.1576+0.1674] Train:[94.34] val:[94.00] Test:[91.50] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001427
BIAS:[0.30] | Model:[CausalGAT] Epoch:[38/100] Loss:[0.2345=0.0001+0.1532+0.1625] Train:[94.51] val:[94.12] Test:[92.62] | Update Test:[co:91.25,c:24.44,o:90.75] at Epoch:[20] | lr:0.001400
BIAS:[0.30] | Model:[CausalGAT] Epoch:[39/100] Loss:[0.2359=0.0001+0.1544+0.1630] Train:[94.82] val:[95.12] Test:[93.38] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001372
BIAS:[0.30] | Model:[CausalGAT] Epoch:[40/100] Loss:[0.2398=0.0001+0.1579+0.1636] Train:[94.46] val:[94.88] Test:[92.88] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001344
BIAS:[0.30] | Model:[CausalGAT] Epoch:[41/100] Loss:[0.2194=0.0001+0.1430+0.1529] Train:[94.67] val:[94.38] Test:[92.38] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001315
BIAS:[0.30] | Model:[CausalGAT] Epoch:[42/100] Loss:[0.2281=0.0001+0.1502+0.1557] Train:[94.67] val:[92.00] Test:[91.94] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001286
BIAS:[0.30] | Model:[CausalGAT] Epoch:[43/100] Loss:[0.2211=0.0001+0.1452+0.1517] Train:[94.75] val:[48.50] Test:[49.00] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001257
BIAS:[0.30] | Model:[CausalGAT] Epoch:[44/100] Loss:[0.2154=0.0001+0.1420+0.1468] Train:[94.73] val:[94.12] Test:[93.94] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001228
BIAS:[0.30] | Model:[CausalGAT] Epoch:[45/100] Loss:[0.1993=0.0001+0.1299+0.1388] Train:[95.39] val:[94.62] Test:[92.88] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001199
BIAS:[0.30] | Model:[CausalGAT] Epoch:[46/100] Loss:[0.1949=0.0000+0.1262+0.1374] Train:[95.25] val:[94.50] Test:[92.94] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001169
BIAS:[0.30] | Model:[CausalGAT] Epoch:[47/100] Loss:[0.2167=0.0000+0.1426+0.1481] Train:[94.66] val:[94.25] Test:[93.62] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001139
BIAS:[0.30] | Model:[CausalGAT] Epoch:[48/100] Loss:[0.1884=0.0000+0.1233+0.1302] Train:[95.48] val:[95.12] Test:[93.88] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001110
BIAS:[0.30] | Model:[CausalGAT] Epoch:[49/100] Loss:[0.2006=0.0000+0.1308+0.1396] Train:[95.26] val:[95.00] Test:[93.62] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001080
BIAS:[0.30] | Model:[CausalGAT] Epoch:[50/100] Loss:[0.1844=0.0000+0.1207+0.1273] Train:[95.57] val:[93.75] Test:[91.62] | Update Test:[co:93.81,c:24.38,o:93.38] at Epoch:[39] | lr:0.001050
BIAS:[0.30] | Model:[CausalGAT] Epoch:[51/100] Loss:[0.2103=0.0000+0.1378+0.1449] Train:[95.00] val:[95.25] Test:[94.75] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.001020
BIAS:[0.30] | Model:[CausalGAT] Epoch:[52/100] Loss:[0.1914=0.0001+0.1254+0.1319] Train:[95.00] val:[93.62] Test:[92.38] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000990
BIAS:[0.30] | Model:[CausalGAT] Epoch:[53/100] Loss:[0.1734=0.0001+0.1133+0.1202] Train:[95.82] val:[95.12] Test:[94.12] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000961
BIAS:[0.30] | Model:[CausalGAT] Epoch:[54/100] Loss:[0.1631=0.0000+0.1058+0.1144] Train:[96.21] val:[95.25] Test:[94.19] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000931
BIAS:[0.30] | Model:[CausalGAT] Epoch:[55/100] Loss:[0.1612=0.0000+0.1044+0.1137] Train:[96.09] val:[94.62] Test:[93.69] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000901
BIAS:[0.30] | Model:[CausalGAT] Epoch:[56/100] Loss:[0.1785=0.0000+0.1167+0.1237] Train:[95.89] val:[94.75] Test:[93.25] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000872
BIAS:[0.30] | Model:[CausalGAT] Epoch:[57/100] Loss:[0.1895=0.0000+0.1231+0.1327] Train:[95.41] val:[94.25] Test:[92.56] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000843
BIAS:[0.30] | Model:[CausalGAT] Epoch:[58/100] Loss:[0.1664=0.0000+0.1082+0.1164] Train:[96.19] val:[94.50] Test:[92.94] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000814
BIAS:[0.30] | Model:[CausalGAT] Epoch:[59/100] Loss:[0.1510=0.0000+0.0985+0.1050] Train:[96.50] val:[94.88] Test:[93.81] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000785
BIAS:[0.30] | Model:[CausalGAT] Epoch:[60/100] Loss:[0.1476=0.0000+0.0961+0.1028] Train:[96.48] val:[95.25] Test:[93.94] | Update Test:[co:93.81,c:26.69,o:94.75] at Epoch:[51] | lr:0.000756
BIAS:[0.30] | Model:[CausalGAT] Epoch:[61/100] Loss:[0.1460=0.0000+0.0946+0.1028] Train:[96.43] val:[95.38] Test:[94.88] | Update Test:[co:94.12,c:30.19,o:94.88] at Epoch:[61] | lr:0.000728
BIAS:[0.30] | Model:[CausalGAT] Epoch:[62/100] Loss:[0.1495=0.0000+0.0972+0.1045] Train:[96.39] val:[93.88] Test:[93.75] | Update Test:[co:94.12,c:30.19,o:94.88] at Epoch:[61] | lr:0.000700
BIAS:[0.30] | Model:[CausalGAT] Epoch:[63/100] Loss:[0.1339=0.0000+0.0873+0.0933] Train:[96.82] val:[95.50] Test:[95.25] | Update Test:[co:94.94,c:27.31,o:95.25] at Epoch:[63] | lr:0.000673
BIAS:[0.30] | Model:[CausalGAT] Epoch:[64/100] Loss:[0.1409=0.0000+0.0916+0.0985] Train:[96.80] val:[95.12] Test:[94.50] | Update Test:[co:94.94,c:27.31,o:95.25] at Epoch:[63] | lr:0.000646
BIAS:[0.30] | Model:[CausalGAT] Epoch:[65/100] Loss:[0.1435=0.0000+0.0927+0.1015] Train:[96.57] val:[95.88] Test:[94.25] | Update Test:[co:94.25,c:25.69,o:94.25] at Epoch:[65] | lr:0.000619
BIAS:[0.30] | Model:[CausalGAT] Epoch:[66/100] Loss:[0.1278=0.0000+0.0829+0.0899] Train:[97.14] val:[95.62] Test:[94.06] | Update Test:[co:94.25,c:25.69,o:94.25] at Epoch:[65] | lr:0.000592
BIAS:[0.30] | Model:[CausalGAT] Epoch:[67/100] Loss:[0.1353=0.0000+0.0879+0.0947] Train:[96.78] val:[94.12] Test:[93.12] | Update Test:[co:94.25,c:25.69,o:94.25] at Epoch:[65] | lr:0.000566
BIAS:[0.30] | Model:[CausalGAT] Epoch:[68/100] Loss:[0.1301=0.0000+0.0849+0.0903] Train:[96.93] val:[95.00] Test:[94.38] | Update Test:[co:94.25,c:25.69,o:94.25] at Epoch:[65] | lr:0.000541
BIAS:[0.30] | Model:[CausalGAT] Epoch:[69/100] Loss:[0.1251=0.0000+0.0807+0.0887] Train:[97.32] val:[96.00] Test:[95.19] | Update Test:[co:94.62,c:23.31,o:95.19] at Epoch:[69] | lr:0.000516
BIAS:[0.30] | Model:[CausalGAT] Epoch:[70/100] Loss:[0.1258=0.0000+0.0809+0.0897] Train:[97.11] val:[95.50] Test:[94.94] | Update Test:[co:94.62,c:23.31,o:95.19] at Epoch:[69] | lr:0.000492
BIAS:[0.30] | Model:[CausalGAT] Epoch:[71/100] Loss:[0.1096=0.0000+0.0705+0.0782] Train:[97.61] val:[95.88] Test:[95.12] | Update Test:[co:94.62,c:23.31,o:95.19] at Epoch:[69] | lr:0.000468
BIAS:[0.30] | Model:[CausalGAT] Epoch:[72/100] Loss:[0.1126=0.0000+0.0723+0.0805] Train:[97.48] val:[96.12] Test:[94.94] | Update Test:[co:93.56,c:19.25,o:94.94] at Epoch:[72] | lr:0.000444
BIAS:[0.30] | Model:[CausalGAT] Epoch:[73/100] Loss:[0.1056=0.0000+0.0677+0.0756] Train:[97.52] val:[96.50] Test:[95.19] | Update Test:[co:95.31,c:20.88,o:95.19] at Epoch:[73] | lr:0.000422
BIAS:[0.30] | Model:[CausalGAT] Epoch:[74/100] Loss:[0.0968=0.0000+0.0621+0.0694] Train:[97.84] val:[95.62] Test:[94.44] | Update Test:[co:95.31,c:20.88,o:95.19] at Epoch:[73] | lr:0.000400
BIAS:[0.30] | Model:[CausalGAT] Epoch:[75/100] Loss:[0.0955=0.0000+0.0608+0.0694] Train:[97.62] val:[96.62] Test:[95.50] | Update Test:[co:94.94,c:25.37,o:95.50] at Epoch:[75] | lr:0.000378
BIAS:[0.30] | Model:[CausalGAT] Epoch:[76/100] Loss:[0.1000=0.0000+0.0645+0.0710] Train:[97.71] val:[97.25] Test:[95.81] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000357
BIAS:[0.30] | Model:[CausalGAT] Epoch:[77/100] Loss:[0.0967=0.0000+0.0624+0.0686] Train:[97.80] val:[96.38] Test:[95.56] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000337
BIAS:[0.30] | Model:[CausalGAT] Epoch:[78/100] Loss:[0.0986=0.0000+0.0637+0.0697] Train:[97.71] val:[96.50] Test:[95.00] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000318
BIAS:[0.30] | Model:[CausalGAT] Epoch:[79/100] Loss:[0.0955=0.0000+0.0609+0.0693] Train:[97.86] val:[97.00] Test:[96.50] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000299
BIAS:[0.30] | Model:[CausalGAT] Epoch:[80/100] Loss:[0.0871=0.0000+0.0561+0.0619] Train:[98.18] val:[96.12] Test:[95.00] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000281
BIAS:[0.30] | Model:[CausalGAT] Epoch:[81/100] Loss:[0.0938=0.0000+0.0607+0.0663] Train:[97.77] val:[97.25] Test:[95.38] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000264
BIAS:[0.30] | Model:[CausalGAT] Epoch:[82/100] Loss:[0.0862=0.0000+0.0555+0.0613] Train:[98.09] val:[97.12] Test:[96.00] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000248
BIAS:[0.30] | Model:[CausalGAT] Epoch:[83/100] Loss:[0.0865=0.0000+0.0557+0.0614] Train:[98.20] val:[97.25] Test:[95.62] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000232
BIAS:[0.30] | Model:[CausalGAT] Epoch:[84/100] Loss:[0.0892=0.0000+0.0570+0.0643] Train:[97.91] val:[96.75] Test:[95.00] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000218
BIAS:[0.30] | Model:[CausalGAT] Epoch:[85/100] Loss:[0.0816=0.0000+0.0522+0.0588] Train:[98.34] val:[96.75] Test:[95.69] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000204
BIAS:[0.30] | Model:[CausalGAT] Epoch:[86/100] Loss:[0.0749=0.0000+0.0475+0.0548] Train:[98.27] val:[97.00] Test:[96.12] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000190
BIAS:[0.30] | Model:[CausalGAT] Epoch:[87/100] Loss:[0.0776=0.0000+0.0493+0.0566] Train:[98.21] val:[96.88] Test:[95.38] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000178
BIAS:[0.30] | Model:[CausalGAT] Epoch:[88/100] Loss:[0.0801=0.0000+0.0510+0.0582] Train:[98.36] val:[96.50] Test:[95.56] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000167
BIAS:[0.30] | Model:[CausalGAT] Epoch:[89/100] Loss:[0.0821=0.0000+0.0531+0.0578] Train:[98.21] val:[97.00] Test:[96.06] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000156
BIAS:[0.30] | Model:[CausalGAT] Epoch:[90/100] Loss:[0.0675=0.0000+0.0436+0.0478] Train:[98.62] val:[96.50] Test:[94.94] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000146
BIAS:[0.30] | Model:[CausalGAT] Epoch:[91/100] Loss:[0.0841=0.0000+0.0538+0.0606] Train:[98.12] val:[97.12] Test:[95.50] | Update Test:[co:95.75,c:32.50,o:95.81] at Epoch:[76] | lr:0.000138
BIAS:[0.30] | Model:[CausalGAT] Epoch:[92/100] Loss:[0.0745=0.0000+0.0471+0.0549] Train:[98.45] val:[97.38] Test:[96.00] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000130
BIAS:[0.30] | Model:[CausalGAT] Epoch:[93/100] Loss:[0.0830=0.0000+0.0532+0.0596] Train:[98.20] val:[96.88] Test:[95.50] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000123
BIAS:[0.30] | Model:[CausalGAT] Epoch:[94/100] Loss:[0.0740=0.0000+0.0474+0.0533] Train:[98.43] val:[96.88] Test:[95.38] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000117
BIAS:[0.30] | Model:[CausalGAT] Epoch:[95/100] Loss:[0.0720=0.0000+0.0460+0.0520] Train:[98.52] val:[97.00] Test:[96.19] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000112
BIAS:[0.30] | Model:[CausalGAT] Epoch:[96/100] Loss:[0.0673=0.0000+0.0428+0.0490] Train:[98.48] val:[96.62] Test:[95.81] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000107
BIAS:[0.30] | Model:[CausalGAT] Epoch:[97/100] Loss:[0.0734=0.0000+0.0474+0.0521] Train:[98.41] val:[96.75] Test:[95.38] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000104
BIAS:[0.30] | Model:[CausalGAT] Epoch:[98/100] Loss:[0.0678=0.0000+0.0427+0.0501] Train:[98.59] val:[97.00] Test:[95.69] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000102
BIAS:[0.30] | Model:[CausalGAT] Epoch:[99/100] Loss:[0.0648=0.0000+0.0415+0.0465] Train:[98.50] val:[96.62] Test:[95.62] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000100
BIAS:[0.30] | Model:[CausalGAT] Epoch:[100/100] Loss:[0.0633=0.0000+0.0398+0.0470] Train:[98.66] val:[96.00] Test:[95.06] | Update Test:[co:95.81,c:25.19,o:96.00] at Epoch:[92] | lr:0.000100
syd: BIAS:[0.30] | Val acc:[96.00] Test acc:[co:95.81,c:25.19,o:96.00] at epoch:[92]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.5
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GAT
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5600
| Tree: House:700  , Cycle:700  , Grids:700  , Diams:700   
| BA  : House:700  , Cycle:700  , Grids:700  , Diams:700   
| All : House:1400 , Cycle:1400 , Grids:1400 , Diams:1400  
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:100  , Cycle:100  , Grids:100  , Diams:100   
| BA  : House:100  , Cycle:100  , Grids:100  , Diams:100   
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.50] | Model:[GAT] Epoch:[1/100] Loss:[0.9554] Train:[59.36] val:[57.63] Test:[55.06] | Best Val:[57.63] Update Test:[55.06] at Epoch:[1] | lr:0.002000
BIAS:[0.50] | Model:[GAT] Epoch:[2/100] Loss:[0.5953] Train:[75.96] val:[81.88] Test:[80.44] | Best Val:[81.88] Update Test:[80.44] at Epoch:[2] | lr:0.001998
BIAS:[0.50] | Model:[GAT] Epoch:[3/100] Loss:[0.5091] Train:[79.23] val:[85.00] Test:[83.56] | Best Val:[85.00] Update Test:[83.56] at Epoch:[3] | lr:0.001996
BIAS:[0.50] | Model:[GAT] Epoch:[4/100] Loss:[0.4765] Train:[81.18] val:[83.12] Test:[82.12] | Best Val:[85.00] Update Test:[83.56] at Epoch:[3] | lr:0.001993
BIAS:[0.50] | Model:[GAT] Epoch:[5/100] Loss:[0.4649] Train:[81.71] val:[84.50] Test:[83.44] | Best Val:[85.00] Update Test:[83.56] at Epoch:[3] | lr:0.001988
BIAS:[0.50] | Model:[GAT] Epoch:[6/100] Loss:[0.4328] Train:[82.96] val:[86.62] Test:[85.50] | Best Val:[86.62] Update Test:[85.50] at Epoch:[6] | lr:0.001983
BIAS:[0.50] | Model:[GAT] Epoch:[7/100] Loss:[0.4256] Train:[83.27] val:[86.25] Test:[85.81] | Best Val:[86.62] Update Test:[85.50] at Epoch:[6] | lr:0.001977
BIAS:[0.50] | Model:[GAT] Epoch:[8/100] Loss:[0.4126] Train:[83.36] val:[84.75] Test:[84.56] | Best Val:[86.62] Update Test:[85.50] at Epoch:[6] | lr:0.001970
BIAS:[0.50] | Model:[GAT] Epoch:[9/100] Loss:[0.4013] Train:[83.59] val:[85.50] Test:[84.88] | Best Val:[86.62] Update Test:[85.50] at Epoch:[6] | lr:0.001962
BIAS:[0.50] | Model:[GAT] Epoch:[10/100] Loss:[0.3933] Train:[84.52] val:[87.25] Test:[86.38] | Best Val:[87.25] Update Test:[86.38] at Epoch:[10] | lr:0.001954
BIAS:[0.50] | Model:[GAT] Epoch:[11/100] Loss:[0.3748] Train:[84.75] val:[89.62] Test:[87.94] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001944
BIAS:[0.50] | Model:[GAT] Epoch:[12/100] Loss:[0.3730] Train:[85.36] val:[86.75] Test:[84.06] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001933
BIAS:[0.50] | Model:[GAT] Epoch:[13/100] Loss:[0.3765] Train:[84.95] val:[87.12] Test:[84.38] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001922
BIAS:[0.50] | Model:[GAT] Epoch:[14/100] Loss:[0.3589] Train:[85.75] val:[89.62] Test:[87.94] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001910
BIAS:[0.50] | Model:[GAT] Epoch:[15/100] Loss:[0.3610] Train:[85.46] val:[87.62] Test:[87.44] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001896
BIAS:[0.50] | Model:[GAT] Epoch:[16/100] Loss:[0.3628] Train:[86.02] val:[89.62] Test:[88.00] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001882
BIAS:[0.50] | Model:[GAT] Epoch:[17/100] Loss:[0.3465] Train:[86.25] val:[89.25] Test:[88.62] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001868
BIAS:[0.50] | Model:[GAT] Epoch:[18/100] Loss:[0.3525] Train:[85.79] val:[85.38] Test:[84.00] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001852
BIAS:[0.50] | Model:[GAT] Epoch:[19/100] Loss:[0.3400] Train:[86.91] val:[89.00] Test:[88.06] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001836
BIAS:[0.50] | Model:[GAT] Epoch:[20/100] Loss:[0.3346] Train:[86.84] val:[89.38] Test:[88.94] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001819
BIAS:[0.50] | Model:[GAT] Epoch:[21/100] Loss:[0.3336] Train:[86.89] val:[88.25] Test:[87.69] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001801
BIAS:[0.50] | Model:[GAT] Epoch:[22/100] Loss:[0.3244] Train:[87.00] val:[87.88] Test:[87.62] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001782
BIAS:[0.50] | Model:[GAT] Epoch:[23/100] Loss:[0.3271] Train:[87.20] val:[87.50] Test:[85.75] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001763
BIAS:[0.50] | Model:[GAT] Epoch:[24/100] Loss:[0.3091] Train:[87.70] val:[89.62] Test:[88.12] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001743
BIAS:[0.50] | Model:[GAT] Epoch:[25/100] Loss:[0.3187] Train:[87.88] val:[88.12] Test:[87.81] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001722
BIAS:[0.50] | Model:[GAT] Epoch:[26/100] Loss:[0.3038] Train:[88.11] val:[88.12] Test:[87.50] | Best Val:[89.62] Update Test:[87.94] at Epoch:[11] | lr:0.001700
BIAS:[0.50] | Model:[GAT] Epoch:[27/100] Loss:[0.3130] Train:[87.48] val:[90.38] Test:[89.69] | Best Val:[90.38] Update Test:[89.69] at Epoch:[27] | lr:0.001678
BIAS:[0.50] | Model:[GAT] Epoch:[28/100] Loss:[0.3108] Train:[87.73] val:[91.38] Test:[89.25] | Best Val:[91.38] Update Test:[89.25] at Epoch:[28] | lr:0.001656
BIAS:[0.50] | Model:[GAT] Epoch:[29/100] Loss:[0.3074] Train:[88.02] val:[89.75] Test:[88.88] | Best Val:[91.38] Update Test:[89.25] at Epoch:[28] | lr:0.001632
BIAS:[0.50] | Model:[GAT] Epoch:[30/100] Loss:[0.2996] Train:[88.23] val:[91.62] Test:[90.00] | Best Val:[91.62] Update Test:[90.00] at Epoch:[30] | lr:0.001608
BIAS:[0.50] | Model:[GAT] Epoch:[31/100] Loss:[0.2907] Train:[88.77] val:[90.00] Test:[88.31] | Best Val:[91.62] Update Test:[90.00] at Epoch:[30] | lr:0.001584
BIAS:[0.50] | Model:[GAT] Epoch:[32/100] Loss:[0.2895] Train:[88.48] val:[92.38] Test:[91.06] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001559
BIAS:[0.50] | Model:[GAT] Epoch:[33/100] Loss:[0.2961] Train:[88.52] val:[88.50] Test:[87.62] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001534
BIAS:[0.50] | Model:[GAT] Epoch:[34/100] Loss:[0.2958] Train:[88.55] val:[90.62] Test:[89.38] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001508
BIAS:[0.50] | Model:[GAT] Epoch:[35/100] Loss:[0.2820] Train:[88.89] val:[90.75] Test:[89.88] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001481
BIAS:[0.50] | Model:[GAT] Epoch:[36/100] Loss:[0.2817] Train:[89.05] val:[89.62] Test:[88.00] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001454
BIAS:[0.50] | Model:[GAT] Epoch:[37/100] Loss:[0.2755] Train:[88.82] val:[89.25] Test:[89.12] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001427
BIAS:[0.50] | Model:[GAT] Epoch:[38/100] Loss:[0.2803] Train:[88.89] val:[91.25] Test:[89.75] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001400
BIAS:[0.50] | Model:[GAT] Epoch:[39/100] Loss:[0.2818] Train:[88.86] val:[90.50] Test:[88.94] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001372
BIAS:[0.50] | Model:[GAT] Epoch:[40/100] Loss:[0.2778] Train:[89.39] val:[90.88] Test:[89.62] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001344
BIAS:[0.50] | Model:[GAT] Epoch:[41/100] Loss:[0.2662] Train:[89.71] val:[88.88] Test:[88.31] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001315
BIAS:[0.50] | Model:[GAT] Epoch:[42/100] Loss:[0.2651] Train:[89.18] val:[90.25] Test:[89.44] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001286
BIAS:[0.50] | Model:[GAT] Epoch:[43/100] Loss:[0.2565] Train:[90.12] val:[92.00] Test:[90.25] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001257
BIAS:[0.50] | Model:[GAT] Epoch:[44/100] Loss:[0.2716] Train:[89.79] val:[91.25] Test:[91.00] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001228
BIAS:[0.50] | Model:[GAT] Epoch:[45/100] Loss:[0.2712] Train:[89.68] val:[90.75] Test:[89.69] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001199
BIAS:[0.50] | Model:[GAT] Epoch:[46/100] Loss:[0.2656] Train:[89.23] val:[91.25] Test:[89.75] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001169
BIAS:[0.50] | Model:[GAT] Epoch:[47/100] Loss:[0.2571] Train:[90.07] val:[91.62] Test:[91.31] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001139
BIAS:[0.50] | Model:[GAT] Epoch:[48/100] Loss:[0.2483] Train:[90.48] val:[91.50] Test:[89.62] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001110
BIAS:[0.50] | Model:[GAT] Epoch:[49/100] Loss:[0.2485] Train:[90.18] val:[90.50] Test:[89.00] | Best Val:[92.38] Update Test:[91.06] at Epoch:[32] | lr:0.001080
BIAS:[0.50] | Model:[GAT] Epoch:[50/100] Loss:[0.2539] Train:[89.91] val:[92.50] Test:[91.25] | Best Val:[92.50] Update Test:[91.25] at Epoch:[50] | lr:0.001050
BIAS:[0.50] | Model:[GAT] Epoch:[51/100] Loss:[0.2409] Train:[91.02] val:[92.00] Test:[90.50] | Best Val:[92.50] Update Test:[91.25] at Epoch:[50] | lr:0.001020
BIAS:[0.50] | Model:[GAT] Epoch:[52/100] Loss:[0.2439] Train:[90.52] val:[90.75] Test:[89.06] | Best Val:[92.50] Update Test:[91.25] at Epoch:[50] | lr:0.000990
BIAS:[0.50] | Model:[GAT] Epoch:[53/100] Loss:[0.2413] Train:[90.68] val:[91.00] Test:[89.38] | Best Val:[92.50] Update Test:[91.25] at Epoch:[50] | lr:0.000961
BIAS:[0.50] | Model:[GAT] Epoch:[54/100] Loss:[0.2452] Train:[90.54] val:[90.12] Test:[89.50] | Best Val:[92.50] Update Test:[91.25] at Epoch:[50] | lr:0.000931
BIAS:[0.50] | Model:[GAT] Epoch:[55/100] Loss:[0.2433] Train:[91.07] val:[90.88] Test:[89.50] | Best Val:[92.50] Update Test:[91.25] at Epoch:[50] | lr:0.000901
BIAS:[0.50] | Model:[GAT] Epoch:[56/100] Loss:[0.2416] Train:[90.61] val:[91.50] Test:[90.12] | Best Val:[92.50] Update Test:[91.25] at Epoch:[50] | lr:0.000872
BIAS:[0.50] | Model:[GAT] Epoch:[57/100] Loss:[0.2345] Train:[90.98] val:[92.12] Test:[90.56] | Best Val:[92.50] Update Test:[91.25] at Epoch:[50] | lr:0.000843
BIAS:[0.50] | Model:[GAT] Epoch:[58/100] Loss:[0.2265] Train:[91.25] val:[93.50] Test:[91.06] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000814
BIAS:[0.50] | Model:[GAT] Epoch:[59/100] Loss:[0.2351] Train:[90.88] val:[91.38] Test:[90.38] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000785
BIAS:[0.50] | Model:[GAT] Epoch:[60/100] Loss:[0.2203] Train:[91.27] val:[90.88] Test:[90.19] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000756
BIAS:[0.50] | Model:[GAT] Epoch:[61/100] Loss:[0.2256] Train:[91.61] val:[91.50] Test:[90.06] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000728
BIAS:[0.50] | Model:[GAT] Epoch:[62/100] Loss:[0.2178] Train:[92.09] val:[92.12] Test:[91.00] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000700
BIAS:[0.50] | Model:[GAT] Epoch:[63/100] Loss:[0.2338] Train:[90.93] val:[91.88] Test:[90.94] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000673
BIAS:[0.50] | Model:[GAT] Epoch:[64/100] Loss:[0.2190] Train:[91.45] val:[92.12] Test:[91.31] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000646
BIAS:[0.50] | Model:[GAT] Epoch:[65/100] Loss:[0.2259] Train:[91.14] val:[92.75] Test:[91.00] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000619
BIAS:[0.50] | Model:[GAT] Epoch:[66/100] Loss:[0.2204] Train:[91.34] val:[91.25] Test:[90.31] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000592
BIAS:[0.50] | Model:[GAT] Epoch:[67/100] Loss:[0.2135] Train:[91.88] val:[91.50] Test:[90.38] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000566
BIAS:[0.50] | Model:[GAT] Epoch:[68/100] Loss:[0.2095] Train:[91.55] val:[92.25] Test:[91.00] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000541
BIAS:[0.50] | Model:[GAT] Epoch:[69/100] Loss:[0.2118] Train:[91.70] val:[92.12] Test:[91.19] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000516
BIAS:[0.50] | Model:[GAT] Epoch:[70/100] Loss:[0.2157] Train:[91.55] val:[92.62] Test:[91.88] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000492
BIAS:[0.50] | Model:[GAT] Epoch:[71/100] Loss:[0.2093] Train:[92.32] val:[92.62] Test:[91.38] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000468
BIAS:[0.50] | Model:[GAT] Epoch:[72/100] Loss:[0.2006] Train:[92.38] val:[93.00] Test:[91.50] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000444
BIAS:[0.50] | Model:[GAT] Epoch:[73/100] Loss:[0.1951] Train:[92.54] val:[93.38] Test:[91.94] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000422
BIAS:[0.50] | Model:[GAT] Epoch:[74/100] Loss:[0.1994] Train:[92.73] val:[92.50] Test:[91.50] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000400
BIAS:[0.50] | Model:[GAT] Epoch:[75/100] Loss:[0.1935] Train:[92.71] val:[92.50] Test:[91.38] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000378
BIAS:[0.50] | Model:[GAT] Epoch:[76/100] Loss:[0.1916] Train:[92.57] val:[92.12] Test:[91.38] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000357
BIAS:[0.50] | Model:[GAT] Epoch:[77/100] Loss:[0.1932] Train:[92.50] val:[92.50] Test:[91.12] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000337
BIAS:[0.50] | Model:[GAT] Epoch:[78/100] Loss:[0.1966] Train:[92.43] val:[92.38] Test:[91.31] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000318
BIAS:[0.50] | Model:[GAT] Epoch:[79/100] Loss:[0.1809] Train:[93.30] val:[92.75] Test:[91.69] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000299
BIAS:[0.50] | Model:[GAT] Epoch:[80/100] Loss:[0.1860] Train:[93.00] val:[92.62] Test:[91.69] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000281
BIAS:[0.50] | Model:[GAT] Epoch:[81/100] Loss:[0.1981] Train:[92.50] val:[91.75] Test:[91.38] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000264
BIAS:[0.50] | Model:[GAT] Epoch:[82/100] Loss:[0.1979] Train:[92.80] val:[92.38] Test:[90.88] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000248
BIAS:[0.50] | Model:[GAT] Epoch:[83/100] Loss:[0.1858] Train:[93.05] val:[93.00] Test:[91.81] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000232
BIAS:[0.50] | Model:[GAT] Epoch:[84/100] Loss:[0.1829] Train:[93.20] val:[93.00] Test:[91.81] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000218
BIAS:[0.50] | Model:[GAT] Epoch:[85/100] Loss:[0.1900] Train:[92.55] val:[92.62] Test:[91.56] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000204
BIAS:[0.50] | Model:[GAT] Epoch:[86/100] Loss:[0.1909] Train:[92.39] val:[93.25] Test:[91.56] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000190
BIAS:[0.50] | Model:[GAT] Epoch:[87/100] Loss:[0.1867] Train:[92.95] val:[93.25] Test:[91.69] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000178
BIAS:[0.50] | Model:[GAT] Epoch:[88/100] Loss:[0.1836] Train:[93.18] val:[92.50] Test:[91.69] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000167
BIAS:[0.50] | Model:[GAT] Epoch:[89/100] Loss:[0.1875] Train:[92.89] val:[93.38] Test:[91.31] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000156
BIAS:[0.50] | Model:[GAT] Epoch:[90/100] Loss:[0.1801] Train:[93.29] val:[93.25] Test:[91.62] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000146
BIAS:[0.50] | Model:[GAT] Epoch:[91/100] Loss:[0.1746] Train:[93.75] val:[93.25] Test:[91.69] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000138
BIAS:[0.50] | Model:[GAT] Epoch:[92/100] Loss:[0.1850] Train:[93.00] val:[92.25] Test:[91.81] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000130
BIAS:[0.50] | Model:[GAT] Epoch:[93/100] Loss:[0.1768] Train:[93.04] val:[93.38] Test:[92.00] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000123
BIAS:[0.50] | Model:[GAT] Epoch:[94/100] Loss:[0.1809] Train:[93.14] val:[92.38] Test:[91.50] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000117
BIAS:[0.50] | Model:[GAT] Epoch:[95/100] Loss:[0.1736] Train:[93.34] val:[92.75] Test:[91.38] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000112
BIAS:[0.50] | Model:[GAT] Epoch:[96/100] Loss:[0.1765] Train:[93.32] val:[93.38] Test:[91.81] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000107
BIAS:[0.50] | Model:[GAT] Epoch:[97/100] Loss:[0.1754] Train:[93.55] val:[93.12] Test:[91.81] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000104
BIAS:[0.50] | Model:[GAT] Epoch:[98/100] Loss:[0.1817] Train:[93.09] val:[92.38] Test:[91.44] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000102
BIAS:[0.50] | Model:[GAT] Epoch:[99/100] Loss:[0.1700] Train:[93.39] val:[92.25] Test:[91.62] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000100
BIAS:[0.50] | Model:[GAT] Epoch:[100/100] Loss:[0.1746] Train:[93.20] val:[93.00] Test:[91.38] | Best Val:[93.50] Update Test:[91.06] at Epoch:[58] | lr:0.000100
syd: BIAS:[0.50] | Best Val acc:[93.50] Test acc:[91.06] at epoch:[58]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.5
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGAT
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5600
| Tree: House:700  , Cycle:700  , Grids:700  , Diams:700   
| BA  : House:700  , Cycle:700  , Grids:700  , Diams:700   
| All : House:1400 , Cycle:1400 , Grids:1400 , Diams:1400  
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:100  , Cycle:100  , Grids:100  , Diams:100   
| BA  : House:100  , Cycle:100  , Grids:100  , Diams:100   
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.50] | Model:[CausalGAT] Epoch:[1/100] Loss:[1.2675=0.0223+0.7957+0.9214] Train:[67.23] val:[73.12] Test:[71.69] | Update Test:[co:58.75,c:24.56,o:71.69] at Epoch:[1] | lr:0.002000
BIAS:[0.50] | Model:[CausalGAT] Epoch:[2/100] Loss:[0.7540=0.0021+0.4866+0.5328] Train:[80.61] val:[85.75] Test:[85.12] | Update Test:[co:84.19,c:25.00,o:85.12] at Epoch:[2] | lr:0.001998
BIAS:[0.50] | Model:[CausalGAT] Epoch:[3/100] Loss:[0.6532=0.0012+0.4283+0.4487] Train:[83.21] val:[88.75] Test:[87.44] | Update Test:[co:85.44,c:25.81,o:87.44] at Epoch:[3] | lr:0.001996
BIAS:[0.50] | Model:[CausalGAT] Epoch:[4/100] Loss:[0.6092=0.0008+0.4007+0.4163] Train:[84.21] val:[87.75] Test:[87.75] | Update Test:[co:85.44,c:25.81,o:87.44] at Epoch:[3] | lr:0.001993
BIAS:[0.50] | Model:[CausalGAT] Epoch:[5/100] Loss:[0.5942=0.0006+0.3900+0.4079] Train:[85.38] val:[87.25] Test:[87.75] | Update Test:[co:85.44,c:25.81,o:87.44] at Epoch:[3] | lr:0.001988
BIAS:[0.50] | Model:[CausalGAT] Epoch:[6/100] Loss:[0.5506=0.0004+0.3614+0.3780] Train:[85.73] val:[87.12] Test:[86.31] | Update Test:[co:85.44,c:25.81,o:87.44] at Epoch:[3] | lr:0.001983
BIAS:[0.50] | Model:[CausalGAT] Epoch:[7/100] Loss:[0.5398=0.0004+0.3541+0.3710] Train:[86.21] val:[90.25] Test:[90.44] | Update Test:[co:87.88,c:25.69,o:90.44] at Epoch:[7] | lr:0.001977
BIAS:[0.50] | Model:[CausalGAT] Epoch:[8/100] Loss:[0.5245=0.0004+0.3439+0.3609] Train:[86.77] val:[90.75] Test:[90.06] | Update Test:[co:90.19,c:23.81,o:90.06] at Epoch:[8] | lr:0.001970
BIAS:[0.50] | Model:[CausalGAT] Epoch:[9/100] Loss:[0.4951=0.0003+0.3260+0.3379] Train:[87.39] val:[91.50] Test:[90.31] | Update Test:[co:88.44,c:25.06,o:90.31] at Epoch:[9] | lr:0.001962
BIAS:[0.50] | Model:[CausalGAT] Epoch:[10/100] Loss:[0.4926=0.0002+0.3240+0.3370] Train:[87.82] val:[89.50] Test:[89.88] | Update Test:[co:88.44,c:25.06,o:90.31] at Epoch:[9] | lr:0.001954
BIAS:[0.50] | Model:[CausalGAT] Epoch:[11/100] Loss:[0.4833=0.0002+0.3182+0.3301] Train:[88.39] val:[90.50] Test:[88.81] | Update Test:[co:88.44,c:25.06,o:90.31] at Epoch:[9] | lr:0.001944
BIAS:[0.50] | Model:[CausalGAT] Epoch:[12/100] Loss:[0.4569=0.0002+0.3000+0.3136] Train:[89.09] val:[87.50] Test:[86.50] | Update Test:[co:88.44,c:25.06,o:90.31] at Epoch:[9] | lr:0.001933
BIAS:[0.50] | Model:[CausalGAT] Epoch:[13/100] Loss:[0.4682=0.0002+0.3077+0.3209] Train:[88.07] val:[90.38] Test:[90.44] | Update Test:[co:88.44,c:25.06,o:90.31] at Epoch:[9] | lr:0.001922
BIAS:[0.50] | Model:[CausalGAT] Epoch:[14/100] Loss:[0.4481=0.0002+0.2953+0.3055] Train:[88.54] val:[88.62] Test:[88.00] | Update Test:[co:88.44,c:25.06,o:90.31] at Epoch:[9] | lr:0.001910
BIAS:[0.50] | Model:[CausalGAT] Epoch:[15/100] Loss:[0.4518=0.0001+0.2969+0.3097] Train:[88.50] val:[91.75] Test:[90.44] | Update Test:[co:90.50,c:24.25,o:90.44] at Epoch:[15] | lr:0.001896
BIAS:[0.50] | Model:[CausalGAT] Epoch:[16/100] Loss:[0.4345=0.0001+0.2861+0.2967] Train:[88.86] val:[90.50] Test:[90.50] | Update Test:[co:90.50,c:24.25,o:90.44] at Epoch:[15] | lr:0.001882
BIAS:[0.50] | Model:[CausalGAT] Epoch:[17/100] Loss:[0.4281=0.0001+0.2812+0.2936] Train:[89.27] val:[91.50] Test:[90.81] | Update Test:[co:90.50,c:24.25,o:90.44] at Epoch:[15] | lr:0.001868
BIAS:[0.50] | Model:[CausalGAT] Epoch:[18/100] Loss:[0.3876=0.0001+0.2536+0.2679] Train:[90.32] val:[91.38] Test:[90.75] | Update Test:[co:90.50,c:24.25,o:90.44] at Epoch:[15] | lr:0.001852
BIAS:[0.50] | Model:[CausalGAT] Epoch:[19/100] Loss:[0.3848=0.0001+0.2508+0.2679] Train:[90.34] val:[89.25] Test:[89.56] | Update Test:[co:90.50,c:24.25,o:90.44] at Epoch:[15] | lr:0.001836
BIAS:[0.50] | Model:[CausalGAT] Epoch:[20/100] Loss:[0.4007=0.0002+0.2617+0.2776] Train:[89.88] val:[89.75] Test:[90.94] | Update Test:[co:90.50,c:24.25,o:90.44] at Epoch:[15] | lr:0.001819
BIAS:[0.50] | Model:[CausalGAT] Epoch:[21/100] Loss:[0.3798=0.0001+0.2504+0.2587] Train:[90.45] val:[91.75] Test:[91.50] | Update Test:[co:90.50,c:24.25,o:90.44] at Epoch:[15] | lr:0.001801
BIAS:[0.50] | Model:[CausalGAT] Epoch:[22/100] Loss:[0.3796=0.0001+0.2492+0.2607] Train:[90.29] val:[92.25] Test:[92.88] | Update Test:[co:93.44,c:24.94,o:92.88] at Epoch:[22] | lr:0.001782
BIAS:[0.50] | Model:[CausalGAT] Epoch:[23/100] Loss:[0.3655=0.0001+0.2399+0.2511] Train:[90.27] val:[92.88] Test:[90.75] | Update Test:[co:91.62,c:20.69,o:90.75] at Epoch:[23] | lr:0.001763
BIAS:[0.50] | Model:[CausalGAT] Epoch:[24/100] Loss:[0.3588=0.0001+0.2354+0.2466] Train:[91.38] val:[87.12] Test:[86.19] | Update Test:[co:91.62,c:20.69,o:90.75] at Epoch:[23] | lr:0.001743
BIAS:[0.50] | Model:[CausalGAT] Epoch:[25/100] Loss:[0.3551=0.0001+0.2341+0.2419] Train:[91.16] val:[93.00] Test:[92.94] | Update Test:[co:92.94,c:24.81,o:92.94] at Epoch:[25] | lr:0.001722
BIAS:[0.50] | Model:[CausalGAT] Epoch:[26/100] Loss:[0.3408=0.0001+0.2237+0.2342] Train:[91.62] val:[90.38] Test:[90.81] | Update Test:[co:92.94,c:24.81,o:92.94] at Epoch:[25] | lr:0.001700
BIAS:[0.50] | Model:[CausalGAT] Epoch:[27/100] Loss:[0.3490=0.0002+0.2305+0.2368] Train:[91.02] val:[90.75] Test:[90.12] | Update Test:[co:92.94,c:24.81,o:92.94] at Epoch:[25] | lr:0.001678
BIAS:[0.50] | Model:[CausalGAT] Epoch:[28/100] Loss:[0.3332=0.0001+0.2187+0.2289] Train:[91.91] val:[89.00] Test:[88.75] | Update Test:[co:92.94,c:24.81,o:92.94] at Epoch:[25] | lr:0.001656
BIAS:[0.50] | Model:[CausalGAT] Epoch:[29/100] Loss:[0.3092=0.0001+0.2036+0.2111] Train:[92.41] val:[89.75] Test:[88.88] | Update Test:[co:92.94,c:24.81,o:92.94] at Epoch:[25] | lr:0.001632
BIAS:[0.50] | Model:[CausalGAT] Epoch:[30/100] Loss:[0.3313=0.0001+0.2169+0.2285] Train:[91.96] val:[94.50] Test:[92.94] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001608
BIAS:[0.50] | Model:[CausalGAT] Epoch:[31/100] Loss:[0.3053=0.0001+0.1999+0.2107] Train:[92.84] val:[80.88] Test:[82.75] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001584
BIAS:[0.50] | Model:[CausalGAT] Epoch:[32/100] Loss:[0.3147=0.0001+0.2072+0.2150] Train:[92.29] val:[90.25] Test:[90.25] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001559
BIAS:[0.50] | Model:[CausalGAT] Epoch:[33/100] Loss:[0.3142=0.0001+0.2067+0.2149] Train:[91.96] val:[90.62] Test:[89.94] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001534
BIAS:[0.50] | Model:[CausalGAT] Epoch:[34/100] Loss:[0.2758=0.0002+0.1806+0.1904] Train:[93.16] val:[90.88] Test:[89.44] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001508
BIAS:[0.50] | Model:[CausalGAT] Epoch:[35/100] Loss:[0.2703=0.0001+0.1777+0.1852] Train:[93.43] val:[90.00] Test:[90.31] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001481
BIAS:[0.50] | Model:[CausalGAT] Epoch:[36/100] Loss:[0.2755=0.0001+0.1798+0.1912] Train:[93.52] val:[88.88] Test:[88.50] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001454
BIAS:[0.50] | Model:[CausalGAT] Epoch:[37/100] Loss:[0.2796=0.0001+0.1836+0.1919] Train:[93.04] val:[88.75] Test:[89.31] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001427
BIAS:[0.50] | Model:[CausalGAT] Epoch:[38/100] Loss:[0.2673=0.0001+0.1756+0.1835] Train:[93.25] val:[91.88] Test:[90.31] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001400
BIAS:[0.50] | Model:[CausalGAT] Epoch:[39/100] Loss:[0.2413=0.0001+0.1590+0.1647] Train:[94.34] val:[89.00] Test:[90.25] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001372
BIAS:[0.50] | Model:[CausalGAT] Epoch:[40/100] Loss:[0.2581=0.0001+0.1683+0.1795] Train:[94.20] val:[87.75] Test:[88.19] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001344
BIAS:[0.50] | Model:[CausalGAT] Epoch:[41/100] Loss:[0.2506=0.0001+0.1643+0.1726] Train:[94.29] val:[84.38] Test:[85.19] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001315
BIAS:[0.50] | Model:[CausalGAT] Epoch:[42/100] Loss:[0.2212=0.0001+0.1443+0.1537] Train:[95.12] val:[82.50] Test:[82.94] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001286
BIAS:[0.50] | Model:[CausalGAT] Epoch:[43/100] Loss:[0.2210=0.0001+0.1436+0.1548] Train:[94.91] val:[83.12] Test:[82.62] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001257
BIAS:[0.50] | Model:[CausalGAT] Epoch:[44/100] Loss:[0.2351=0.0001+0.1541+0.1618] Train:[94.52] val:[91.75] Test:[91.25] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001228
BIAS:[0.50] | Model:[CausalGAT] Epoch:[45/100] Loss:[0.2390=0.0001+0.1570+0.1641] Train:[94.25] val:[86.50] Test:[86.62] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001199
BIAS:[0.50] | Model:[CausalGAT] Epoch:[46/100] Loss:[0.2118=0.0001+0.1363+0.1509] Train:[95.16] val:[75.25] Test:[75.19] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001169
BIAS:[0.50] | Model:[CausalGAT] Epoch:[47/100] Loss:[0.2037=0.0001+0.1330+0.1414] Train:[95.23] val:[93.50] Test:[93.25] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001139
BIAS:[0.50] | Model:[CausalGAT] Epoch:[48/100] Loss:[0.2081=0.0001+0.1365+0.1432] Train:[95.45] val:[88.50] Test:[88.69] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001110
BIAS:[0.50] | Model:[CausalGAT] Epoch:[49/100] Loss:[0.1959=0.0000+0.1285+0.1347] Train:[95.86] val:[84.12] Test:[84.06] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001080
BIAS:[0.50] | Model:[CausalGAT] Epoch:[50/100] Loss:[0.1988=0.0000+0.1301+0.1374] Train:[95.36] val:[85.50] Test:[87.12] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001050
BIAS:[0.50] | Model:[CausalGAT] Epoch:[51/100] Loss:[0.1657=0.0000+0.1094+0.1126] Train:[96.21] val:[76.12] Test:[75.56] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.001020
BIAS:[0.50] | Model:[CausalGAT] Epoch:[52/100] Loss:[0.1727=0.0000+0.1130+0.1193] Train:[95.89] val:[77.88] Test:[78.69] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.000990
BIAS:[0.50] | Model:[CausalGAT] Epoch:[53/100] Loss:[0.1596=0.0000+0.1048+0.1097] Train:[96.32] val:[91.62] Test:[92.25] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.000961
BIAS:[0.50] | Model:[CausalGAT] Epoch:[54/100] Loss:[0.1548=0.0001+0.1009+0.1079] Train:[96.59] val:[85.38] Test:[84.69] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.000931
BIAS:[0.50] | Model:[CausalGAT] Epoch:[55/100] Loss:[0.1534=0.0001+0.0997+0.1075] Train:[96.55] val:[87.50] Test:[87.06] | Update Test:[co:92.12,c:19.44,o:92.94] at Epoch:[30] | lr:0.000901
BIAS:[0.50] | Model:[CausalGAT] Epoch:[56/100] Loss:[0.1501=0.0001+0.0974+0.1052] Train:[96.61] val:[96.62] Test:[96.81] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000872
BIAS:[0.50] | Model:[CausalGAT] Epoch:[57/100] Loss:[0.1425=0.0000+0.0923+0.1003] Train:[97.04] val:[94.88] Test:[94.56] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000843
BIAS:[0.50] | Model:[CausalGAT] Epoch:[58/100] Loss:[0.1267=0.0000+0.0816+0.0903] Train:[97.07] val:[82.25] Test:[82.50] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000814
BIAS:[0.50] | Model:[CausalGAT] Epoch:[59/100] Loss:[0.1373=0.0000+0.0890+0.0966] Train:[96.98] val:[91.75] Test:[91.88] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000785
BIAS:[0.50] | Model:[CausalGAT] Epoch:[60/100] Loss:[0.1404=0.0000+0.0917+0.0974] Train:[96.79] val:[78.12] Test:[77.88] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000756
BIAS:[0.50] | Model:[CausalGAT] Epoch:[61/100] Loss:[0.1373=0.0000+0.0896+0.0953] Train:[96.71] val:[85.00] Test:[84.56] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000728
BIAS:[0.50] | Model:[CausalGAT] Epoch:[62/100] Loss:[0.1308=0.0000+0.0850+0.0915] Train:[97.09] val:[87.62] Test:[88.12] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000700
BIAS:[0.50] | Model:[CausalGAT] Epoch:[63/100] Loss:[0.1327=0.0001+0.0871+0.0912] Train:[96.82] val:[85.00] Test:[84.94] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000673
BIAS:[0.50] | Model:[CausalGAT] Epoch:[64/100] Loss:[0.1127=0.0000+0.0725+0.0804] Train:[97.62] val:[84.88] Test:[84.38] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000646
BIAS:[0.50] | Model:[CausalGAT] Epoch:[65/100] Loss:[0.1157=0.0000+0.0742+0.0829] Train:[97.48] val:[85.75] Test:[85.69] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000619
BIAS:[0.50] | Model:[CausalGAT] Epoch:[66/100] Loss:[0.1141=0.0000+0.0734+0.0813] Train:[97.32] val:[96.62] Test:[96.62] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000592
BIAS:[0.50] | Model:[CausalGAT] Epoch:[67/100] Loss:[0.1153=0.0000+0.0746+0.0814] Train:[97.45] val:[95.12] Test:[95.00] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000566
BIAS:[0.50] | Model:[CausalGAT] Epoch:[68/100] Loss:[0.1107=0.0000+0.0718+0.0779] Train:[97.36] val:[93.38] Test:[92.94] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000541
BIAS:[0.50] | Model:[CausalGAT] Epoch:[69/100] Loss:[0.1005=0.0000+0.0643+0.0724] Train:[97.64] val:[84.88] Test:[84.56] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000516
BIAS:[0.50] | Model:[CausalGAT] Epoch:[70/100] Loss:[0.1074=0.0000+0.0693+0.0763] Train:[97.88] val:[96.38] Test:[96.50] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000492
BIAS:[0.50] | Model:[CausalGAT] Epoch:[71/100] Loss:[0.1005=0.0000+0.0640+0.0729] Train:[97.68] val:[92.62] Test:[94.00] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000468
BIAS:[0.50] | Model:[CausalGAT] Epoch:[72/100] Loss:[0.0954=0.0000+0.0622+0.0663] Train:[97.88] val:[93.88] Test:[93.50] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000444
BIAS:[0.50] | Model:[CausalGAT] Epoch:[73/100] Loss:[0.0932=0.0000+0.0598+0.0667] Train:[97.95] val:[92.38] Test:[90.75] | Update Test:[co:96.38,c:24.25,o:96.81] at Epoch:[56] | lr:0.000422
BIAS:[0.50] | Model:[CausalGAT] Epoch:[74/100] Loss:[0.0922=0.0000+0.0595+0.0655] Train:[97.98] val:[97.50] Test:[96.88] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000400
BIAS:[0.50] | Model:[CausalGAT] Epoch:[75/100] Loss:[0.0950=0.0000+0.0620+0.0661] Train:[97.84] val:[86.12] Test:[85.62] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000378
BIAS:[0.50] | Model:[CausalGAT] Epoch:[76/100] Loss:[0.0842=0.0000+0.0534+0.0615] Train:[98.21] val:[96.50] Test:[96.88] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000357
BIAS:[0.50] | Model:[CausalGAT] Epoch:[77/100] Loss:[0.0922=0.0000+0.0596+0.0652] Train:[98.14] val:[94.62] Test:[95.12] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000337
BIAS:[0.50] | Model:[CausalGAT] Epoch:[78/100] Loss:[0.0890=0.0000+0.0574+0.0630] Train:[98.04] val:[85.88] Test:[85.44] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000318
BIAS:[0.50] | Model:[CausalGAT] Epoch:[79/100] Loss:[0.0836=0.0000+0.0542+0.0587] Train:[98.09] val:[95.88] Test:[96.38] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000299
BIAS:[0.50] | Model:[CausalGAT] Epoch:[80/100] Loss:[0.0861=0.0000+0.0555+0.0612] Train:[98.04] val:[97.12] Test:[96.88] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000281
BIAS:[0.50] | Model:[CausalGAT] Epoch:[81/100] Loss:[0.0837=0.0000+0.0537+0.0599] Train:[98.29] val:[96.12] Test:[95.75] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000264
BIAS:[0.50] | Model:[CausalGAT] Epoch:[82/100] Loss:[0.0688=0.0000+0.0440+0.0496] Train:[98.55] val:[96.00] Test:[95.69] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000248
BIAS:[0.50] | Model:[CausalGAT] Epoch:[83/100] Loss:[0.0830=0.0000+0.0538+0.0583] Train:[98.12] val:[97.25] Test:[97.00] | Update Test:[co:97.00,c:23.44,o:96.88] at Epoch:[74] | lr:0.000232
BIAS:[0.50] | Model:[CausalGAT] Epoch:[84/100] Loss:[0.0867=0.0000+0.0562+0.0611] Train:[98.07] val:[97.75] Test:[97.44] | Update Test:[co:97.38,c:22.50,o:97.44] at Epoch:[84] | lr:0.000218
BIAS:[0.50] | Model:[CausalGAT] Epoch:[85/100] Loss:[0.0790=0.0000+0.0513+0.0553] Train:[98.27] val:[97.12] Test:[96.94] | Update Test:[co:97.38,c:22.50,o:97.44] at Epoch:[84] | lr:0.000204
BIAS:[0.50] | Model:[CausalGAT] Epoch:[86/100] Loss:[0.0759=0.0000+0.0495+0.0529] Train:[98.32] val:[97.50] Test:[96.94] | Update Test:[co:97.38,c:22.50,o:97.44] at Epoch:[84] | lr:0.000190
BIAS:[0.50] | Model:[CausalGAT] Epoch:[87/100] Loss:[0.0841=0.0000+0.0545+0.0594] Train:[98.25] val:[96.75] Test:[96.81] | Update Test:[co:97.38,c:22.50,o:97.44] at Epoch:[84] | lr:0.000178
BIAS:[0.50] | Model:[CausalGAT] Epoch:[88/100] Loss:[0.0774=0.0000+0.0502+0.0544] Train:[98.12] val:[97.88] Test:[97.31] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000167
BIAS:[0.50] | Model:[CausalGAT] Epoch:[89/100] Loss:[0.0673=0.0000+0.0431+0.0484] Train:[98.59] val:[96.75] Test:[97.12] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000156
BIAS:[0.50] | Model:[CausalGAT] Epoch:[90/100] Loss:[0.0747=0.0000+0.0477+0.0540] Train:[98.23] val:[95.50] Test:[96.81] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000146
BIAS:[0.50] | Model:[CausalGAT] Epoch:[91/100] Loss:[0.0700=0.0000+0.0450+0.0498] Train:[98.52] val:[96.00] Test:[96.88] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000138
BIAS:[0.50] | Model:[CausalGAT] Epoch:[92/100] Loss:[0.0714=0.0000+0.0465+0.0497] Train:[98.25] val:[97.75] Test:[97.12] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000130
BIAS:[0.50] | Model:[CausalGAT] Epoch:[93/100] Loss:[0.0708=0.0000+0.0452+0.0512] Train:[98.55] val:[96.75] Test:[96.75] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000123
BIAS:[0.50] | Model:[CausalGAT] Epoch:[94/100] Loss:[0.0672=0.0000+0.0434+0.0476] Train:[98.30] val:[95.88] Test:[96.62] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000117
BIAS:[0.50] | Model:[CausalGAT] Epoch:[95/100] Loss:[0.0726=0.0000+0.0469+0.0514] Train:[98.25] val:[95.88] Test:[96.38] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000112
BIAS:[0.50] | Model:[CausalGAT] Epoch:[96/100] Loss:[0.0713=0.0000+0.0463+0.0501] Train:[98.54] val:[96.00] Test:[96.50] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000107
BIAS:[0.50] | Model:[CausalGAT] Epoch:[97/100] Loss:[0.0689=0.0000+0.0446+0.0486] Train:[98.38] val:[97.00] Test:[96.88] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000104
BIAS:[0.50] | Model:[CausalGAT] Epoch:[98/100] Loss:[0.0627=0.0000+0.0405+0.0443] Train:[98.61] val:[96.00] Test:[96.56] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000102
BIAS:[0.50] | Model:[CausalGAT] Epoch:[99/100] Loss:[0.0766=0.0000+0.0495+0.0541] Train:[98.30] val:[96.50] Test:[96.56] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000100
BIAS:[0.50] | Model:[CausalGAT] Epoch:[100/100] Loss:[0.0762=0.0000+0.0493+0.0538] Train:[98.25] val:[97.12] Test:[97.00] | Update Test:[co:97.31,c:23.69,o:97.31] at Epoch:[88] | lr:0.000100
syd: BIAS:[0.50] | Val acc:[97.12] Test acc:[co:97.31,c:23.69,o:97.31] at epoch:[88]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.7
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GAT
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:979  , Cycle:420  , Grids:420  , Diams:420   
| BA  : House:420  , Cycle:979  , Grids:979  , Diams:979   
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:70.0%, Cycle:30.0%, Grids:30.0%, Diams:30.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:140  , Cycle:60   , Grids:60   , Diams:60    
| BA  : House:60   , Cycle:140  , Grids:140  , Diams:140   
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:70.0%, Cycle:30.0%, Grids:30.0%, Diams:30.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.70] | Model:[GAT] Epoch:[1/100] Loss:[0.9543] Train:[59.01] val:[62.00] Test:[63.00] | Best Val:[62.00] Update Test:[63.00] at Epoch:[1] | lr:0.002000
BIAS:[0.70] | Model:[GAT] Epoch:[2/100] Loss:[0.6101] Train:[74.82] val:[77.25] Test:[80.38] | Best Val:[77.25] Update Test:[80.38] at Epoch:[2] | lr:0.001998
BIAS:[0.70] | Model:[GAT] Epoch:[3/100] Loss:[0.5533] Train:[77.54] val:[81.62] Test:[84.88] | Best Val:[81.62] Update Test:[84.88] at Epoch:[3] | lr:0.001996
BIAS:[0.70] | Model:[GAT] Epoch:[4/100] Loss:[0.4986] Train:[80.25] val:[84.12] Test:[83.06] | Best Val:[84.12] Update Test:[83.06] at Epoch:[4] | lr:0.001993
BIAS:[0.70] | Model:[GAT] Epoch:[5/100] Loss:[0.4536] Train:[81.63] val:[85.00] Test:[85.19] | Best Val:[85.00] Update Test:[85.19] at Epoch:[5] | lr:0.001988
BIAS:[0.70] | Model:[GAT] Epoch:[6/100] Loss:[0.4709] Train:[80.59] val:[83.38] Test:[84.12] | Best Val:[85.00] Update Test:[85.19] at Epoch:[5] | lr:0.001983
BIAS:[0.70] | Model:[GAT] Epoch:[7/100] Loss:[0.4370] Train:[82.63] val:[81.12] Test:[81.25] | Best Val:[85.00] Update Test:[85.19] at Epoch:[5] | lr:0.001977
BIAS:[0.70] | Model:[GAT] Epoch:[8/100] Loss:[0.4109] Train:[83.74] val:[86.12] Test:[85.31] | Best Val:[86.12] Update Test:[85.31] at Epoch:[8] | lr:0.001970
BIAS:[0.70] | Model:[GAT] Epoch:[9/100] Loss:[0.4111] Train:[83.81] val:[86.62] Test:[86.44] | Best Val:[86.62] Update Test:[86.44] at Epoch:[9] | lr:0.001962
BIAS:[0.70] | Model:[GAT] Epoch:[10/100] Loss:[0.4019] Train:[83.60] val:[84.62] Test:[87.50] | Best Val:[86.62] Update Test:[86.44] at Epoch:[9] | lr:0.001954
BIAS:[0.70] | Model:[GAT] Epoch:[11/100] Loss:[0.3926] Train:[83.99] val:[86.25] Test:[85.50] | Best Val:[86.62] Update Test:[86.44] at Epoch:[9] | lr:0.001944
BIAS:[0.70] | Model:[GAT] Epoch:[12/100] Loss:[0.3973] Train:[83.63] val:[86.12] Test:[86.38] | Best Val:[86.62] Update Test:[86.44] at Epoch:[9] | lr:0.001933
BIAS:[0.70] | Model:[GAT] Epoch:[13/100] Loss:[0.4019] Train:[83.51] val:[87.75] Test:[87.12] | Best Val:[87.75] Update Test:[87.12] at Epoch:[13] | lr:0.001922
BIAS:[0.70] | Model:[GAT] Epoch:[14/100] Loss:[0.3791] Train:[85.35] val:[84.00] Test:[86.62] | Best Val:[87.75] Update Test:[87.12] at Epoch:[13] | lr:0.001910
BIAS:[0.70] | Model:[GAT] Epoch:[15/100] Loss:[0.3827] Train:[84.36] val:[84.88] Test:[87.62] | Best Val:[87.75] Update Test:[87.12] at Epoch:[13] | lr:0.001896
BIAS:[0.70] | Model:[GAT] Epoch:[16/100] Loss:[0.3848] Train:[84.61] val:[85.50] Test:[88.12] | Best Val:[87.75] Update Test:[87.12] at Epoch:[13] | lr:0.001882
BIAS:[0.70] | Model:[GAT] Epoch:[17/100] Loss:[0.3544] Train:[85.74] val:[86.88] Test:[88.25] | Best Val:[87.75] Update Test:[87.12] at Epoch:[13] | lr:0.001868
BIAS:[0.70] | Model:[GAT] Epoch:[18/100] Loss:[0.3398] Train:[86.20] val:[88.50] Test:[87.56] | Best Val:[88.50] Update Test:[87.56] at Epoch:[18] | lr:0.001852
BIAS:[0.70] | Model:[GAT] Epoch:[19/100] Loss:[0.3625] Train:[86.06] val:[88.38] Test:[87.00] | Best Val:[88.50] Update Test:[87.56] at Epoch:[18] | lr:0.001836
BIAS:[0.70] | Model:[GAT] Epoch:[20/100] Loss:[0.3697] Train:[85.08] val:[89.88] Test:[89.50] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001819
BIAS:[0.70] | Model:[GAT] Epoch:[21/100] Loss:[0.3533] Train:[86.10] val:[88.50] Test:[88.38] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001801
BIAS:[0.70] | Model:[GAT] Epoch:[22/100] Loss:[0.3340] Train:[87.10] val:[88.50] Test:[88.19] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001782
BIAS:[0.70] | Model:[GAT] Epoch:[23/100] Loss:[0.3333] Train:[86.85] val:[88.38] Test:[87.94] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001763
BIAS:[0.70] | Model:[GAT] Epoch:[24/100] Loss:[0.3268] Train:[87.31] val:[89.12] Test:[88.56] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001743
BIAS:[0.70] | Model:[GAT] Epoch:[25/100] Loss:[0.3376] Train:[86.58] val:[88.12] Test:[87.25] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001722
BIAS:[0.70] | Model:[GAT] Epoch:[26/100] Loss:[0.3336] Train:[86.90] val:[86.88] Test:[87.62] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001700
BIAS:[0.70] | Model:[GAT] Epoch:[27/100] Loss:[0.3341] Train:[87.03] val:[88.38] Test:[89.75] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001678
BIAS:[0.70] | Model:[GAT] Epoch:[28/100] Loss:[0.3200] Train:[87.40] val:[85.75] Test:[87.81] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001656
BIAS:[0.70] | Model:[GAT] Epoch:[29/100] Loss:[0.3255] Train:[87.04] val:[88.88] Test:[88.75] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001632
BIAS:[0.70] | Model:[GAT] Epoch:[30/100] Loss:[0.3240] Train:[87.03] val:[89.38] Test:[89.38] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001608
BIAS:[0.70] | Model:[GAT] Epoch:[31/100] Loss:[0.3070] Train:[88.38] val:[89.12] Test:[90.19] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001584
BIAS:[0.70] | Model:[GAT] Epoch:[32/100] Loss:[0.3040] Train:[87.90] val:[88.62] Test:[90.81] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001559
BIAS:[0.70] | Model:[GAT] Epoch:[33/100] Loss:[0.3083] Train:[88.21] val:[89.62] Test:[89.62] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001534
BIAS:[0.70] | Model:[GAT] Epoch:[34/100] Loss:[0.3161] Train:[87.72] val:[89.12] Test:[91.25] | Best Val:[89.88] Update Test:[89.50] at Epoch:[20] | lr:0.001508
BIAS:[0.70] | Model:[GAT] Epoch:[35/100] Loss:[0.3007] Train:[88.44] val:[90.62] Test:[90.44] | Best Val:[90.62] Update Test:[90.44] at Epoch:[35] | lr:0.001481
BIAS:[0.70] | Model:[GAT] Epoch:[36/100] Loss:[0.2912] Train:[88.78] val:[90.88] Test:[90.94] | Best Val:[90.88] Update Test:[90.94] at Epoch:[36] | lr:0.001454
BIAS:[0.70] | Model:[GAT] Epoch:[37/100] Loss:[0.2976] Train:[88.42] val:[90.62] Test:[90.31] | Best Val:[90.88] Update Test:[90.94] at Epoch:[36] | lr:0.001427
BIAS:[0.70] | Model:[GAT] Epoch:[38/100] Loss:[0.2785] Train:[89.53] val:[88.88] Test:[91.19] | Best Val:[90.88] Update Test:[90.94] at Epoch:[36] | lr:0.001400
BIAS:[0.70] | Model:[GAT] Epoch:[39/100] Loss:[0.2748] Train:[89.19] val:[90.50] Test:[91.38] | Best Val:[90.88] Update Test:[90.94] at Epoch:[36] | lr:0.001372
BIAS:[0.70] | Model:[GAT] Epoch:[40/100] Loss:[0.2806] Train:[88.78] val:[90.12] Test:[89.69] | Best Val:[90.88] Update Test:[90.94] at Epoch:[36] | lr:0.001344
BIAS:[0.70] | Model:[GAT] Epoch:[41/100] Loss:[0.2940] Train:[88.58] val:[86.50] Test:[87.94] | Best Val:[90.88] Update Test:[90.94] at Epoch:[36] | lr:0.001315
BIAS:[0.70] | Model:[GAT] Epoch:[42/100] Loss:[0.2858] Train:[89.12] val:[91.00] Test:[90.81] | Best Val:[91.00] Update Test:[90.81] at Epoch:[42] | lr:0.001286
BIAS:[0.70] | Model:[GAT] Epoch:[43/100] Loss:[0.2779] Train:[89.05] val:[90.88] Test:[90.94] | Best Val:[91.00] Update Test:[90.81] at Epoch:[42] | lr:0.001257
BIAS:[0.70] | Model:[GAT] Epoch:[44/100] Loss:[0.2727] Train:[89.14] val:[87.50] Test:[89.88] | Best Val:[91.00] Update Test:[90.81] at Epoch:[42] | lr:0.001228
BIAS:[0.70] | Model:[GAT] Epoch:[45/100] Loss:[0.2649] Train:[89.62] val:[90.50] Test:[89.19] | Best Val:[91.00] Update Test:[90.81] at Epoch:[42] | lr:0.001199
BIAS:[0.70] | Model:[GAT] Epoch:[46/100] Loss:[0.2555] Train:[90.01] val:[89.00] Test:[89.94] | Best Val:[91.00] Update Test:[90.81] at Epoch:[42] | lr:0.001169
BIAS:[0.70] | Model:[GAT] Epoch:[47/100] Loss:[0.2657] Train:[89.74] val:[91.38] Test:[91.31] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.001139
BIAS:[0.70] | Model:[GAT] Epoch:[48/100] Loss:[0.2669] Train:[90.03] val:[89.12] Test:[90.94] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.001110
BIAS:[0.70] | Model:[GAT] Epoch:[49/100] Loss:[0.2579] Train:[89.96] val:[91.38] Test:[89.69] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.001080
BIAS:[0.70] | Model:[GAT] Epoch:[50/100] Loss:[0.2545] Train:[90.05] val:[89.50] Test:[91.38] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.001050
BIAS:[0.70] | Model:[GAT] Epoch:[51/100] Loss:[0.2581] Train:[89.89] val:[91.38] Test:[92.19] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.001020
BIAS:[0.70] | Model:[GAT] Epoch:[52/100] Loss:[0.2536] Train:[90.21] val:[87.75] Test:[88.69] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.000990
BIAS:[0.70] | Model:[GAT] Epoch:[53/100] Loss:[0.2677] Train:[89.78] val:[89.25] Test:[91.06] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.000961
BIAS:[0.70] | Model:[GAT] Epoch:[54/100] Loss:[0.2684] Train:[89.56] val:[90.88] Test:[91.00] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.000931
BIAS:[0.70] | Model:[GAT] Epoch:[55/100] Loss:[0.2549] Train:[90.17] val:[90.88] Test:[90.88] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.000901
BIAS:[0.70] | Model:[GAT] Epoch:[56/100] Loss:[0.2507] Train:[90.40] val:[91.12] Test:[90.81] | Best Val:[91.38] Update Test:[91.31] at Epoch:[47] | lr:0.000872
BIAS:[0.70] | Model:[GAT] Epoch:[57/100] Loss:[0.2504] Train:[90.35] val:[92.00] Test:[91.81] | Best Val:[92.00] Update Test:[91.81] at Epoch:[57] | lr:0.000843
BIAS:[0.70] | Model:[GAT] Epoch:[58/100] Loss:[0.2506] Train:[90.23] val:[91.25] Test:[91.50] | Best Val:[92.00] Update Test:[91.81] at Epoch:[57] | lr:0.000814
BIAS:[0.70] | Model:[GAT] Epoch:[59/100] Loss:[0.2463] Train:[90.49] val:[91.38] Test:[90.50] | Best Val:[92.00] Update Test:[91.81] at Epoch:[57] | lr:0.000785
BIAS:[0.70] | Model:[GAT] Epoch:[60/100] Loss:[0.2495] Train:[90.40] val:[90.50] Test:[91.31] | Best Val:[92.00] Update Test:[91.81] at Epoch:[57] | lr:0.000756
BIAS:[0.70] | Model:[GAT] Epoch:[61/100] Loss:[0.2439] Train:[90.62] val:[91.75] Test:[91.75] | Best Val:[92.00] Update Test:[91.81] at Epoch:[57] | lr:0.000728
BIAS:[0.70] | Model:[GAT] Epoch:[62/100] Loss:[0.2424] Train:[90.83] val:[92.38] Test:[91.38] | Best Val:[92.38] Update Test:[91.38] at Epoch:[62] | lr:0.000700
BIAS:[0.70] | Model:[GAT] Epoch:[63/100] Loss:[0.2287] Train:[91.17] val:[90.75] Test:[91.50] | Best Val:[92.38] Update Test:[91.38] at Epoch:[62] | lr:0.000673
BIAS:[0.70] | Model:[GAT] Epoch:[64/100] Loss:[0.2257] Train:[91.37] val:[91.75] Test:[91.00] | Best Val:[92.38] Update Test:[91.38] at Epoch:[62] | lr:0.000646
BIAS:[0.70] | Model:[GAT] Epoch:[65/100] Loss:[0.2339] Train:[90.76] val:[92.75] Test:[91.62] | Best Val:[92.75] Update Test:[91.62] at Epoch:[65] | lr:0.000619
BIAS:[0.70] | Model:[GAT] Epoch:[66/100] Loss:[0.2414] Train:[90.39] val:[89.12] Test:[91.31] | Best Val:[92.75] Update Test:[91.62] at Epoch:[65] | lr:0.000592
BIAS:[0.70] | Model:[GAT] Epoch:[67/100] Loss:[0.2368] Train:[90.48] val:[91.38] Test:[91.56] | Best Val:[92.75] Update Test:[91.62] at Epoch:[65] | lr:0.000566
BIAS:[0.70] | Model:[GAT] Epoch:[68/100] Loss:[0.2214] Train:[91.53] val:[90.88] Test:[92.31] | Best Val:[92.75] Update Test:[91.62] at Epoch:[65] | lr:0.000541
BIAS:[0.70] | Model:[GAT] Epoch:[69/100] Loss:[0.2203] Train:[91.74] val:[90.88] Test:[90.94] | Best Val:[92.75] Update Test:[91.62] at Epoch:[65] | lr:0.000516
BIAS:[0.70] | Model:[GAT] Epoch:[70/100] Loss:[0.2154] Train:[91.69] val:[91.25] Test:[91.38] | Best Val:[92.75] Update Test:[91.62] at Epoch:[65] | lr:0.000492
BIAS:[0.70] | Model:[GAT] Epoch:[71/100] Loss:[0.2188] Train:[91.78] val:[90.62] Test:[90.81] | Best Val:[92.75] Update Test:[91.62] at Epoch:[65] | lr:0.000468
BIAS:[0.70] | Model:[GAT] Epoch:[72/100] Loss:[0.2131] Train:[91.87] val:[93.25] Test:[92.62] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000444
BIAS:[0.70] | Model:[GAT] Epoch:[73/100] Loss:[0.2181] Train:[91.62] val:[92.25] Test:[91.62] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000422
BIAS:[0.70] | Model:[GAT] Epoch:[74/100] Loss:[0.2141] Train:[91.71] val:[91.38] Test:[92.06] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000400
BIAS:[0.70] | Model:[GAT] Epoch:[75/100] Loss:[0.2054] Train:[92.14] val:[91.38] Test:[91.38] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000378
BIAS:[0.70] | Model:[GAT] Epoch:[76/100] Loss:[0.2015] Train:[92.26] val:[92.25] Test:[91.88] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000357
BIAS:[0.70] | Model:[GAT] Epoch:[77/100] Loss:[0.2071] Train:[92.01] val:[92.00] Test:[91.75] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000337
BIAS:[0.70] | Model:[GAT] Epoch:[78/100] Loss:[0.2066] Train:[92.16] val:[90.75] Test:[91.81] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000318
BIAS:[0.70] | Model:[GAT] Epoch:[79/100] Loss:[0.2065] Train:[92.35] val:[92.25] Test:[92.50] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000299
BIAS:[0.70] | Model:[GAT] Epoch:[80/100] Loss:[0.2189] Train:[91.71] val:[92.88] Test:[92.19] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000281
BIAS:[0.70] | Model:[GAT] Epoch:[81/100] Loss:[0.1995] Train:[92.37] val:[92.25] Test:[92.56] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000264
BIAS:[0.70] | Model:[GAT] Epoch:[82/100] Loss:[0.2056] Train:[91.87] val:[93.00] Test:[92.50] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000248
BIAS:[0.70] | Model:[GAT] Epoch:[83/100] Loss:[0.2023] Train:[92.66] val:[92.38] Test:[92.31] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000232
BIAS:[0.70] | Model:[GAT] Epoch:[84/100] Loss:[0.2035] Train:[92.01] val:[92.88] Test:[92.69] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000218
BIAS:[0.70] | Model:[GAT] Epoch:[85/100] Loss:[0.1982] Train:[92.35] val:[92.62] Test:[92.44] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000204
BIAS:[0.70] | Model:[GAT] Epoch:[86/100] Loss:[0.2037] Train:[92.21] val:[92.00] Test:[92.62] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000190
BIAS:[0.70] | Model:[GAT] Epoch:[87/100] Loss:[0.2007] Train:[92.19] val:[92.00] Test:[92.38] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000178
BIAS:[0.70] | Model:[GAT] Epoch:[88/100] Loss:[0.1914] Train:[92.64] val:[91.62] Test:[91.81] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000167
BIAS:[0.70] | Model:[GAT] Epoch:[89/100] Loss:[0.2000] Train:[92.03] val:[92.12] Test:[92.31] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000156
BIAS:[0.70] | Model:[GAT] Epoch:[90/100] Loss:[0.2037] Train:[92.10] val:[92.62] Test:[92.88] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000146
BIAS:[0.70] | Model:[GAT] Epoch:[91/100] Loss:[0.1881] Train:[92.53] val:[91.50] Test:[92.31] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000138
BIAS:[0.70] | Model:[GAT] Epoch:[92/100] Loss:[0.1909] Train:[92.66] val:[92.50] Test:[92.38] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000130
BIAS:[0.70] | Model:[GAT] Epoch:[93/100] Loss:[0.1926] Train:[92.53] val:[92.00] Test:[92.50] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000123
BIAS:[0.70] | Model:[GAT] Epoch:[94/100] Loss:[0.1902] Train:[92.83] val:[91.88] Test:[92.44] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000117
BIAS:[0.70] | Model:[GAT] Epoch:[95/100] Loss:[0.1871] Train:[92.94] val:[91.75] Test:[92.12] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000112
BIAS:[0.70] | Model:[GAT] Epoch:[96/100] Loss:[0.1923] Train:[92.48] val:[92.38] Test:[92.25] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000107
BIAS:[0.70] | Model:[GAT] Epoch:[97/100] Loss:[0.1817] Train:[93.28] val:[92.25] Test:[92.75] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000104
BIAS:[0.70] | Model:[GAT] Epoch:[98/100] Loss:[0.1886] Train:[92.82] val:[92.12] Test:[92.62] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000102
BIAS:[0.70] | Model:[GAT] Epoch:[99/100] Loss:[0.1902] Train:[92.53] val:[92.00] Test:[93.00] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000100
BIAS:[0.70] | Model:[GAT] Epoch:[100/100] Loss:[0.1854] Train:[92.94] val:[91.75] Test:[92.62] | Best Val:[93.25] Update Test:[92.62] at Epoch:[72] | lr:0.000100
syd: BIAS:[0.70] | Best Val acc:[93.25] Test acc:[92.62] at epoch:[72]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.7
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGAT
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:979  , Cycle:420  , Grids:420  , Diams:420   
| BA  : House:420  , Cycle:979  , Grids:979  , Diams:979   
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:70.0%, Cycle:30.0%, Grids:30.0%, Diams:30.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:140  , Cycle:60   , Grids:60   , Diams:60    
| BA  : House:60   , Cycle:140  , Grids:140  , Diams:140   
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:70.0%, Cycle:30.0%, Grids:30.0%, Diams:30.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.70] | Model:[CausalGAT] Epoch:[1/100] Loss:[1.2389=0.0203+0.7908+0.8760] Train:[66.87] val:[71.50] Test:[71.94] | Update Test:[co:55.56,c:25.00,o:71.94] at Epoch:[1] | lr:0.002000
BIAS:[0.70] | Model:[CausalGAT] Epoch:[2/100] Loss:[0.7628=0.0017+0.4973+0.5293] Train:[80.33] val:[79.62] Test:[80.31] | Update Test:[co:79.25,c:24.62,o:80.31] at Epoch:[2] | lr:0.001998
BIAS:[0.70] | Model:[CausalGAT] Epoch:[3/100] Loss:[0.7288=0.0010+0.4802+0.4963] Train:[80.33] val:[83.62] Test:[83.44] | Update Test:[co:81.44,c:25.31,o:83.44] at Epoch:[3] | lr:0.001996
BIAS:[0.70] | Model:[CausalGAT] Epoch:[4/100] Loss:[0.6401=0.0008+0.4179+0.4436] Train:[83.31] val:[83.12] Test:[85.38] | Update Test:[co:81.44,c:25.31,o:83.44] at Epoch:[3] | lr:0.001993
BIAS:[0.70] | Model:[CausalGAT] Epoch:[5/100] Loss:[0.5910=0.0005+0.3898+0.4018] Train:[84.65] val:[85.88] Test:[87.31] | Update Test:[co:86.94,c:27.94,o:87.31] at Epoch:[5] | lr:0.001988
BIAS:[0.70] | Model:[CausalGAT] Epoch:[6/100] Loss:[0.5694=0.0004+0.3724+0.3936] Train:[85.04] val:[82.38] Test:[81.69] | Update Test:[co:86.94,c:27.94,o:87.31] at Epoch:[5] | lr:0.001983
BIAS:[0.70] | Model:[CausalGAT] Epoch:[7/100] Loss:[0.5735=0.0004+0.3770+0.3926] Train:[85.38] val:[88.25] Test:[87.44] | Update Test:[co:87.12,c:25.25,o:87.44] at Epoch:[7] | lr:0.001977
BIAS:[0.70] | Model:[CausalGAT] Epoch:[8/100] Loss:[0.5481=0.0003+0.3615+0.3731] Train:[85.97] val:[88.00] Test:[89.12] | Update Test:[co:87.12,c:25.25,o:87.44] at Epoch:[7] | lr:0.001970
BIAS:[0.70] | Model:[CausalGAT] Epoch:[9/100] Loss:[0.5212=0.0004+0.3421+0.3578] Train:[86.53] val:[90.50] Test:[90.25] | Update Test:[co:89.88,c:24.31,o:90.25] at Epoch:[9] | lr:0.001962
BIAS:[0.70] | Model:[CausalGAT] Epoch:[10/100] Loss:[0.5231=0.0003+0.3433+0.3593] Train:[86.31] val:[85.50] Test:[87.31] | Update Test:[co:89.88,c:24.31,o:90.25] at Epoch:[9] | lr:0.001954
BIAS:[0.70] | Model:[CausalGAT] Epoch:[11/100] Loss:[0.5187=0.0003+0.3413+0.3546] Train:[86.87] val:[88.75] Test:[87.50] | Update Test:[co:89.88,c:24.31,o:90.25] at Epoch:[9] | lr:0.001944
BIAS:[0.70] | Model:[CausalGAT] Epoch:[12/100] Loss:[0.5189=0.0002+0.3421+0.3534] Train:[87.17] val:[91.50] Test:[91.25] | Update Test:[co:90.56,c:24.50,o:91.25] at Epoch:[12] | lr:0.001933
BIAS:[0.70] | Model:[CausalGAT] Epoch:[13/100] Loss:[0.4801=0.0002+0.3134+0.3331] Train:[87.65] val:[85.12] Test:[87.94] | Update Test:[co:90.56,c:24.50,o:91.25] at Epoch:[12] | lr:0.001922
BIAS:[0.70] | Model:[CausalGAT] Epoch:[14/100] Loss:[0.4801=0.0002+0.3153+0.3293] Train:[87.99] val:[89.88] Test:[89.94] | Update Test:[co:90.56,c:24.50,o:91.25] at Epoch:[12] | lr:0.001910
BIAS:[0.70] | Model:[CausalGAT] Epoch:[15/100] Loss:[0.4762=0.0002+0.3119+0.3285] Train:[87.76] val:[86.12] Test:[85.44] | Update Test:[co:90.56,c:24.50,o:91.25] at Epoch:[12] | lr:0.001896
BIAS:[0.70] | Model:[CausalGAT] Epoch:[16/100] Loss:[0.4702=0.0002+0.3077+0.3248] Train:[88.17] val:[91.88] Test:[91.69] | Update Test:[co:90.94,c:25.44,o:91.69] at Epoch:[16] | lr:0.001882
BIAS:[0.70] | Model:[CausalGAT] Epoch:[17/100] Loss:[0.4173=0.0001+0.2713+0.2919] Train:[89.89] val:[84.25] Test:[85.88] | Update Test:[co:90.94,c:25.44,o:91.69] at Epoch:[16] | lr:0.001868
BIAS:[0.70] | Model:[CausalGAT] Epoch:[18/100] Loss:[0.4251=0.0002+0.2769+0.2963] Train:[89.26] val:[92.25] Test:[91.06] | Update Test:[co:92.06,c:25.12,o:91.06] at Epoch:[18] | lr:0.001852
BIAS:[0.70] | Model:[CausalGAT] Epoch:[19/100] Loss:[0.4198=0.0002+0.2746+0.2902] Train:[89.89] val:[86.62] Test:[87.31] | Update Test:[co:92.06,c:25.12,o:91.06] at Epoch:[18] | lr:0.001836
BIAS:[0.70] | Model:[CausalGAT] Epoch:[20/100] Loss:[0.3917=0.0002+0.2549+0.2735] Train:[90.35] val:[89.50] Test:[90.00] | Update Test:[co:92.06,c:25.12,o:91.06] at Epoch:[18] | lr:0.001819
BIAS:[0.70] | Model:[CausalGAT] Epoch:[21/100] Loss:[0.3818=0.0001+0.2488+0.2659] Train:[90.60] val:[86.62] Test:[85.69] | Update Test:[co:92.06,c:25.12,o:91.06] at Epoch:[18] | lr:0.001801
BIAS:[0.70] | Model:[CausalGAT] Epoch:[22/100] Loss:[0.3945=0.0002+0.2568+0.2753] Train:[90.42] val:[86.88] Test:[86.81] | Update Test:[co:92.06,c:25.12,o:91.06] at Epoch:[18] | lr:0.001782
BIAS:[0.70] | Model:[CausalGAT] Epoch:[23/100] Loss:[0.3762=0.0003+0.2470+0.2582] Train:[90.64] val:[90.88] Test:[92.06] | Update Test:[co:92.06,c:25.12,o:91.06] at Epoch:[18] | lr:0.001763
BIAS:[0.70] | Model:[CausalGAT] Epoch:[24/100] Loss:[0.3890=0.0002+0.2535+0.2706] Train:[90.40] val:[92.62] Test:[92.81] | Update Test:[co:93.19,c:24.81,o:92.81] at Epoch:[24] | lr:0.001743
BIAS:[0.70] | Model:[CausalGAT] Epoch:[25/100] Loss:[0.3633=0.0001+0.2386+0.2493] Train:[90.85] val:[80.75] Test:[84.56] | Update Test:[co:93.19,c:24.81,o:92.81] at Epoch:[24] | lr:0.001722
BIAS:[0.70] | Model:[CausalGAT] Epoch:[26/100] Loss:[0.3818=0.0001+0.2513+0.2608] Train:[90.55] val:[84.00] Test:[86.00] | Update Test:[co:93.19,c:24.81,o:92.81] at Epoch:[24] | lr:0.001700
BIAS:[0.70] | Model:[CausalGAT] Epoch:[27/100] Loss:[0.3663=0.0001+0.2405+0.2515] Train:[91.05] val:[89.00] Test:[89.50] | Update Test:[co:93.19,c:24.81,o:92.81] at Epoch:[24] | lr:0.001678
BIAS:[0.70] | Model:[CausalGAT] Epoch:[28/100] Loss:[0.3545=0.0001+0.2325+0.2440] Train:[90.98] val:[74.75] Test:[79.75] | Update Test:[co:93.19,c:24.81,o:92.81] at Epoch:[24] | lr:0.001656
BIAS:[0.70] | Model:[CausalGAT] Epoch:[29/100] Loss:[0.3400=0.0001+0.2219+0.2363] Train:[91.80] val:[75.25] Test:[80.88] | Update Test:[co:93.19,c:24.81,o:92.81] at Epoch:[24] | lr:0.001632
BIAS:[0.70] | Model:[CausalGAT] Epoch:[30/100] Loss:[0.3116=0.0001+0.2037+0.2157] Train:[92.55] val:[78.25] Test:[81.62] | Update Test:[co:93.19,c:24.81,o:92.81] at Epoch:[24] | lr:0.001608
BIAS:[0.70] | Model:[CausalGAT] Epoch:[31/100] Loss:[0.3408=0.0001+0.2236+0.2344] Train:[91.99] val:[91.00] Test:[90.81] | Update Test:[co:93.19,c:24.81,o:92.81] at Epoch:[24] | lr:0.001584
BIAS:[0.70] | Model:[CausalGAT] Epoch:[32/100] Loss:[0.3138=0.0001+0.2046+0.2183] Train:[92.26] val:[92.75] Test:[93.94] | Update Test:[co:93.31,c:23.81,o:93.94] at Epoch:[32] | lr:0.001559
BIAS:[0.70] | Model:[CausalGAT] Epoch:[33/100] Loss:[0.3128=0.0001+0.2039+0.2179] Train:[92.74] val:[78.62] Test:[84.56] | Update Test:[co:93.31,c:23.81,o:93.94] at Epoch:[32] | lr:0.001534
BIAS:[0.70] | Model:[CausalGAT] Epoch:[34/100] Loss:[0.3086=0.0001+0.2007+0.2157] Train:[92.58] val:[74.38] Test:[78.88] | Update Test:[co:93.31,c:23.81,o:93.94] at Epoch:[32] | lr:0.001508
BIAS:[0.70] | Model:[CausalGAT] Epoch:[35/100] Loss:[0.2969=0.0001+0.1933+0.2071] Train:[92.89] val:[74.88] Test:[78.00] | Update Test:[co:93.31,c:23.81,o:93.94] at Epoch:[32] | lr:0.001481
BIAS:[0.70] | Model:[CausalGAT] Epoch:[36/100] Loss:[0.2910=0.0001+0.1892+0.2036] Train:[93.05] val:[85.38] Test:[88.50] | Update Test:[co:93.31,c:23.81,o:93.94] at Epoch:[32] | lr:0.001454
BIAS:[0.70] | Model:[CausalGAT] Epoch:[37/100] Loss:[0.2907=0.0001+0.1914+0.1986] Train:[93.08] val:[71.75] Test:[73.81] | Update Test:[co:93.31,c:23.81,o:93.94] at Epoch:[32] | lr:0.001427
BIAS:[0.70] | Model:[CausalGAT] Epoch:[38/100] Loss:[0.2870=0.0001+0.1887+0.1966] Train:[93.05] val:[92.88] Test:[92.31] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001400
BIAS:[0.70] | Model:[CausalGAT] Epoch:[39/100] Loss:[0.2737=0.0001+0.1794+0.1886] Train:[93.41] val:[76.25] Test:[81.00] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001372
BIAS:[0.70] | Model:[CausalGAT] Epoch:[40/100] Loss:[0.2848=0.0001+0.1856+0.1983] Train:[93.23] val:[73.62] Test:[75.25] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001344
BIAS:[0.70] | Model:[CausalGAT] Epoch:[41/100] Loss:[0.2643=0.0000+0.1733+0.1818] Train:[93.85] val:[76.25] Test:[82.38] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001315
BIAS:[0.70] | Model:[CausalGAT] Epoch:[42/100] Loss:[0.2487=0.0001+0.1616+0.1741] Train:[94.01] val:[76.62] Test:[80.31] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001286
BIAS:[0.70] | Model:[CausalGAT] Epoch:[43/100] Loss:[0.2567=0.0001+0.1685+0.1762] Train:[93.80] val:[83.50] Test:[85.56] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001257
BIAS:[0.70] | Model:[CausalGAT] Epoch:[44/100] Loss:[0.2596=0.0001+0.1709+0.1774] Train:[93.82] val:[87.50] Test:[89.50] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001228
BIAS:[0.70] | Model:[CausalGAT] Epoch:[45/100] Loss:[0.2281=0.0001+0.1496+0.1569] Train:[94.39] val:[76.88] Test:[81.19] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001199
BIAS:[0.70] | Model:[CausalGAT] Epoch:[46/100] Loss:[0.2324=0.0001+0.1517+0.1612] Train:[94.23] val:[75.00] Test:[79.62] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001169
BIAS:[0.70] | Model:[CausalGAT] Epoch:[47/100] Loss:[0.2278=0.0000+0.1494+0.1567] Train:[94.55] val:[86.50] Test:[88.62] | Update Test:[co:91.38,c:25.06,o:92.31] at Epoch:[38] | lr:0.001139
BIAS:[0.70] | Model:[CausalGAT] Epoch:[48/100] Loss:[0.2272=0.0001+0.1483+0.1577] Train:[94.55] val:[93.00] Test:[93.50] | Update Test:[co:90.25,c:24.69,o:93.50] at Epoch:[48] | lr:0.001110
BIAS:[0.70] | Model:[CausalGAT] Epoch:[49/100] Loss:[0.2188=0.0001+0.1432+0.1513] Train:[94.92] val:[89.00] Test:[89.00] | Update Test:[co:90.25,c:24.69,o:93.50] at Epoch:[48] | lr:0.001080
BIAS:[0.70] | Model:[CausalGAT] Epoch:[50/100] Loss:[0.2036=0.0001+0.1334+0.1402] Train:[95.35] val:[92.75] Test:[93.25] | Update Test:[co:90.25,c:24.69,o:93.50] at Epoch:[48] | lr:0.001050
BIAS:[0.70] | Model:[CausalGAT] Epoch:[51/100] Loss:[0.2034=0.0000+0.1332+0.1403] Train:[95.23] val:[76.75] Test:[82.75] | Update Test:[co:90.25,c:24.69,o:93.50] at Epoch:[48] | lr:0.001020
BIAS:[0.70] | Model:[CausalGAT] Epoch:[52/100] Loss:[0.2058=0.0000+0.1353+0.1409] Train:[95.23] val:[75.25] Test:[78.38] | Update Test:[co:90.25,c:24.69,o:93.50] at Epoch:[48] | lr:0.000990
BIAS:[0.70] | Model:[CausalGAT] Epoch:[53/100] Loss:[0.1956=0.0000+0.1286+0.1340] Train:[95.43] val:[92.12] Test:[94.12] | Update Test:[co:90.25,c:24.69,o:93.50] at Epoch:[48] | lr:0.000961
BIAS:[0.70] | Model:[CausalGAT] Epoch:[54/100] Loss:[0.1766=0.0000+0.1156+0.1220] Train:[95.57] val:[97.38] Test:[96.94] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000931
BIAS:[0.70] | Model:[CausalGAT] Epoch:[55/100] Loss:[0.1750=0.0000+0.1137+0.1225] Train:[95.96] val:[74.50] Test:[78.19] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000901
BIAS:[0.70] | Model:[CausalGAT] Epoch:[56/100] Loss:[0.1547=0.0000+0.1001+0.1092] Train:[96.35] val:[84.00] Test:[88.06] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000872
BIAS:[0.70] | Model:[CausalGAT] Epoch:[57/100] Loss:[0.1792=0.0000+0.1166+0.1252] Train:[95.66] val:[76.62] Test:[79.12] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000843
BIAS:[0.70] | Model:[CausalGAT] Epoch:[58/100] Loss:[0.1772=0.0000+0.1162+0.1221] Train:[95.64] val:[76.12] Test:[80.38] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000814
BIAS:[0.70] | Model:[CausalGAT] Epoch:[59/100] Loss:[0.1502=0.0000+0.0978+0.1047] Train:[96.44] val:[76.12] Test:[81.12] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000785
BIAS:[0.70] | Model:[CausalGAT] Epoch:[60/100] Loss:[0.1456=0.0000+0.0950+0.1011] Train:[96.55] val:[79.00] Test:[83.38] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000756
BIAS:[0.70] | Model:[CausalGAT] Epoch:[61/100] Loss:[0.1393=0.0000+0.0915+0.0955] Train:[96.89] val:[77.12] Test:[81.62] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000728
BIAS:[0.70] | Model:[CausalGAT] Epoch:[62/100] Loss:[0.1367=0.0000+0.0888+0.0957] Train:[96.84] val:[95.50] Test:[96.38] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000700
BIAS:[0.70] | Model:[CausalGAT] Epoch:[63/100] Loss:[0.1539=0.0000+0.1002+0.1074] Train:[96.55] val:[80.50] Test:[85.19] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000673
BIAS:[0.70] | Model:[CausalGAT] Epoch:[64/100] Loss:[0.1435=0.0000+0.0931+0.1009] Train:[96.75] val:[77.62] Test:[83.56] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000646
BIAS:[0.70] | Model:[CausalGAT] Epoch:[65/100] Loss:[0.1486=0.0000+0.0966+0.1041] Train:[96.80] val:[78.62] Test:[84.44] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000619
BIAS:[0.70] | Model:[CausalGAT] Epoch:[66/100] Loss:[0.1265=0.0000+0.0824+0.0882] Train:[97.14] val:[78.62] Test:[84.00] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000592
BIAS:[0.70] | Model:[CausalGAT] Epoch:[67/100] Loss:[0.1242=0.0000+0.0806+0.0873] Train:[96.91] val:[77.88] Test:[83.62] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000566
BIAS:[0.70] | Model:[CausalGAT] Epoch:[68/100] Loss:[0.1385=0.0000+0.0893+0.0984] Train:[96.91] val:[77.12] Test:[81.44] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000541
BIAS:[0.70] | Model:[CausalGAT] Epoch:[69/100] Loss:[0.1166=0.0000+0.0761+0.0809] Train:[97.55] val:[92.12] Test:[94.06] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000516
BIAS:[0.70] | Model:[CausalGAT] Epoch:[70/100] Loss:[0.1221=0.0000+0.0790+0.0861] Train:[97.16] val:[91.25] Test:[93.38] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000492
BIAS:[0.70] | Model:[CausalGAT] Epoch:[71/100] Loss:[0.1094=0.0000+0.0712+0.0765] Train:[97.39] val:[85.50] Test:[89.06] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000468
BIAS:[0.70] | Model:[CausalGAT] Epoch:[72/100] Loss:[0.1235=0.0000+0.0802+0.0866] Train:[97.03] val:[95.12] Test:[95.31] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000444
BIAS:[0.70] | Model:[CausalGAT] Epoch:[73/100] Loss:[0.1091=0.0000+0.0711+0.0760] Train:[97.50] val:[79.50] Test:[84.94] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000422
BIAS:[0.70] | Model:[CausalGAT] Epoch:[74/100] Loss:[0.1008=0.0000+0.0639+0.0737] Train:[97.96] val:[84.50] Test:[88.44] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000400
BIAS:[0.70] | Model:[CausalGAT] Epoch:[75/100] Loss:[0.1078=0.0000+0.0700+0.0756] Train:[97.50] val:[96.00] Test:[96.44] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000378
BIAS:[0.70] | Model:[CausalGAT] Epoch:[76/100] Loss:[0.1027=0.0000+0.0665+0.0725] Train:[97.69] val:[79.75] Test:[84.62] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000357
BIAS:[0.70] | Model:[CausalGAT] Epoch:[77/100] Loss:[0.1022=0.0000+0.0655+0.0734] Train:[97.75] val:[96.12] Test:[96.75] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000337
BIAS:[0.70] | Model:[CausalGAT] Epoch:[78/100] Loss:[0.0984=0.0000+0.0634+0.0700] Train:[97.62] val:[79.12] Test:[85.06] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000318
BIAS:[0.70] | Model:[CausalGAT] Epoch:[79/100] Loss:[0.1060=0.0000+0.0682+0.0755] Train:[97.53] val:[82.25] Test:[86.56] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000299
BIAS:[0.70] | Model:[CausalGAT] Epoch:[80/100] Loss:[0.0955=0.0000+0.0614+0.0680] Train:[97.64] val:[80.00] Test:[84.88] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000281
BIAS:[0.70] | Model:[CausalGAT] Epoch:[81/100] Loss:[0.0964=0.0000+0.0616+0.0696] Train:[97.91] val:[97.00] Test:[96.69] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000264
BIAS:[0.70] | Model:[CausalGAT] Epoch:[82/100] Loss:[0.1012=0.0000+0.0656+0.0711] Train:[97.66] val:[96.62] Test:[96.88] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000248
BIAS:[0.70] | Model:[CausalGAT] Epoch:[83/100] Loss:[0.0943=0.0000+0.0613+0.0660] Train:[97.73] val:[96.50] Test:[96.00] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000232
BIAS:[0.70] | Model:[CausalGAT] Epoch:[84/100] Loss:[0.1024=0.0000+0.0661+0.0725] Train:[97.66] val:[94.88] Test:[95.31] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000218
BIAS:[0.70] | Model:[CausalGAT] Epoch:[85/100] Loss:[0.0946=0.0000+0.0611+0.0670] Train:[97.82] val:[95.75] Test:[96.25] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000204
BIAS:[0.70] | Model:[CausalGAT] Epoch:[86/100] Loss:[0.0946=0.0000+0.0616+0.0659] Train:[97.75] val:[96.62] Test:[96.25] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000190
BIAS:[0.70] | Model:[CausalGAT] Epoch:[87/100] Loss:[0.0875=0.0000+0.0564+0.0623] Train:[98.00] val:[95.62] Test:[96.81] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000178
BIAS:[0.70] | Model:[CausalGAT] Epoch:[88/100] Loss:[0.0958=0.0000+0.0621+0.0675] Train:[97.80] val:[97.00] Test:[96.75] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000167
BIAS:[0.70] | Model:[CausalGAT] Epoch:[89/100] Loss:[0.0963=0.0000+0.0618+0.0688] Train:[97.94] val:[96.75] Test:[97.12] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000156
BIAS:[0.70] | Model:[CausalGAT] Epoch:[90/100] Loss:[0.0922=0.0000+0.0591+0.0661] Train:[97.84] val:[96.88] Test:[97.38] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000146
BIAS:[0.70] | Model:[CausalGAT] Epoch:[91/100] Loss:[0.0861=0.0000+0.0552+0.0617] Train:[98.07] val:[96.62] Test:[96.69] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000138
BIAS:[0.70] | Model:[CausalGAT] Epoch:[92/100] Loss:[0.0839=0.0000+0.0536+0.0605] Train:[98.12] val:[96.50] Test:[97.25] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000130
BIAS:[0.70] | Model:[CausalGAT] Epoch:[93/100] Loss:[0.0884=0.0000+0.0578+0.0612] Train:[97.82] val:[93.12] Test:[94.50] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000123
BIAS:[0.70] | Model:[CausalGAT] Epoch:[94/100] Loss:[0.0823=0.0000+0.0539+0.0568] Train:[98.16] val:[96.75] Test:[97.00] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000117
BIAS:[0.70] | Model:[CausalGAT] Epoch:[95/100] Loss:[0.0858=0.0000+0.0561+0.0595] Train:[97.98] val:[96.12] Test:[97.06] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000112
BIAS:[0.70] | Model:[CausalGAT] Epoch:[96/100] Loss:[0.0840=0.0000+0.0539+0.0604] Train:[98.23] val:[96.75] Test:[96.94] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000107
BIAS:[0.70] | Model:[CausalGAT] Epoch:[97/100] Loss:[0.0757=0.0000+0.0489+0.0537] Train:[98.32] val:[96.88] Test:[96.94] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000104
BIAS:[0.70] | Model:[CausalGAT] Epoch:[98/100] Loss:[0.0838=0.0000+0.0541+0.0594] Train:[98.09] val:[96.75] Test:[96.81] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000102
BIAS:[0.70] | Model:[CausalGAT] Epoch:[99/100] Loss:[0.0729=0.0000+0.0468+0.0521] Train:[98.43] val:[96.75] Test:[96.88] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000100
BIAS:[0.70] | Model:[CausalGAT] Epoch:[100/100] Loss:[0.0803=0.0000+0.0513+0.0579] Train:[98.09] val:[96.62] Test:[96.62] | Update Test:[co:96.75,c:25.00,o:96.94] at Epoch:[54] | lr:0.000100
syd: BIAS:[0.70] | Val acc:[96.62] Test acc:[co:96.75,c:25.00,o:96.94] at epoch:[54]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.9
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GAT
hidden.......................................................................128
seed.........................................................................666
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:1260 , Cycle:139  , Grids:139  , Diams:139   
| BA  : House:139  , Cycle:1260 , Grids:1260 , Diams:1260  
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:90.1%, Cycle:9.9%, Grids:9.9%, Diams:9.9%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:796
| Tree: House:180  , Cycle:19   , Grids:19   , Diams:19    
| BA  : House:19   , Cycle:180  , Grids:180  , Diams:180   
| All : House:199  , Cycle:199  , Grids:199  , Diams:199   
| BIAS: House:90.5%, Cycle:9.5%, Grids:9.5%, Diams:9.5%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.90] | Model:[GAT] Epoch:[1/100] Loss:[0.8045] Train:[66.07] val:[38.69] Test:[32.88] | Best Val:[38.69] Update Test:[32.88] at Epoch:[1] | lr:0.002000
BIAS:[0.90] | Model:[GAT] Epoch:[2/100] Loss:[0.4995] Train:[80.33] val:[82.16] Test:[78.94] | Best Val:[82.16] Update Test:[78.94] at Epoch:[2] | lr:0.001998
BIAS:[0.90] | Model:[GAT] Epoch:[3/100] Loss:[0.4514] Train:[81.90] val:[80.90] Test:[74.81] | Best Val:[82.16] Update Test:[78.94] at Epoch:[2] | lr:0.001996
BIAS:[0.90] | Model:[GAT] Epoch:[4/100] Loss:[0.4335] Train:[82.61] val:[56.28] Test:[44.56] | Best Val:[82.16] Update Test:[78.94] at Epoch:[2] | lr:0.001993
BIAS:[0.90] | Model:[GAT] Epoch:[5/100] Loss:[0.4303] Train:[83.18] val:[85.30] Test:[76.94] | Best Val:[85.30] Update Test:[76.94] at Epoch:[5] | lr:0.001988
BIAS:[0.90] | Model:[GAT] Epoch:[6/100] Loss:[0.3977] Train:[84.61] val:[84.17] Test:[76.94] | Best Val:[85.30] Update Test:[76.94] at Epoch:[5] | lr:0.001983
BIAS:[0.90] | Model:[GAT] Epoch:[7/100] Loss:[0.3906] Train:[84.88] val:[86.31] Test:[77.19] | Best Val:[86.31] Update Test:[77.19] at Epoch:[7] | lr:0.001977
BIAS:[0.90] | Model:[GAT] Epoch:[8/100] Loss:[0.3811] Train:[85.45] val:[86.56] Test:[80.88] | Best Val:[86.56] Update Test:[80.88] at Epoch:[8] | lr:0.001970
BIAS:[0.90] | Model:[GAT] Epoch:[9/100] Loss:[0.3650] Train:[85.95] val:[87.56] Test:[79.88] | Best Val:[87.56] Update Test:[79.88] at Epoch:[9] | lr:0.001962
BIAS:[0.90] | Model:[GAT] Epoch:[10/100] Loss:[0.3794] Train:[85.29] val:[87.56] Test:[78.19] | Best Val:[87.56] Update Test:[79.88] at Epoch:[9] | lr:0.001954
BIAS:[0.90] | Model:[GAT] Epoch:[11/100] Loss:[0.3541] Train:[86.15] val:[84.05] Test:[77.75] | Best Val:[87.56] Update Test:[79.88] at Epoch:[9] | lr:0.001944
BIAS:[0.90] | Model:[GAT] Epoch:[12/100] Loss:[0.3676] Train:[85.22] val:[87.81] Test:[76.81] | Best Val:[87.81] Update Test:[76.81] at Epoch:[12] | lr:0.001933
BIAS:[0.90] | Model:[GAT] Epoch:[13/100] Loss:[0.3527] Train:[86.04] val:[87.31] Test:[78.56] | Best Val:[87.81] Update Test:[76.81] at Epoch:[12] | lr:0.001922
BIAS:[0.90] | Model:[GAT] Epoch:[14/100] Loss:[0.3437] Train:[86.47] val:[88.69] Test:[78.88] | Best Val:[88.69] Update Test:[78.88] at Epoch:[14] | lr:0.001910
BIAS:[0.90] | Model:[GAT] Epoch:[15/100] Loss:[0.3373] Train:[86.85] val:[86.68] Test:[82.81] | Best Val:[88.69] Update Test:[78.88] at Epoch:[14] | lr:0.001896
BIAS:[0.90] | Model:[GAT] Epoch:[16/100] Loss:[0.3295] Train:[87.24] val:[88.32] Test:[83.75] | Best Val:[88.69] Update Test:[78.88] at Epoch:[14] | lr:0.001882
BIAS:[0.90] | Model:[GAT] Epoch:[17/100] Loss:[0.3343] Train:[86.88] val:[88.69] Test:[78.44] | Best Val:[88.69] Update Test:[78.88] at Epoch:[14] | lr:0.001868
BIAS:[0.90] | Model:[GAT] Epoch:[18/100] Loss:[0.3322] Train:[87.92] val:[87.94] Test:[80.75] | Best Val:[88.69] Update Test:[78.88] at Epoch:[14] | lr:0.001852
BIAS:[0.90] | Model:[GAT] Epoch:[19/100] Loss:[0.3170] Train:[88.21] val:[84.30] Test:[82.06] | Best Val:[88.69] Update Test:[78.88] at Epoch:[14] | lr:0.001836
BIAS:[0.90] | Model:[GAT] Epoch:[20/100] Loss:[0.3120] Train:[88.31] val:[87.94] Test:[84.38] | Best Val:[88.69] Update Test:[78.88] at Epoch:[14] | lr:0.001819
BIAS:[0.90] | Model:[GAT] Epoch:[21/100] Loss:[0.3085] Train:[88.47] val:[89.20] Test:[81.12] | Best Val:[89.20] Update Test:[81.12] at Epoch:[21] | lr:0.001801
BIAS:[0.90] | Model:[GAT] Epoch:[22/100] Loss:[0.2964] Train:[88.76] val:[90.20] Test:[87.19] | Best Val:[90.20] Update Test:[87.19] at Epoch:[22] | lr:0.001782
BIAS:[0.90] | Model:[GAT] Epoch:[23/100] Loss:[0.3020] Train:[87.90] val:[86.43] Test:[81.00] | Best Val:[90.20] Update Test:[87.19] at Epoch:[22] | lr:0.001763
BIAS:[0.90] | Model:[GAT] Epoch:[24/100] Loss:[0.2963] Train:[88.87] val:[91.08] Test:[84.94] | Best Val:[91.08] Update Test:[84.94] at Epoch:[24] | lr:0.001743
BIAS:[0.90] | Model:[GAT] Epoch:[25/100] Loss:[0.2970] Train:[88.85] val:[89.20] Test:[81.06] | Best Val:[91.08] Update Test:[84.94] at Epoch:[24] | lr:0.001722
BIAS:[0.90] | Model:[GAT] Epoch:[26/100] Loss:[0.3067] Train:[88.49] val:[89.95] Test:[83.94] | Best Val:[91.08] Update Test:[84.94] at Epoch:[24] | lr:0.001700
BIAS:[0.90] | Model:[GAT] Epoch:[27/100] Loss:[0.2888] Train:[89.62] val:[90.08] Test:[85.38] | Best Val:[91.08] Update Test:[84.94] at Epoch:[24] | lr:0.001678
BIAS:[0.90] | Model:[GAT] Epoch:[28/100] Loss:[0.2869] Train:[88.87] val:[91.33] Test:[87.56] | Best Val:[91.33] Update Test:[87.56] at Epoch:[28] | lr:0.001656
BIAS:[0.90] | Model:[GAT] Epoch:[29/100] Loss:[0.2884] Train:[89.14] val:[88.07] Test:[81.88] | Best Val:[91.33] Update Test:[87.56] at Epoch:[28] | lr:0.001632
BIAS:[0.90] | Model:[GAT] Epoch:[30/100] Loss:[0.2690] Train:[90.06] val:[91.33] Test:[87.94] | Best Val:[91.33] Update Test:[87.56] at Epoch:[28] | lr:0.001608
BIAS:[0.90] | Model:[GAT] Epoch:[31/100] Loss:[0.2702] Train:[89.83] val:[90.95] Test:[84.69] | Best Val:[91.33] Update Test:[87.56] at Epoch:[28] | lr:0.001584
BIAS:[0.90] | Model:[GAT] Epoch:[32/100] Loss:[0.2840] Train:[89.12] val:[90.95] Test:[84.88] | Best Val:[91.33] Update Test:[87.56] at Epoch:[28] | lr:0.001559
BIAS:[0.90] | Model:[GAT] Epoch:[33/100] Loss:[0.2661] Train:[90.06] val:[89.20] Test:[80.69] | Best Val:[91.33] Update Test:[87.56] at Epoch:[28] | lr:0.001534
BIAS:[0.90] | Model:[GAT] Epoch:[34/100] Loss:[0.2723] Train:[89.94] val:[89.20] Test:[84.56] | Best Val:[91.33] Update Test:[87.56] at Epoch:[28] | lr:0.001508
BIAS:[0.90] | Model:[GAT] Epoch:[35/100] Loss:[0.2599] Train:[90.30] val:[91.46] Test:[85.38] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001481
BIAS:[0.90] | Model:[GAT] Epoch:[36/100] Loss:[0.2526] Train:[90.69] val:[90.20] Test:[87.31] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001454
BIAS:[0.90] | Model:[GAT] Epoch:[37/100] Loss:[0.2475] Train:[91.15] val:[88.57] Test:[87.25] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001427
BIAS:[0.90] | Model:[GAT] Epoch:[38/100] Loss:[0.2464] Train:[91.03] val:[89.70] Test:[88.12] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001400
BIAS:[0.90] | Model:[GAT] Epoch:[39/100] Loss:[0.2396] Train:[91.01] val:[89.70] Test:[85.31] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001372
BIAS:[0.90] | Model:[GAT] Epoch:[40/100] Loss:[0.2580] Train:[90.30] val:[91.33] Test:[84.94] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001344
BIAS:[0.90] | Model:[GAT] Epoch:[41/100] Loss:[0.2410] Train:[90.87] val:[91.21] Test:[85.75] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001315
BIAS:[0.90] | Model:[GAT] Epoch:[42/100] Loss:[0.2531] Train:[90.35] val:[88.94] Test:[85.38] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001286
BIAS:[0.90] | Model:[GAT] Epoch:[43/100] Loss:[0.2497] Train:[90.23] val:[90.70] Test:[87.62] | Best Val:[91.46] Update Test:[85.38] at Epoch:[35] | lr:0.001257
BIAS:[0.90] | Model:[GAT] Epoch:[44/100] Loss:[0.2315] Train:[91.40] val:[92.34] Test:[84.50] | Best Val:[92.34] Update Test:[84.50] at Epoch:[44] | lr:0.001228
BIAS:[0.90] | Model:[GAT] Epoch:[45/100] Loss:[0.2254] Train:[91.64] val:[91.83] Test:[87.94] | Best Val:[92.34] Update Test:[84.50] at Epoch:[44] | lr:0.001199
BIAS:[0.90] | Model:[GAT] Epoch:[46/100] Loss:[0.2240] Train:[91.87] val:[91.08] Test:[88.81] | Best Val:[92.34] Update Test:[84.50] at Epoch:[44] | lr:0.001169
BIAS:[0.90] | Model:[GAT] Epoch:[47/100] Loss:[0.2270] Train:[91.94] val:[89.82] Test:[86.75] | Best Val:[92.34] Update Test:[84.50] at Epoch:[44] | lr:0.001139
BIAS:[0.90] | Model:[GAT] Epoch:[48/100] Loss:[0.2273] Train:[91.60] val:[91.83] Test:[86.62] | Best Val:[92.34] Update Test:[84.50] at Epoch:[44] | lr:0.001110
BIAS:[0.90] | Model:[GAT] Epoch:[49/100] Loss:[0.2238] Train:[91.44] val:[90.58] Test:[89.19] | Best Val:[92.34] Update Test:[84.50] at Epoch:[44] | lr:0.001080
BIAS:[0.90] | Model:[GAT] Epoch:[50/100] Loss:[0.2297] Train:[91.40] val:[93.34] Test:[88.44] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.001050
BIAS:[0.90] | Model:[GAT] Epoch:[51/100] Loss:[0.2245] Train:[91.71] val:[91.58] Test:[87.69] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.001020
BIAS:[0.90] | Model:[GAT] Epoch:[52/100] Loss:[0.2191] Train:[92.23] val:[91.33] Test:[85.88] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000990
BIAS:[0.90] | Model:[GAT] Epoch:[53/100] Loss:[0.2191] Train:[91.49] val:[91.33] Test:[89.69] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000961
BIAS:[0.90] | Model:[GAT] Epoch:[54/100] Loss:[0.2153] Train:[92.26] val:[90.58] Test:[86.00] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000931
BIAS:[0.90] | Model:[GAT] Epoch:[55/100] Loss:[0.2036] Train:[92.60] val:[91.83] Test:[88.06] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000901
BIAS:[0.90] | Model:[GAT] Epoch:[56/100] Loss:[0.2004] Train:[92.98] val:[92.71] Test:[87.06] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000872
BIAS:[0.90] | Model:[GAT] Epoch:[57/100] Loss:[0.2007] Train:[92.57] val:[90.83] Test:[89.00] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000843
BIAS:[0.90] | Model:[GAT] Epoch:[58/100] Loss:[0.2087] Train:[92.46] val:[90.58] Test:[87.06] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000814
BIAS:[0.90] | Model:[GAT] Epoch:[59/100] Loss:[0.1998] Train:[92.69] val:[91.21] Test:[88.75] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000785
BIAS:[0.90] | Model:[GAT] Epoch:[60/100] Loss:[0.1887] Train:[92.91] val:[91.83] Test:[89.06] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000756
BIAS:[0.90] | Model:[GAT] Epoch:[61/100] Loss:[0.2052] Train:[92.12] val:[92.21] Test:[89.31] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000728
BIAS:[0.90] | Model:[GAT] Epoch:[62/100] Loss:[0.2002] Train:[92.60] val:[91.96] Test:[85.94] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000700
BIAS:[0.90] | Model:[GAT] Epoch:[63/100] Loss:[0.1855] Train:[92.98] val:[92.34] Test:[89.06] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000673
BIAS:[0.90] | Model:[GAT] Epoch:[64/100] Loss:[0.1960] Train:[93.07] val:[89.70] Test:[88.31] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000646
BIAS:[0.90] | Model:[GAT] Epoch:[65/100] Loss:[0.1915] Train:[93.28] val:[92.09] Test:[87.75] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000619
BIAS:[0.90] | Model:[GAT] Epoch:[66/100] Loss:[0.1896] Train:[92.80] val:[90.83] Test:[87.38] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000592
BIAS:[0.90] | Model:[GAT] Epoch:[67/100] Loss:[0.1889] Train:[92.91] val:[90.83] Test:[89.25] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000566
BIAS:[0.90] | Model:[GAT] Epoch:[68/100] Loss:[0.1793] Train:[93.14] val:[91.58] Test:[88.56] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000541
BIAS:[0.90] | Model:[GAT] Epoch:[69/100] Loss:[0.1828] Train:[93.48] val:[91.58] Test:[90.12] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000516
BIAS:[0.90] | Model:[GAT] Epoch:[70/100] Loss:[0.1748] Train:[93.69] val:[91.58] Test:[87.81] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000492
BIAS:[0.90] | Model:[GAT] Epoch:[71/100] Loss:[0.1735] Train:[93.60] val:[91.58] Test:[89.00] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000468
BIAS:[0.90] | Model:[GAT] Epoch:[72/100] Loss:[0.1781] Train:[93.26] val:[91.96] Test:[87.81] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000444
BIAS:[0.90] | Model:[GAT] Epoch:[73/100] Loss:[0.1773] Train:[93.21] val:[91.08] Test:[88.62] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000422
BIAS:[0.90] | Model:[GAT] Epoch:[74/100] Loss:[0.1653] Train:[94.03] val:[90.70] Test:[89.06] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000400
BIAS:[0.90] | Model:[GAT] Epoch:[75/100] Loss:[0.1668] Train:[94.03] val:[91.58] Test:[89.44] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000378
BIAS:[0.90] | Model:[GAT] Epoch:[76/100] Loss:[0.1667] Train:[93.94] val:[90.83] Test:[89.19] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000357
BIAS:[0.90] | Model:[GAT] Epoch:[77/100] Loss:[0.1721] Train:[93.80] val:[92.21] Test:[89.44] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000337
BIAS:[0.90] | Model:[GAT] Epoch:[78/100] Loss:[0.1715] Train:[93.41] val:[92.71] Test:[89.12] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000318
BIAS:[0.90] | Model:[GAT] Epoch:[79/100] Loss:[0.1642] Train:[93.69] val:[91.96] Test:[89.88] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000299
BIAS:[0.90] | Model:[GAT] Epoch:[80/100] Loss:[0.1575] Train:[94.62] val:[91.83] Test:[89.69] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000281
BIAS:[0.90] | Model:[GAT] Epoch:[81/100] Loss:[0.1572] Train:[94.12] val:[91.71] Test:[89.75] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000264
BIAS:[0.90] | Model:[GAT] Epoch:[82/100] Loss:[0.1583] Train:[93.78] val:[91.58] Test:[89.50] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000248
BIAS:[0.90] | Model:[GAT] Epoch:[83/100] Loss:[0.1623] Train:[94.28] val:[91.71] Test:[89.31] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000232
BIAS:[0.90] | Model:[GAT] Epoch:[84/100] Loss:[0.1547] Train:[94.28] val:[92.46] Test:[88.94] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000218
BIAS:[0.90] | Model:[GAT] Epoch:[85/100] Loss:[0.1620] Train:[94.16] val:[91.83] Test:[89.25] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000204
BIAS:[0.90] | Model:[GAT] Epoch:[86/100] Loss:[0.1448] Train:[94.67] val:[92.71] Test:[89.62] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000190
BIAS:[0.90] | Model:[GAT] Epoch:[87/100] Loss:[0.1493] Train:[94.75] val:[92.21] Test:[88.81] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000178
BIAS:[0.90] | Model:[GAT] Epoch:[88/100] Loss:[0.1509] Train:[94.71] val:[92.21] Test:[89.94] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000167
BIAS:[0.90] | Model:[GAT] Epoch:[89/100] Loss:[0.1484] Train:[94.57] val:[92.34] Test:[89.38] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000156
BIAS:[0.90] | Model:[GAT] Epoch:[90/100] Loss:[0.1490] Train:[94.42] val:[91.96] Test:[89.44] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000146
BIAS:[0.90] | Model:[GAT] Epoch:[91/100] Loss:[0.1501] Train:[94.78] val:[91.96] Test:[89.56] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000138
BIAS:[0.90] | Model:[GAT] Epoch:[92/100] Loss:[0.1544] Train:[94.09] val:[92.21] Test:[89.25] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000130
BIAS:[0.90] | Model:[GAT] Epoch:[93/100] Loss:[0.1517] Train:[94.53] val:[92.46] Test:[89.56] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000123
BIAS:[0.90] | Model:[GAT] Epoch:[94/100] Loss:[0.1538] Train:[94.48] val:[92.34] Test:[89.06] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000117
BIAS:[0.90] | Model:[GAT] Epoch:[95/100] Loss:[0.1509] Train:[94.39] val:[92.09] Test:[90.06] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000112
BIAS:[0.90] | Model:[GAT] Epoch:[96/100] Loss:[0.1440] Train:[95.05] val:[91.83] Test:[89.44] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000107
BIAS:[0.90] | Model:[GAT] Epoch:[97/100] Loss:[0.1545] Train:[94.35] val:[92.09] Test:[89.50] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000104
BIAS:[0.90] | Model:[GAT] Epoch:[98/100] Loss:[0.1503] Train:[94.59] val:[91.96] Test:[89.62] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000102
BIAS:[0.90] | Model:[GAT] Epoch:[99/100] Loss:[0.1509] Train:[94.48] val:[92.21] Test:[89.75] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000100
BIAS:[0.90] | Model:[GAT] Epoch:[100/100] Loss:[0.1383] Train:[94.80] val:[92.21] Test:[89.31] | Best Val:[93.34] Update Test:[88.44] at Epoch:[50] | lr:0.000100
syd: BIAS:[0.90] | Best Val acc:[93.34] Test acc:[88.44] at epoch:[50]
step_size..................................................................0.001
min_lr....................................................................0.0001
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.9
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGAT
hidden.......................................................................128
seed.........................................................................666
lr.........................................................................0.002
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:1260 , Cycle:139  , Grids:139  , Diams:139   
| BA  : House:139  , Cycle:1260 , Grids:1260 , Diams:1260  
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:90.1%, Cycle:9.9%, Grids:9.9%, Diams:9.9%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:796
| Tree: House:180  , Cycle:19   , Grids:19   , Diams:19    
| BA  : House:19   , Cycle:180  , Grids:180  , Diams:180   
| All : House:199  , Cycle:199  , Grids:199  , Diams:199   
| BIAS: House:90.5%, Cycle:9.5%, Grids:9.5%, Diams:9.5%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.90] | Model:[CausalGAT] Epoch:[1/100] Loss:[1.1184=0.0207+0.7075+0.8010] Train:[71.32] val:[65.58] Test:[54.06] | Update Test:[co:45.00,c:25.00,o:54.06] at Epoch:[1] | lr:0.002000
BIAS:[0.90] | Model:[CausalGAT] Epoch:[2/100] Loss:[0.6992=0.0017+0.4574+0.4819] Train:[82.17] val:[84.92] Test:[76.38] | Update Test:[co:78.06,c:25.00,o:76.38] at Epoch:[2] | lr:0.001998
BIAS:[0.90] | Model:[CausalGAT] Epoch:[3/100] Loss:[0.6053=0.0010+0.3968+0.4161] Train:[84.69] val:[86.56] Test:[75.31] | Update Test:[co:76.62,c:23.88,o:75.31] at Epoch:[3] | lr:0.001996
BIAS:[0.90] | Model:[CausalGAT] Epoch:[4/100] Loss:[0.5549=0.0004+0.3662+0.3770] Train:[85.86] val:[88.94] Test:[84.12] | Update Test:[co:84.19,c:25.62,o:84.12] at Epoch:[4] | lr:0.001993
BIAS:[0.90] | Model:[CausalGAT] Epoch:[5/100] Loss:[0.5467=0.0004+0.3612+0.3707] Train:[86.10] val:[81.78] Test:[77.12] | Update Test:[co:84.19,c:25.62,o:84.12] at Epoch:[4] | lr:0.001988
BIAS:[0.90] | Model:[CausalGAT] Epoch:[6/100] Loss:[0.5157=0.0004+0.3384+0.3542] Train:[87.01] val:[89.07] Test:[79.94] | Update Test:[co:80.62,c:23.50,o:79.94] at Epoch:[6] | lr:0.001983
BIAS:[0.90] | Model:[CausalGAT] Epoch:[7/100] Loss:[0.5463=0.0003+0.3618+0.3688] Train:[86.28] val:[86.56] Test:[81.06] | Update Test:[co:80.62,c:23.50,o:79.94] at Epoch:[6] | lr:0.001977
BIAS:[0.90] | Model:[CausalGAT] Epoch:[8/100] Loss:[0.4883=0.0002+0.3216+0.3332] Train:[87.71] val:[90.08] Test:[84.00] | Update Test:[co:79.38,c:25.00,o:84.00] at Epoch:[8] | lr:0.001970
BIAS:[0.90] | Model:[CausalGAT] Epoch:[9/100] Loss:[0.4886=0.0002+0.3216+0.3338] Train:[87.78] val:[89.45] Test:[86.50] | Update Test:[co:79.38,c:25.00,o:84.00] at Epoch:[8] | lr:0.001962
BIAS:[0.90] | Model:[CausalGAT] Epoch:[10/100] Loss:[0.4760=0.0003+0.3141+0.3235] Train:[88.30] val:[87.19] Test:[82.56] | Update Test:[co:79.38,c:25.00,o:84.00] at Epoch:[8] | lr:0.001954
BIAS:[0.90] | Model:[CausalGAT] Epoch:[11/100] Loss:[0.4671=0.0002+0.3082+0.3175] Train:[88.37] val:[85.30] Test:[84.31] | Update Test:[co:79.38,c:25.00,o:84.00] at Epoch:[8] | lr:0.001944
BIAS:[0.90] | Model:[CausalGAT] Epoch:[12/100] Loss:[0.4559=0.0002+0.2996+0.3122] Train:[88.92] val:[89.82] Test:[81.44] | Update Test:[co:79.38,c:25.00,o:84.00] at Epoch:[8] | lr:0.001933
BIAS:[0.90] | Model:[CausalGAT] Epoch:[13/100] Loss:[0.4468=0.0002+0.2935+0.3066] Train:[88.56] val:[90.45] Test:[87.12] | Update Test:[co:86.12,c:23.81,o:87.12] at Epoch:[13] | lr:0.001922
BIAS:[0.90] | Model:[CausalGAT] Epoch:[14/100] Loss:[0.4242=0.0001+0.2793+0.2896] Train:[89.49] val:[88.57] Test:[83.19] | Update Test:[co:86.12,c:23.81,o:87.12] at Epoch:[13] | lr:0.001910
BIAS:[0.90] | Model:[CausalGAT] Epoch:[15/100] Loss:[0.4200=0.0002+0.2761+0.2876] Train:[89.65] val:[87.06] Test:[81.88] | Update Test:[co:86.12,c:23.81,o:87.12] at Epoch:[13] | lr:0.001896
BIAS:[0.90] | Model:[CausalGAT] Epoch:[16/100] Loss:[0.4051=0.0001+0.2660+0.2781] Train:[90.17] val:[89.70] Test:[85.50] | Update Test:[co:86.12,c:23.81,o:87.12] at Epoch:[13] | lr:0.001882
BIAS:[0.90] | Model:[CausalGAT] Epoch:[17/100] Loss:[0.3858=0.0002+0.2533+0.2648] Train:[90.64] val:[91.58] Test:[87.44] | Update Test:[co:85.12,c:24.00,o:87.44] at Epoch:[17] | lr:0.001868
BIAS:[0.90] | Model:[CausalGAT] Epoch:[18/100] Loss:[0.4107=0.0001+0.2703+0.2807] Train:[90.03] val:[87.94] Test:[81.06] | Update Test:[co:85.12,c:24.00,o:87.44] at Epoch:[17] | lr:0.001852
BIAS:[0.90] | Model:[CausalGAT] Epoch:[19/100] Loss:[0.3837=0.0001+0.2520+0.2631] Train:[90.10] val:[91.08] Test:[89.94] | Update Test:[co:85.12,c:24.00,o:87.44] at Epoch:[17] | lr:0.001836
BIAS:[0.90] | Model:[CausalGAT] Epoch:[20/100] Loss:[0.3672=0.0001+0.2408+0.2527] Train:[91.05] val:[87.81] Test:[82.94] | Update Test:[co:85.12,c:24.00,o:87.44] at Epoch:[17] | lr:0.001819
BIAS:[0.90] | Model:[CausalGAT] Epoch:[21/100] Loss:[0.3620=0.0001+0.2389+0.2462] Train:[90.94] val:[91.33] Test:[87.44] | Update Test:[co:85.12,c:24.00,o:87.44] at Epoch:[17] | lr:0.001801
BIAS:[0.90] | Model:[CausalGAT] Epoch:[22/100] Loss:[0.3447=0.0001+0.2273+0.2347] Train:[91.58] val:[88.57] Test:[85.88] | Update Test:[co:85.12,c:24.00,o:87.44] at Epoch:[17] | lr:0.001782
BIAS:[0.90] | Model:[CausalGAT] Epoch:[23/100] Loss:[0.3534=0.0001+0.2325+0.2416] Train:[91.03] val:[88.94] Test:[83.38] | Update Test:[co:85.12,c:24.00,o:87.44] at Epoch:[17] | lr:0.001763
BIAS:[0.90] | Model:[CausalGAT] Epoch:[24/100] Loss:[0.3320=0.0001+0.2186+0.2266] Train:[91.90] val:[92.96] Test:[89.44] | Update Test:[co:88.62,c:24.81,o:89.44] at Epoch:[24] | lr:0.001743
BIAS:[0.90] | Model:[CausalGAT] Epoch:[25/100] Loss:[0.3207=0.0001+0.2106+0.2201] Train:[92.58] val:[92.21] Test:[86.69] | Update Test:[co:88.62,c:24.81,o:89.44] at Epoch:[24] | lr:0.001722
BIAS:[0.90] | Model:[CausalGAT] Epoch:[26/100] Loss:[0.3296=0.0001+0.2170+0.2250] Train:[91.82] val:[91.96] Test:[86.75] | Update Test:[co:88.62,c:24.81,o:89.44] at Epoch:[24] | lr:0.001700
BIAS:[0.90] | Model:[CausalGAT] Epoch:[27/100] Loss:[0.3255=0.0001+0.2141+0.2227] Train:[92.16] val:[89.07] Test:[84.94] | Update Test:[co:88.62,c:24.81,o:89.44] at Epoch:[24] | lr:0.001678
BIAS:[0.90] | Model:[CausalGAT] Epoch:[28/100] Loss:[0.3224=0.0001+0.2127+0.2192] Train:[92.30] val:[94.47] Test:[89.75] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001656
BIAS:[0.90] | Model:[CausalGAT] Epoch:[29/100] Loss:[0.3138=0.0001+0.2054+0.2168] Train:[92.30] val:[92.46] Test:[88.56] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001632
BIAS:[0.90] | Model:[CausalGAT] Epoch:[30/100] Loss:[0.3187=0.0001+0.2100+0.2173] Train:[91.85] val:[91.33] Test:[91.25] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001608
BIAS:[0.90] | Model:[CausalGAT] Epoch:[31/100] Loss:[0.2993=0.0001+0.1966+0.2054] Train:[93.14] val:[93.47] Test:[91.44] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001584
BIAS:[0.90] | Model:[CausalGAT] Epoch:[32/100] Loss:[0.2986=0.0001+0.1951+0.2069] Train:[92.99] val:[93.59] Test:[88.38] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001559
BIAS:[0.90] | Model:[CausalGAT] Epoch:[33/100] Loss:[0.2815=0.0001+0.1854+0.1920] Train:[92.74] val:[83.79] Test:[85.06] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001534
BIAS:[0.90] | Model:[CausalGAT] Epoch:[34/100] Loss:[0.3037=0.0001+0.1997+0.2081] Train:[92.89] val:[94.47] Test:[89.62] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001508
BIAS:[0.90] | Model:[CausalGAT] Epoch:[35/100] Loss:[0.2710=0.0001+0.1786+0.1846] Train:[93.35] val:[93.84] Test:[89.81] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001481
BIAS:[0.90] | Model:[CausalGAT] Epoch:[36/100] Loss:[0.2687=0.0001+0.1768+0.1837] Train:[93.67] val:[93.47] Test:[90.88] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001454
BIAS:[0.90] | Model:[CausalGAT] Epoch:[37/100] Loss:[0.2494=0.0001+0.1637+0.1714] Train:[94.25] val:[92.96] Test:[90.81] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001427
BIAS:[0.90] | Model:[CausalGAT] Epoch:[38/100] Loss:[0.2658=0.0001+0.1757+0.1801] Train:[93.76] val:[81.28] Test:[79.38] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001400
BIAS:[0.90] | Model:[CausalGAT] Epoch:[39/100] Loss:[0.2525=0.0001+0.1666+0.1719] Train:[93.69] val:[91.58] Test:[92.00] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001372
BIAS:[0.90] | Model:[CausalGAT] Epoch:[40/100] Loss:[0.2649=0.0001+0.1746+0.1805] Train:[93.53] val:[93.72] Test:[91.62] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001344
BIAS:[0.90] | Model:[CausalGAT] Epoch:[41/100] Loss:[0.2355=0.0001+0.1545+0.1619] Train:[94.69] val:[86.81] Test:[85.81] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001315
BIAS:[0.90] | Model:[CausalGAT] Epoch:[42/100] Loss:[0.2552=0.0000+0.1684+0.1735] Train:[93.98] val:[93.09] Test:[90.12] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001286
BIAS:[0.90] | Model:[CausalGAT] Epoch:[43/100] Loss:[0.2413=0.0000+0.1592+0.1642] Train:[94.26] val:[92.96] Test:[90.88] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001257
BIAS:[0.90] | Model:[CausalGAT] Epoch:[44/100] Loss:[0.2329=0.0000+0.1532+0.1594] Train:[94.34] val:[74.12] Test:[74.88] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001228
BIAS:[0.90] | Model:[CausalGAT] Epoch:[45/100] Loss:[0.2229=0.0000+0.1468+0.1522] Train:[94.92] val:[93.72] Test:[92.69] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001199
BIAS:[0.90] | Model:[CausalGAT] Epoch:[46/100] Loss:[0.2271=0.0000+0.1486+0.1570] Train:[95.05] val:[88.07] Test:[86.06] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001169
BIAS:[0.90] | Model:[CausalGAT] Epoch:[47/100] Loss:[0.2289=0.0000+0.1501+0.1576] Train:[94.26] val:[86.18] Test:[85.94] | Update Test:[co:91.50,c:20.88,o:89.75] at Epoch:[28] | lr:0.001139
BIAS:[0.90] | Model:[CausalGAT] Epoch:[48/100] Loss:[0.2304=0.0000+0.1509+0.1589] Train:[94.69] val:[94.72] Test:[91.00] | Update Test:[co:90.50,c:25.19,o:91.00] at Epoch:[48] | lr:0.001110
BIAS:[0.90] | Model:[CausalGAT] Epoch:[49/100] Loss:[0.2196=0.0000+0.1441+0.1509] Train:[94.94] val:[87.44] Test:[89.00] | Update Test:[co:90.50,c:25.19,o:91.00] at Epoch:[48] | lr:0.001080
BIAS:[0.90] | Model:[CausalGAT] Epoch:[50/100] Loss:[0.2098=0.0000+0.1377+0.1441] Train:[95.14] val:[94.85] Test:[91.25] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.001050
BIAS:[0.90] | Model:[CausalGAT] Epoch:[51/100] Loss:[0.2152=0.0000+0.1416+0.1472] Train:[94.85] val:[94.85] Test:[93.00] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.001020
BIAS:[0.90] | Model:[CausalGAT] Epoch:[52/100] Loss:[0.2086=0.0000+0.1366+0.1441] Train:[95.09] val:[91.58] Test:[91.75] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.000990
BIAS:[0.90] | Model:[CausalGAT] Epoch:[53/100] Loss:[0.1981=0.0000+0.1304+0.1353] Train:[95.30] val:[94.60] Test:[92.75] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.000961
BIAS:[0.90] | Model:[CausalGAT] Epoch:[54/100] Loss:[0.1985=0.0000+0.1309+0.1353] Train:[95.23] val:[92.96] Test:[93.69] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.000931
BIAS:[0.90] | Model:[CausalGAT] Epoch:[55/100] Loss:[0.1979=0.0000+0.1291+0.1376] Train:[95.53] val:[93.97] Test:[94.00] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.000901
BIAS:[0.90] | Model:[CausalGAT] Epoch:[56/100] Loss:[0.1920=0.0000+0.1262+0.1315] Train:[95.59] val:[94.60] Test:[91.12] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.000872
BIAS:[0.90] | Model:[CausalGAT] Epoch:[57/100] Loss:[0.1847=0.0000+0.1211+0.1272] Train:[95.59] val:[86.43] Test:[89.25] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.000843
BIAS:[0.90] | Model:[CausalGAT] Epoch:[58/100] Loss:[0.1861=0.0000+0.1222+0.1277] Train:[95.68] val:[92.09] Test:[92.31] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.000814
BIAS:[0.90] | Model:[CausalGAT] Epoch:[59/100] Loss:[0.1880=0.0000+0.1234+0.1292] Train:[95.62] val:[93.97] Test:[93.81] | Update Test:[co:91.00,c:23.88,o:91.25] at Epoch:[50] | lr:0.000785
BIAS:[0.90] | Model:[CausalGAT] Epoch:[60/100] Loss:[0.1749=0.0000+0.1154+0.1191] Train:[95.75] val:[95.35] Test:[93.69] | Update Test:[co:93.69,c:24.62,o:93.69] at Epoch:[60] | lr:0.000756
BIAS:[0.90] | Model:[CausalGAT] Epoch:[61/100] Loss:[0.1843=0.0000+0.1211+0.1265] Train:[95.53] val:[93.84] Test:[93.81] | Update Test:[co:93.69,c:24.62,o:93.69] at Epoch:[60] | lr:0.000728
BIAS:[0.90] | Model:[CausalGAT] Epoch:[62/100] Loss:[0.1773=0.0000+0.1156+0.1232] Train:[96.03] val:[93.84] Test:[93.81] | Update Test:[co:93.69,c:24.62,o:93.69] at Epoch:[60] | lr:0.000700
BIAS:[0.90] | Model:[CausalGAT] Epoch:[63/100] Loss:[0.1819=0.0000+0.1191+0.1257] Train:[95.43] val:[94.47] Test:[93.81] | Update Test:[co:93.69,c:24.62,o:93.69] at Epoch:[60] | lr:0.000673
BIAS:[0.90] | Model:[CausalGAT] Epoch:[64/100] Loss:[0.1879=0.0000+0.1237+0.1284] Train:[95.78] val:[94.22] Test:[93.44] | Update Test:[co:93.69,c:24.62,o:93.69] at Epoch:[60] | lr:0.000646
BIAS:[0.90] | Model:[CausalGAT] Epoch:[65/100] Loss:[0.1677=0.0000+0.1097+0.1159] Train:[96.14] val:[94.22] Test:[93.44] | Update Test:[co:93.69,c:24.62,o:93.69] at Epoch:[60] | lr:0.000619
BIAS:[0.90] | Model:[CausalGAT] Epoch:[66/100] Loss:[0.1451=0.0000+0.0949+0.1005] Train:[96.71] val:[95.85] Test:[93.06] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000592
BIAS:[0.90] | Model:[CausalGAT] Epoch:[67/100] Loss:[0.1607=0.0000+0.1043+0.1128] Train:[96.21] val:[91.83] Test:[93.75] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000566
BIAS:[0.90] | Model:[CausalGAT] Epoch:[68/100] Loss:[0.1570=0.0000+0.1030+0.1079] Train:[96.39] val:[95.10] Test:[94.56] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000541
BIAS:[0.90] | Model:[CausalGAT] Epoch:[69/100] Loss:[0.1546=0.0000+0.1017+0.1057] Train:[96.27] val:[94.35] Test:[93.06] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000516
BIAS:[0.90] | Model:[CausalGAT] Epoch:[70/100] Loss:[0.1458=0.0000+0.0952+0.1011] Train:[96.44] val:[95.35] Test:[95.56] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000492
BIAS:[0.90] | Model:[CausalGAT] Epoch:[71/100] Loss:[0.1613=0.0000+0.1056+0.1115] Train:[96.21] val:[75.88] Test:[82.25] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000468
BIAS:[0.90] | Model:[CausalGAT] Epoch:[72/100] Loss:[0.1451=0.0000+0.0948+0.1008] Train:[96.85] val:[95.85] Test:[94.94] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000444
BIAS:[0.90] | Model:[CausalGAT] Epoch:[73/100] Loss:[0.1539=0.0000+0.1007+0.1065] Train:[96.46] val:[95.35] Test:[94.94] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000422
BIAS:[0.90] | Model:[CausalGAT] Epoch:[74/100] Loss:[0.1475=0.0000+0.0973+0.1004] Train:[96.43] val:[95.10] Test:[93.75] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000400
BIAS:[0.90] | Model:[CausalGAT] Epoch:[75/100] Loss:[0.1323=0.0000+0.0866+0.0914] Train:[97.14] val:[95.60] Test:[95.69] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000378
BIAS:[0.90] | Model:[CausalGAT] Epoch:[76/100] Loss:[0.1476=0.0000+0.0964+0.1024] Train:[96.66] val:[95.48] Test:[95.06] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000357
BIAS:[0.90] | Model:[CausalGAT] Epoch:[77/100] Loss:[0.1352=0.0000+0.0882+0.0940] Train:[96.78] val:[94.85] Test:[94.56] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000337
BIAS:[0.90] | Model:[CausalGAT] Epoch:[78/100] Loss:[0.1348=0.0000+0.0884+0.0929] Train:[96.94] val:[94.10] Test:[93.19] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000318
BIAS:[0.90] | Model:[CausalGAT] Epoch:[79/100] Loss:[0.1230=0.0000+0.0805+0.0849] Train:[97.23] val:[94.72] Test:[95.88] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000299
BIAS:[0.90] | Model:[CausalGAT] Epoch:[80/100] Loss:[0.1258=0.0000+0.0823+0.0870] Train:[97.02] val:[95.85] Test:[94.75] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000281
BIAS:[0.90] | Model:[CausalGAT] Epoch:[81/100] Loss:[0.1374=0.0000+0.0899+0.0950] Train:[96.73] val:[92.96] Test:[92.00] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000264
BIAS:[0.90] | Model:[CausalGAT] Epoch:[82/100] Loss:[0.1238=0.0000+0.0803+0.0871] Train:[97.16] val:[93.09] Test:[93.19] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000248
BIAS:[0.90] | Model:[CausalGAT] Epoch:[83/100] Loss:[0.1256=0.0000+0.0818+0.0876] Train:[97.02] val:[93.59] Test:[92.38] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000232
BIAS:[0.90] | Model:[CausalGAT] Epoch:[84/100] Loss:[0.1230=0.0000+0.0796+0.0869] Train:[97.19] val:[95.10] Test:[95.00] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000218
BIAS:[0.90] | Model:[CausalGAT] Epoch:[85/100] Loss:[0.1201=0.0000+0.0781+0.0839] Train:[97.30] val:[95.35] Test:[95.56] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000204
BIAS:[0.90] | Model:[CausalGAT] Epoch:[86/100] Loss:[0.1192=0.0000+0.0779+0.0825] Train:[97.30] val:[94.22] Test:[93.38] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000190
BIAS:[0.90] | Model:[CausalGAT] Epoch:[87/100] Loss:[0.1154=0.0000+0.0752+0.0805] Train:[97.44] val:[94.85] Test:[94.81] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000178
BIAS:[0.90] | Model:[CausalGAT] Epoch:[88/100] Loss:[0.1211=0.0000+0.0790+0.0843] Train:[97.44] val:[92.21] Test:[92.50] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000167
BIAS:[0.90] | Model:[CausalGAT] Epoch:[89/100] Loss:[0.1148=0.0000+0.0747+0.0803] Train:[97.18] val:[94.47] Test:[94.50] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000156
BIAS:[0.90] | Model:[CausalGAT] Epoch:[90/100] Loss:[0.1138=0.0000+0.0740+0.0796] Train:[97.36] val:[90.08] Test:[90.62] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000146
BIAS:[0.90] | Model:[CausalGAT] Epoch:[91/100] Loss:[0.1163=0.0000+0.0763+0.0799] Train:[97.12] val:[94.85] Test:[93.69] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000138
BIAS:[0.90] | Model:[CausalGAT] Epoch:[92/100] Loss:[0.1014=0.0000+0.0660+0.0709] Train:[97.77] val:[95.60] Test:[95.62] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000130
BIAS:[0.90] | Model:[CausalGAT] Epoch:[93/100] Loss:[0.1139=0.0000+0.0741+0.0796] Train:[97.41] val:[95.60] Test:[95.19] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000123
BIAS:[0.90] | Model:[CausalGAT] Epoch:[94/100] Loss:[0.1045=0.0000+0.0680+0.0730] Train:[97.62] val:[95.23] Test:[93.94] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000117
BIAS:[0.90] | Model:[CausalGAT] Epoch:[95/100] Loss:[0.1178=0.0000+0.0767+0.0822] Train:[97.39] val:[95.60] Test:[94.62] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000112
BIAS:[0.90] | Model:[CausalGAT] Epoch:[96/100] Loss:[0.1189=0.0000+0.0779+0.0820] Train:[97.36] val:[94.85] Test:[94.19] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000107
BIAS:[0.90] | Model:[CausalGAT] Epoch:[97/100] Loss:[0.1056=0.0000+0.0683+0.0747] Train:[97.68] val:[94.85] Test:[94.62] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000104
BIAS:[0.90] | Model:[CausalGAT] Epoch:[98/100] Loss:[0.1104=0.0000+0.0722+0.0764] Train:[97.36] val:[95.23] Test:[94.81] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000102
BIAS:[0.90] | Model:[CausalGAT] Epoch:[99/100] Loss:[0.1080=0.0000+0.0705+0.0750] Train:[97.55] val:[95.60] Test:[95.12] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000100
BIAS:[0.90] | Model:[CausalGAT] Epoch:[100/100] Loss:[0.1115=0.0000+0.0729+0.0771] Train:[97.23] val:[94.85] Test:[94.56] | Update Test:[co:93.31,c:26.31,o:93.06] at Epoch:[66] | lr:0.000100
syd: BIAS:[0.90] | Val acc:[94.85] Test acc:[co:93.31,c:26.31,o:93.06] at epoch:[66]
