step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.1
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5597
| Tree: House:140  , Cycle:1260 , Grids:1260 , Diams:1260  
| BA  : House:1260 , Cycle:139  , Grids:139  , Diams:139   
| All : House:1400 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:10.0%, Cycle:90.1%, Grids:90.1%, Diams:90.1%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:797
| Tree: House:20   , Cycle:180  , Grids:180  , Diams:180   
| BA  : House:180  , Cycle:19   , Grids:19   , Diams:19    
| All : House:200  , Cycle:199  , Grids:199  , Diams:199   
| BIAS: House:10.0%, Cycle:90.5%, Grids:90.5%, Diams:90.5%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.10] | Model:[GIN] Epoch:[1/100] Loss:[0.6561] Train:[79.65] val:[88.83] Test:[49.06] | Best Val:[88.83] Update Test:[49.06] at Epoch:[1] | lr:0.001000
BIAS:[0.10] | Model:[GIN] Epoch:[2/100] Loss:[0.3207] Train:[90.07] val:[91.84] Test:[66.94] | Best Val:[91.84] Update Test:[66.94] at Epoch:[2] | lr:0.000999
BIAS:[0.10] | Model:[GIN] Epoch:[3/100] Loss:[0.2404] Train:[92.30] val:[91.72] Test:[72.12] | Best Val:[91.84] Update Test:[66.94] at Epoch:[2] | lr:0.000998
BIAS:[0.10] | Model:[GIN] Epoch:[4/100] Loss:[0.2354] Train:[92.60] val:[92.60] Test:[70.56] | Best Val:[92.60] Update Test:[70.56] at Epoch:[4] | lr:0.000996
BIAS:[0.10] | Model:[GIN] Epoch:[5/100] Loss:[0.1962] Train:[93.69] val:[85.82] Test:[76.50] | Best Val:[92.60] Update Test:[70.56] at Epoch:[4] | lr:0.000994
BIAS:[0.10] | Model:[GIN] Epoch:[6/100] Loss:[0.1884] Train:[94.10] val:[93.98] Test:[77.06] | Best Val:[93.98] Update Test:[77.06] at Epoch:[6] | lr:0.000991
BIAS:[0.10] | Model:[GIN] Epoch:[7/100] Loss:[0.1682] Train:[94.94] val:[94.23] Test:[77.19] | Best Val:[94.23] Update Test:[77.19] at Epoch:[7] | lr:0.000988
BIAS:[0.10] | Model:[GIN] Epoch:[8/100] Loss:[0.1721] Train:[94.82] val:[89.46] Test:[78.25] | Best Val:[94.23] Update Test:[77.19] at Epoch:[7] | lr:0.000984
BIAS:[0.10] | Model:[GIN] Epoch:[9/100] Loss:[0.1639] Train:[94.94] val:[94.35] Test:[86.44] | Best Val:[94.35] Update Test:[86.44] at Epoch:[9] | lr:0.000980
BIAS:[0.10] | Model:[GIN] Epoch:[10/100] Loss:[0.1446] Train:[95.34] val:[91.59] Test:[72.06] | Best Val:[94.35] Update Test:[86.44] at Epoch:[9] | lr:0.000976
BIAS:[0.10] | Model:[GIN] Epoch:[11/100] Loss:[0.1419] Train:[95.55] val:[95.98] Test:[81.69] | Best Val:[95.98] Update Test:[81.69] at Epoch:[11] | lr:0.000970
BIAS:[0.10] | Model:[GIN] Epoch:[12/100] Loss:[0.1361] Train:[95.71] val:[95.11] Test:[83.88] | Best Val:[95.98] Update Test:[81.69] at Epoch:[11] | lr:0.000965
BIAS:[0.10] | Model:[GIN] Epoch:[13/100] Loss:[0.1310] Train:[96.09] val:[94.35] Test:[75.50] | Best Val:[95.98] Update Test:[81.69] at Epoch:[11] | lr:0.000959
BIAS:[0.10] | Model:[GIN] Epoch:[14/100] Loss:[0.1267] Train:[96.34] val:[95.36] Test:[78.12] | Best Val:[95.98] Update Test:[81.69] at Epoch:[11] | lr:0.000952
BIAS:[0.10] | Model:[GIN] Epoch:[15/100] Loss:[0.1207] Train:[96.53] val:[92.35] Test:[87.31] | Best Val:[95.98] Update Test:[81.69] at Epoch:[11] | lr:0.000946
BIAS:[0.10] | Model:[GIN] Epoch:[16/100] Loss:[0.1235] Train:[96.34] val:[90.84] Test:[76.25] | Best Val:[95.98] Update Test:[81.69] at Epoch:[11] | lr:0.000938
BIAS:[0.10] | Model:[GIN] Epoch:[17/100] Loss:[0.1418] Train:[95.78] val:[94.86] Test:[82.50] | Best Val:[95.98] Update Test:[81.69] at Epoch:[11] | lr:0.000930
BIAS:[0.10] | Model:[GIN] Epoch:[18/100] Loss:[0.1297] Train:[95.87] val:[95.61] Test:[83.75] | Best Val:[95.98] Update Test:[81.69] at Epoch:[11] | lr:0.000922
BIAS:[0.10] | Model:[GIN] Epoch:[19/100] Loss:[0.1200] Train:[96.34] val:[96.86] Test:[86.56] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000914
BIAS:[0.10] | Model:[GIN] Epoch:[20/100] Loss:[0.1154] Train:[96.61] val:[82.06] Test:[68.19] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000905
BIAS:[0.10] | Model:[GIN] Epoch:[21/100] Loss:[0.1095] Train:[96.75] val:[95.48] Test:[81.38] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000895
BIAS:[0.10] | Model:[GIN] Epoch:[22/100] Loss:[0.1172] Train:[96.50] val:[84.57] Test:[77.75] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000885
BIAS:[0.10] | Model:[GIN] Epoch:[23/100] Loss:[0.1017] Train:[96.68] val:[96.49] Test:[86.12] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000875
BIAS:[0.10] | Model:[GIN] Epoch:[24/100] Loss:[0.1293] Train:[96.14] val:[92.22] Test:[83.38] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000865
BIAS:[0.10] | Model:[GIN] Epoch:[25/100] Loss:[0.1042] Train:[96.86] val:[85.95] Test:[66.38] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000854
BIAS:[0.10] | Model:[GIN] Epoch:[26/100] Loss:[0.1064] Train:[96.91] val:[95.86] Test:[89.25] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000842
BIAS:[0.10] | Model:[GIN] Epoch:[27/100] Loss:[0.0828] Train:[97.57] val:[95.73] Test:[88.50] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000831
BIAS:[0.10] | Model:[GIN] Epoch:[28/100] Loss:[0.0920] Train:[96.91] val:[88.83] Test:[76.25] | Best Val:[96.86] Update Test:[86.56] at Epoch:[19] | lr:0.000819
BIAS:[0.10] | Model:[GIN] Epoch:[29/100] Loss:[0.1093] Train:[96.91] val:[97.49] Test:[86.50] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000807
BIAS:[0.10] | Model:[GIN] Epoch:[30/100] Loss:[0.0779] Train:[97.66] val:[87.33] Test:[81.56] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000794
BIAS:[0.10] | Model:[GIN] Epoch:[31/100] Loss:[0.0821] Train:[97.32] val:[96.24] Test:[90.56] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000781
BIAS:[0.10] | Model:[GIN] Epoch:[32/100] Loss:[0.0834] Train:[97.32] val:[95.86] Test:[83.31] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000768
BIAS:[0.10] | Model:[GIN] Epoch:[33/100] Loss:[0.0731] Train:[97.80] val:[96.99] Test:[91.31] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000755
BIAS:[0.10] | Model:[GIN] Epoch:[34/100] Loss:[0.0699] Train:[97.71] val:[94.73] Test:[90.19] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000741
BIAS:[0.10] | Model:[GIN] Epoch:[35/100] Loss:[0.0789] Train:[97.62] val:[95.98] Test:[86.00] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000727
BIAS:[0.10] | Model:[GIN] Epoch:[36/100] Loss:[0.0634] Train:[98.09] val:[95.61] Test:[83.56] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000713
BIAS:[0.10] | Model:[GIN] Epoch:[37/100] Loss:[0.0661] Train:[98.03] val:[96.99] Test:[86.62] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000699
BIAS:[0.10] | Model:[GIN] Epoch:[38/100] Loss:[0.0705] Train:[97.84] val:[96.49] Test:[82.38] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000684
BIAS:[0.10] | Model:[GIN] Epoch:[39/100] Loss:[0.0630] Train:[97.95] val:[96.74] Test:[84.88] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000670
BIAS:[0.10] | Model:[GIN] Epoch:[40/100] Loss:[0.0600] Train:[98.12] val:[95.98] Test:[91.25] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000655
BIAS:[0.10] | Model:[GIN] Epoch:[41/100] Loss:[0.0618] Train:[98.03] val:[97.37] Test:[85.06] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000640
BIAS:[0.10] | Model:[GIN] Epoch:[42/100] Loss:[0.0456] Train:[98.70] val:[97.37] Test:[87.31] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000625
BIAS:[0.10] | Model:[GIN] Epoch:[43/100] Loss:[0.0455] Train:[98.48] val:[96.99] Test:[84.69] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000609
BIAS:[0.10] | Model:[GIN] Epoch:[44/100] Loss:[0.0489] Train:[98.32] val:[97.11] Test:[88.56] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000594
BIAS:[0.10] | Model:[GIN] Epoch:[45/100] Loss:[0.0467] Train:[98.55] val:[96.61] Test:[87.25] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000579
BIAS:[0.10] | Model:[GIN] Epoch:[46/100] Loss:[0.0499] Train:[98.52] val:[88.46] Test:[77.06] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000563
BIAS:[0.10] | Model:[GIN] Epoch:[47/100] Loss:[0.0463] Train:[98.62] val:[95.86] Test:[89.31] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000548
BIAS:[0.10] | Model:[GIN] Epoch:[48/100] Loss:[0.0475] Train:[98.37] val:[96.99] Test:[85.38] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000532
BIAS:[0.10] | Model:[GIN] Epoch:[49/100] Loss:[0.0494] Train:[98.27] val:[97.24] Test:[89.25] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000516
BIAS:[0.10] | Model:[GIN] Epoch:[50/100] Loss:[0.0506] Train:[98.23] val:[97.49] Test:[90.19] | Best Val:[97.49] Update Test:[86.50] at Epoch:[29] | lr:0.000501
BIAS:[0.10] | Model:[GIN] Epoch:[51/100] Loss:[0.0336] Train:[99.11] val:[97.74] Test:[89.75] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000485
BIAS:[0.10] | Model:[GIN] Epoch:[52/100] Loss:[0.0322] Train:[99.02] val:[97.62] Test:[89.00] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000469
BIAS:[0.10] | Model:[GIN] Epoch:[53/100] Loss:[0.0341] Train:[98.91] val:[95.98] Test:[91.00] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000453
BIAS:[0.10] | Model:[GIN] Epoch:[54/100] Loss:[0.0311] Train:[99.07] val:[95.86] Test:[86.38] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000438
BIAS:[0.10] | Model:[GIN] Epoch:[55/100] Loss:[0.0283] Train:[99.21] val:[97.37] Test:[89.94] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000422
BIAS:[0.10] | Model:[GIN] Epoch:[56/100] Loss:[0.0196] Train:[99.48] val:[96.86] Test:[90.31] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000407
BIAS:[0.10] | Model:[GIN] Epoch:[57/100] Loss:[0.0182] Train:[99.36] val:[97.49] Test:[90.38] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000392
BIAS:[0.10] | Model:[GIN] Epoch:[58/100] Loss:[0.0179] Train:[99.52] val:[96.86] Test:[91.75] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000376
BIAS:[0.10] | Model:[GIN] Epoch:[59/100] Loss:[0.0196] Train:[99.39] val:[97.11] Test:[90.12] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000361
BIAS:[0.10] | Model:[GIN] Epoch:[60/100] Loss:[0.0201] Train:[99.50] val:[95.36] Test:[87.44] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000346
BIAS:[0.10] | Model:[GIN] Epoch:[61/100] Loss:[0.0147] Train:[99.66] val:[96.74] Test:[89.50] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000331
BIAS:[0.10] | Model:[GIN] Epoch:[62/100] Loss:[0.0116] Train:[99.73] val:[96.99] Test:[90.75] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000317
BIAS:[0.10] | Model:[GIN] Epoch:[63/100] Loss:[0.0125] Train:[99.77] val:[97.24] Test:[90.38] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000302
BIAS:[0.10] | Model:[GIN] Epoch:[64/100] Loss:[0.0102] Train:[99.79] val:[97.49] Test:[89.94] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000288
BIAS:[0.10] | Model:[GIN] Epoch:[65/100] Loss:[0.0108] Train:[99.75] val:[96.86] Test:[91.19] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000274
BIAS:[0.10] | Model:[GIN] Epoch:[66/100] Loss:[0.0082] Train:[99.91] val:[97.24] Test:[90.00] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000260
BIAS:[0.10] | Model:[GIN] Epoch:[67/100] Loss:[0.0089] Train:[99.80] val:[96.99] Test:[90.06] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000246
BIAS:[0.10] | Model:[GIN] Epoch:[68/100] Loss:[0.0092] Train:[99.77] val:[97.24] Test:[89.94] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000233
BIAS:[0.10] | Model:[GIN] Epoch:[69/100] Loss:[0.0080] Train:[99.89] val:[96.49] Test:[91.56] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000220
BIAS:[0.10] | Model:[GIN] Epoch:[70/100] Loss:[0.0058] Train:[99.95] val:[97.37] Test:[91.00] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000207
BIAS:[0.10] | Model:[GIN] Epoch:[71/100] Loss:[0.0040] Train:[99.93] val:[97.74] Test:[90.44] | Best Val:[97.74] Update Test:[89.75] at Epoch:[51] | lr:0.000194
BIAS:[0.10] | Model:[GIN] Epoch:[72/100] Loss:[0.0048] Train:[99.95] val:[97.99] Test:[91.00] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000182
BIAS:[0.10] | Model:[GIN] Epoch:[73/100] Loss:[0.0071] Train:[99.87] val:[97.37] Test:[89.19] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000170
BIAS:[0.10] | Model:[GIN] Epoch:[74/100] Loss:[0.0049] Train:[99.89] val:[97.37] Test:[90.81] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000159
BIAS:[0.10] | Model:[GIN] Epoch:[75/100] Loss:[0.0057] Train:[99.89] val:[97.49] Test:[90.75] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000147
BIAS:[0.10] | Model:[GIN] Epoch:[76/100] Loss:[0.0045] Train:[99.96] val:[97.37] Test:[90.75] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000136
BIAS:[0.10] | Model:[GIN] Epoch:[77/100] Loss:[0.0042] Train:[99.95] val:[97.24] Test:[90.12] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000126
BIAS:[0.10] | Model:[GIN] Epoch:[78/100] Loss:[0.0037] Train:[99.95] val:[97.37] Test:[91.19] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000116
BIAS:[0.10] | Model:[GIN] Epoch:[79/100] Loss:[0.0028] Train:[99.98] val:[97.37] Test:[90.81] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000106
BIAS:[0.10] | Model:[GIN] Epoch:[80/100] Loss:[0.0031] Train:[99.96] val:[97.62] Test:[90.75] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000096
BIAS:[0.10] | Model:[GIN] Epoch:[81/100] Loss:[0.0023] Train:[99.96] val:[97.49] Test:[91.50] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000087
BIAS:[0.10] | Model:[GIN] Epoch:[82/100] Loss:[0.0031] Train:[99.95] val:[97.37] Test:[91.12] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000079
BIAS:[0.10] | Model:[GIN] Epoch:[83/100] Loss:[0.0040] Train:[99.91] val:[97.99] Test:[90.50] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000071
BIAS:[0.10] | Model:[GIN] Epoch:[84/100] Loss:[0.0027] Train:[99.96] val:[97.37] Test:[90.81] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000063
BIAS:[0.10] | Model:[GIN] Epoch:[85/100] Loss:[0.0034] Train:[99.93] val:[97.87] Test:[90.50] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000055
BIAS:[0.10] | Model:[GIN] Epoch:[86/100] Loss:[0.0026] Train:[99.93] val:[97.62] Test:[90.44] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000049
BIAS:[0.10] | Model:[GIN] Epoch:[87/100] Loss:[0.0021] Train:[99.96] val:[97.74] Test:[91.06] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000042
BIAS:[0.10] | Model:[GIN] Epoch:[88/100] Loss:[0.0033] Train:[99.93] val:[97.87] Test:[91.19] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000036
BIAS:[0.10] | Model:[GIN] Epoch:[89/100] Loss:[0.0025] Train:[99.95] val:[97.49] Test:[90.75] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000031
BIAS:[0.10] | Model:[GIN] Epoch:[90/100] Loss:[0.0031] Train:[99.93] val:[97.62] Test:[91.31] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000025
BIAS:[0.10] | Model:[GIN] Epoch:[91/100] Loss:[0.0018] Train:[99.98] val:[97.74] Test:[91.31] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000021
BIAS:[0.10] | Model:[GIN] Epoch:[92/100] Loss:[0.0024] Train:[99.96] val:[97.49] Test:[90.94] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000017
BIAS:[0.10] | Model:[GIN] Epoch:[93/100] Loss:[0.0017] Train:[100.00] val:[97.49] Test:[90.81] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000013
BIAS:[0.10] | Model:[GIN] Epoch:[94/100] Loss:[0.0019] Train:[99.96] val:[97.62] Test:[91.06] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000010
BIAS:[0.10] | Model:[GIN] Epoch:[95/100] Loss:[0.0029] Train:[99.95] val:[97.62] Test:[91.00] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000007
BIAS:[0.10] | Model:[GIN] Epoch:[96/100] Loss:[0.0023] Train:[99.98] val:[97.74] Test:[91.06] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000005
BIAS:[0.10] | Model:[GIN] Epoch:[97/100] Loss:[0.0019] Train:[99.98] val:[97.74] Test:[91.31] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000003
BIAS:[0.10] | Model:[GIN] Epoch:[98/100] Loss:[0.0018] Train:[99.96] val:[97.62] Test:[91.06] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000002
BIAS:[0.10] | Model:[GIN] Epoch:[99/100] Loss:[0.0014] Train:[100.00] val:[97.74] Test:[91.00] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000001
BIAS:[0.10] | Model:[GIN] Epoch:[100/100] Loss:[0.0015] Train:[99.98] val:[97.62] Test:[91.06] | Best Val:[97.99] Update Test:[91.00] at Epoch:[72] | lr:0.000001
syd: BIAS:[0.10] | Best Val acc:[97.99] Test acc:[91.00] at epoch:[72]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.1
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5597
| Tree: House:140  , Cycle:1260 , Grids:1260 , Diams:1260  
| BA  : House:1260 , Cycle:139  , Grids:139  , Diams:139   
| All : House:1400 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:10.0%, Cycle:90.1%, Grids:90.1%, Diams:90.1%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:797
| Tree: House:20   , Cycle:180  , Grids:180  , Diams:180   
| BA  : House:180  , Cycle:19   , Grids:19   , Diams:19    
| All : House:200  , Cycle:199  , Grids:199  , Diams:199   
| BIAS: House:10.0%, Cycle:90.5%, Grids:90.5%, Diams:90.5%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.10] | Model:[CausalGIN] Epoch:[1/100] Loss:[1.0105=0.0230+0.5799+0.8381] Train:[82.01] val:[45.04] Test:[25.19] | Update Test:[co:24.31,c:25.00,o:25.19] at Epoch:[1] | lr:0.001000
BIAS:[0.10] | Model:[CausalGIN] Epoch:[2/100] Loss:[0.5523=0.0030+0.3493+0.4030] Train:[88.99] val:[89.59] Test:[59.62] | Update Test:[co:40.56,c:26.50,o:59.62] at Epoch:[2] | lr:0.000999
BIAS:[0.10] | Model:[CausalGIN] Epoch:[3/100] Loss:[0.4413=0.0013+0.2736+0.3341] Train:[90.76] val:[92.47] Test:[71.69] | Update Test:[co:58.75,c:24.62,o:71.69] at Epoch:[3] | lr:0.000998
BIAS:[0.10] | Model:[CausalGIN] Epoch:[4/100] Loss:[0.3923=0.0009+0.2475+0.2886] Train:[91.60] val:[92.47] Test:[68.31] | Update Test:[co:58.75,c:24.62,o:71.69] at Epoch:[3] | lr:0.000996
BIAS:[0.10] | Model:[CausalGIN] Epoch:[5/100] Loss:[0.3239=0.0005+0.2101+0.2272] Train:[93.21] val:[92.72] Test:[81.88] | Update Test:[co:81.12,c:25.00,o:81.88] at Epoch:[5] | lr:0.000994
BIAS:[0.10] | Model:[CausalGIN] Epoch:[6/100] Loss:[0.3003=0.0004+0.1939+0.2123] Train:[94.09] val:[87.58] Test:[66.81] | Update Test:[co:81.12,c:25.00,o:81.88] at Epoch:[5] | lr:0.000991
BIAS:[0.10] | Model:[CausalGIN] Epoch:[7/100] Loss:[0.3168=0.0005+0.2040+0.2252] Train:[93.32] val:[91.97] Test:[73.69] | Update Test:[co:81.12,c:25.00,o:81.88] at Epoch:[5] | lr:0.000988
BIAS:[0.10] | Model:[CausalGIN] Epoch:[8/100] Loss:[0.2647=0.0003+0.1715+0.1861] Train:[94.87] val:[94.35] Test:[75.81] | Update Test:[co:74.56,c:25.81,o:75.81] at Epoch:[8] | lr:0.000984
BIAS:[0.10] | Model:[CausalGIN] Epoch:[9/100] Loss:[0.2667=0.0003+0.1756+0.1820] Train:[94.84] val:[94.48] Test:[74.44] | Update Test:[co:73.44,c:26.81,o:74.44] at Epoch:[9] | lr:0.000980
BIAS:[0.10] | Model:[CausalGIN] Epoch:[10/100] Loss:[0.2225=0.0002+0.1444+0.1559] Train:[95.75] val:[95.48] Test:[78.81] | Update Test:[co:78.94,c:25.06,o:78.81] at Epoch:[10] | lr:0.000976
BIAS:[0.10] | Model:[CausalGIN] Epoch:[11/100] Loss:[0.2267=0.0001+0.1484+0.1563] Train:[95.28] val:[94.35] Test:[85.62] | Update Test:[co:78.94,c:25.06,o:78.81] at Epoch:[10] | lr:0.000970
BIAS:[0.10] | Model:[CausalGIN] Epoch:[12/100] Loss:[0.2319=0.0001+0.1506+0.1625] Train:[95.14] val:[69.01] Test:[51.50] | Update Test:[co:78.94,c:25.06,o:78.81] at Epoch:[10] | lr:0.000965
BIAS:[0.10] | Model:[CausalGIN] Epoch:[13/100] Loss:[0.1959=0.0001+0.1274+0.1368] Train:[96.00] val:[90.84] Test:[73.12] | Update Test:[co:78.94,c:25.06,o:78.81] at Epoch:[10] | lr:0.000959
BIAS:[0.10] | Model:[CausalGIN] Epoch:[14/100] Loss:[0.1780=0.0001+0.1153+0.1255] Train:[96.44] val:[92.97] Test:[88.56] | Update Test:[co:78.94,c:25.06,o:78.81] at Epoch:[10] | lr:0.000952
BIAS:[0.10] | Model:[CausalGIN] Epoch:[15/100] Loss:[0.1840=0.0001+0.1206+0.1268] Train:[96.28] val:[95.86] Test:[79.69] | Update Test:[co:78.56,c:20.81,o:79.69] at Epoch:[15] | lr:0.000946
BIAS:[0.10] | Model:[CausalGIN] Epoch:[16/100] Loss:[0.1697=0.0001+0.1107+0.1181] Train:[96.61] val:[91.59] Test:[79.50] | Update Test:[co:78.56,c:20.81,o:79.69] at Epoch:[15] | lr:0.000938
BIAS:[0.10] | Model:[CausalGIN] Epoch:[17/100] Loss:[0.1742=0.0001+0.1126+0.1232] Train:[96.98] val:[69.01] Test:[61.19] | Update Test:[co:78.56,c:20.81,o:79.69] at Epoch:[15] | lr:0.000930
BIAS:[0.10] | Model:[CausalGIN] Epoch:[18/100] Loss:[0.1638=0.0001+0.1056+0.1162] Train:[96.89] val:[96.61] Test:[85.56] | Update Test:[co:81.75,c:25.56,o:85.56] at Epoch:[18] | lr:0.000922
BIAS:[0.10] | Model:[CausalGIN] Epoch:[19/100] Loss:[0.1670=0.0001+0.1086+0.1167] Train:[96.86] val:[96.74] Test:[89.62] | Update Test:[co:83.12,c:24.00,o:89.62] at Epoch:[19] | lr:0.000914
BIAS:[0.10] | Model:[CausalGIN] Epoch:[20/100] Loss:[0.1465=0.0001+0.0944+0.1040] Train:[96.86] val:[96.74] Test:[88.25] | Update Test:[co:83.12,c:24.00,o:89.62] at Epoch:[19] | lr:0.000905
BIAS:[0.10] | Model:[CausalGIN] Epoch:[21/100] Loss:[0.1342=0.0001+0.0881+0.0922] Train:[97.45] val:[96.86] Test:[87.75] | Update Test:[co:88.38,c:26.19,o:87.75] at Epoch:[21] | lr:0.000895
BIAS:[0.10] | Model:[CausalGIN] Epoch:[22/100] Loss:[0.1364=0.0001+0.0885+0.0958] Train:[97.07] val:[96.49] Test:[87.75] | Update Test:[co:88.38,c:26.19,o:87.75] at Epoch:[21] | lr:0.000885
BIAS:[0.10] | Model:[CausalGIN] Epoch:[23/100] Loss:[0.1328=0.0001+0.0871+0.0914] Train:[97.07] val:[97.11] Test:[88.62] | Update Test:[co:87.06,c:22.81,o:88.62] at Epoch:[23] | lr:0.000875
BIAS:[0.10] | Model:[CausalGIN] Epoch:[24/100] Loss:[0.1311=0.0001+0.0852+0.0916] Train:[97.48] val:[97.37] Test:[88.12] | Update Test:[co:82.12,c:21.00,o:88.12] at Epoch:[24] | lr:0.000865
BIAS:[0.10] | Model:[CausalGIN] Epoch:[25/100] Loss:[0.1229=0.0001+0.0794+0.0870] Train:[97.57] val:[96.11] Test:[88.12] | Update Test:[co:82.12,c:21.00,o:88.12] at Epoch:[24] | lr:0.000854
BIAS:[0.10] | Model:[CausalGIN] Epoch:[26/100] Loss:[0.1209=0.0001+0.0788+0.0841] Train:[97.75] val:[96.61] Test:[84.25] | Update Test:[co:82.12,c:21.00,o:88.12] at Epoch:[24] | lr:0.000842
BIAS:[0.10] | Model:[CausalGIN] Epoch:[27/100] Loss:[0.1112=0.0001+0.0711+0.0799] Train:[97.71] val:[96.86] Test:[87.31] | Update Test:[co:82.12,c:21.00,o:88.12] at Epoch:[24] | lr:0.000831
BIAS:[0.10] | Model:[CausalGIN] Epoch:[28/100] Loss:[0.0985=0.0001+0.0642+0.0685] Train:[97.96] val:[96.74] Test:[86.81] | Update Test:[co:82.12,c:21.00,o:88.12] at Epoch:[24] | lr:0.000819
BIAS:[0.10] | Model:[CausalGIN] Epoch:[29/100] Loss:[0.0981=0.0001+0.0641+0.0679] Train:[97.95] val:[94.60] Test:[87.38] | Update Test:[co:82.12,c:21.00,o:88.12] at Epoch:[24] | lr:0.000807
BIAS:[0.10] | Model:[CausalGIN] Epoch:[30/100] Loss:[0.0903=0.0001+0.0575+0.0654] Train:[98.12] val:[97.87] Test:[88.31] | Update Test:[co:87.94,c:28.81,o:88.31] at Epoch:[30] | lr:0.000794
BIAS:[0.10] | Model:[CausalGIN] Epoch:[31/100] Loss:[0.0912=0.0001+0.0584+0.0653] Train:[98.05] val:[97.87] Test:[87.62] | Update Test:[co:87.94,c:28.81,o:88.31] at Epoch:[30] | lr:0.000781
BIAS:[0.10] | Model:[CausalGIN] Epoch:[32/100] Loss:[0.1002=0.0001+0.0644+0.0714] Train:[97.95] val:[97.24] Test:[90.62] | Update Test:[co:87.94,c:28.81,o:88.31] at Epoch:[30] | lr:0.000768
BIAS:[0.10] | Model:[CausalGIN] Epoch:[33/100] Loss:[0.0936=0.0001+0.0600+0.0670] Train:[98.23] val:[50.82] Test:[52.38] | Update Test:[co:87.94,c:28.81,o:88.31] at Epoch:[30] | lr:0.000755
BIAS:[0.10] | Model:[CausalGIN] Epoch:[34/100] Loss:[0.0878=0.0001+0.0562+0.0631] Train:[98.23] val:[97.74] Test:[88.69] | Update Test:[co:87.94,c:28.81,o:88.31] at Epoch:[30] | lr:0.000741
BIAS:[0.10] | Model:[CausalGIN] Epoch:[35/100] Loss:[0.0795=0.0000+0.0509+0.0573] Train:[98.55] val:[96.49] Test:[89.69] | Update Test:[co:87.94,c:28.81,o:88.31] at Epoch:[30] | lr:0.000727
BIAS:[0.10] | Model:[CausalGIN] Epoch:[36/100] Loss:[0.0738=0.0000+0.0465+0.0545] Train:[98.46] val:[97.74] Test:[88.56] | Update Test:[co:87.94,c:28.81,o:88.31] at Epoch:[30] | lr:0.000713
BIAS:[0.10] | Model:[CausalGIN] Epoch:[37/100] Loss:[0.0668=0.0000+0.0424+0.0487] Train:[98.70] val:[97.99] Test:[90.88] | Update Test:[co:91.06,c:28.50,o:90.88] at Epoch:[37] | lr:0.000699
BIAS:[0.10] | Model:[CausalGIN] Epoch:[38/100] Loss:[0.0647=0.0001+0.0407+0.0480] Train:[98.91] val:[96.86] Test:[88.75] | Update Test:[co:91.06,c:28.50,o:90.88] at Epoch:[37] | lr:0.000684
BIAS:[0.10] | Model:[CausalGIN] Epoch:[39/100] Loss:[0.0568=0.0001+0.0359+0.0417] Train:[98.87] val:[98.24] Test:[90.06] | Update Test:[co:91.75,c:26.56,o:90.06] at Epoch:[39] | lr:0.000670
BIAS:[0.10] | Model:[CausalGIN] Epoch:[40/100] Loss:[0.0593=0.0000+0.0377+0.0433] Train:[98.89] val:[95.86] Test:[92.69] | Update Test:[co:91.75,c:26.56,o:90.06] at Epoch:[39] | lr:0.000655
BIAS:[0.10] | Model:[CausalGIN] Epoch:[41/100] Loss:[0.0650=0.0000+0.0416+0.0467] Train:[98.71] val:[97.74] Test:[88.75] | Update Test:[co:91.75,c:26.56,o:90.06] at Epoch:[39] | lr:0.000640
BIAS:[0.10] | Model:[CausalGIN] Epoch:[42/100] Loss:[0.0674=0.0000+0.0432+0.0484] Train:[98.75] val:[98.24] Test:[91.56] | Update Test:[co:91.75,c:26.56,o:90.06] at Epoch:[39] | lr:0.000625
BIAS:[0.10] | Model:[CausalGIN] Epoch:[43/100] Loss:[0.0554=0.0000+0.0356+0.0395] Train:[99.04] val:[97.99] Test:[94.56] | Update Test:[co:91.75,c:26.56,o:90.06] at Epoch:[39] | lr:0.000609
BIAS:[0.10] | Model:[CausalGIN] Epoch:[44/100] Loss:[0.0636=0.0001+0.0406+0.0460] Train:[98.61] val:[94.98] Test:[90.31] | Update Test:[co:91.75,c:26.56,o:90.06] at Epoch:[39] | lr:0.000594
BIAS:[0.10] | Model:[CausalGIN] Epoch:[45/100] Loss:[0.0478=0.0000+0.0308+0.0339] Train:[99.25] val:[96.49] Test:[86.31] | Update Test:[co:91.75,c:26.56,o:90.06] at Epoch:[39] | lr:0.000579
BIAS:[0.10] | Model:[CausalGIN] Epoch:[46/100] Loss:[0.0581=0.0001+0.0373+0.0415] Train:[98.89] val:[98.12] Test:[92.94] | Update Test:[co:91.75,c:26.56,o:90.06] at Epoch:[39] | lr:0.000563
BIAS:[0.10] | Model:[CausalGIN] Epoch:[47/100] Loss:[0.0441=0.0000+0.0270+0.0342] Train:[99.12] val:[98.49] Test:[90.00] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000548
BIAS:[0.10] | Model:[CausalGIN] Epoch:[48/100] Loss:[0.0435=0.0000+0.0281+0.0308] Train:[99.11] val:[83.94] Test:[83.00] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000532
BIAS:[0.10] | Model:[CausalGIN] Epoch:[49/100] Loss:[0.0459=0.0000+0.0289+0.0340] Train:[99.14] val:[97.74] Test:[87.56] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000516
BIAS:[0.10] | Model:[CausalGIN] Epoch:[50/100] Loss:[0.0563=0.0000+0.0357+0.0413] Train:[98.80] val:[98.49] Test:[92.75] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000501
BIAS:[0.10] | Model:[CausalGIN] Epoch:[51/100] Loss:[0.0372=0.0000+0.0239+0.0265] Train:[99.29] val:[97.87] Test:[90.25] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000485
BIAS:[0.10] | Model:[CausalGIN] Epoch:[52/100] Loss:[0.0451=0.0000+0.0281+0.0339] Train:[99.12] val:[98.24] Test:[91.75] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000469
BIAS:[0.10] | Model:[CausalGIN] Epoch:[53/100] Loss:[0.0445=0.0000+0.0277+0.0336] Train:[99.18] val:[97.99] Test:[92.38] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000453
BIAS:[0.10] | Model:[CausalGIN] Epoch:[54/100] Loss:[0.0317=0.0000+0.0200+0.0234] Train:[99.48] val:[84.32] Test:[78.88] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000438
BIAS:[0.10] | Model:[CausalGIN] Epoch:[55/100] Loss:[0.0234=0.0000+0.0146+0.0175] Train:[99.62] val:[98.12] Test:[92.69] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000422
BIAS:[0.10] | Model:[CausalGIN] Epoch:[56/100] Loss:[0.0208=0.0000+0.0125+0.0166] Train:[99.75] val:[98.37] Test:[93.19] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000407
BIAS:[0.10] | Model:[CausalGIN] Epoch:[57/100] Loss:[0.0261=0.0000+0.0161+0.0199] Train:[99.62] val:[97.11] Test:[93.25] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000392
BIAS:[0.10] | Model:[CausalGIN] Epoch:[58/100] Loss:[0.0205=0.0000+0.0129+0.0153] Train:[99.61] val:[97.62] Test:[92.25] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000376
BIAS:[0.10] | Model:[CausalGIN] Epoch:[59/100] Loss:[0.0218=0.0000+0.0133+0.0170] Train:[99.66] val:[98.49] Test:[94.00] | Update Test:[co:92.00,c:25.12,o:90.00] at Epoch:[47] | lr:0.000361
BIAS:[0.10] | Model:[CausalGIN] Epoch:[60/100] Loss:[0.0164=0.0000+0.0098+0.0132] Train:[99.80] val:[98.62] Test:[92.50] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000346
BIAS:[0.10] | Model:[CausalGIN] Epoch:[61/100] Loss:[0.0166=0.0000+0.0098+0.0136] Train:[99.84] val:[98.12] Test:[91.62] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000331
BIAS:[0.10] | Model:[CausalGIN] Epoch:[62/100] Loss:[0.0131=0.0000+0.0078+0.0104] Train:[99.79] val:[98.62] Test:[93.56] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000317
BIAS:[0.10] | Model:[CausalGIN] Epoch:[63/100] Loss:[0.0144=0.0000+0.0084+0.0120] Train:[99.84] val:[98.49] Test:[91.69] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000302
BIAS:[0.10] | Model:[CausalGIN] Epoch:[64/100] Loss:[0.0158=0.0000+0.0097+0.0121] Train:[99.77] val:[98.49] Test:[92.75] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000288
BIAS:[0.10] | Model:[CausalGIN] Epoch:[65/100] Loss:[0.0126=0.0000+0.0075+0.0103] Train:[99.89] val:[98.37] Test:[91.56] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000274
BIAS:[0.10] | Model:[CausalGIN] Epoch:[66/100] Loss:[0.0138=0.0000+0.0078+0.0120] Train:[99.80] val:[97.99] Test:[91.44] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000260
BIAS:[0.10] | Model:[CausalGIN] Epoch:[67/100] Loss:[0.0099=0.0000+0.0059+0.0078] Train:[99.89] val:[98.49] Test:[92.00] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000246
BIAS:[0.10] | Model:[CausalGIN] Epoch:[68/100] Loss:[0.0064=0.0000+0.0036+0.0055] Train:[99.96] val:[98.49] Test:[92.38] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000233
BIAS:[0.10] | Model:[CausalGIN] Epoch:[69/100] Loss:[0.0088=0.0000+0.0049+0.0078] Train:[99.93] val:[98.49] Test:[93.12] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000220
BIAS:[0.10] | Model:[CausalGIN] Epoch:[70/100] Loss:[0.0082=0.0000+0.0049+0.0066] Train:[99.95] val:[98.49] Test:[91.69] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000207
BIAS:[0.10] | Model:[CausalGIN] Epoch:[71/100] Loss:[0.0116=0.0000+0.0067+0.0099] Train:[99.84] val:[98.49] Test:[92.06] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000194
BIAS:[0.10] | Model:[CausalGIN] Epoch:[72/100] Loss:[0.0089=0.0000+0.0048+0.0082] Train:[99.91] val:[98.12] Test:[92.31] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000182
BIAS:[0.10] | Model:[CausalGIN] Epoch:[73/100] Loss:[0.0074=0.0000+0.0044+0.0061] Train:[99.93] val:[98.62] Test:[92.56] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000170
BIAS:[0.10] | Model:[CausalGIN] Epoch:[74/100] Loss:[0.0043=0.0000+0.0022+0.0041] Train:[99.98] val:[98.49] Test:[92.94] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000159
BIAS:[0.10] | Model:[CausalGIN] Epoch:[75/100] Loss:[0.0050=0.0000+0.0028+0.0044] Train:[99.95] val:[98.62] Test:[92.69] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000147
BIAS:[0.10] | Model:[CausalGIN] Epoch:[76/100] Loss:[0.0058=0.0000+0.0032+0.0051] Train:[99.96] val:[98.62] Test:[93.00] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000136
BIAS:[0.10] | Model:[CausalGIN] Epoch:[77/100] Loss:[0.0051=0.0000+0.0029+0.0043] Train:[99.98] val:[98.62] Test:[93.50] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000126
BIAS:[0.10] | Model:[CausalGIN] Epoch:[78/100] Loss:[0.0042=0.0000+0.0022+0.0040] Train:[99.96] val:[98.62] Test:[92.62] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000116
BIAS:[0.10] | Model:[CausalGIN] Epoch:[79/100] Loss:[0.0048=0.0000+0.0026+0.0044] Train:[99.98] val:[98.37] Test:[92.44] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000106
BIAS:[0.10] | Model:[CausalGIN] Epoch:[80/100] Loss:[0.0060=0.0000+0.0034+0.0052] Train:[99.95] val:[98.12] Test:[93.00] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000096
BIAS:[0.10] | Model:[CausalGIN] Epoch:[81/100] Loss:[0.0044=0.0000+0.0025+0.0038] Train:[99.96] val:[98.49] Test:[92.25] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000087
BIAS:[0.10] | Model:[CausalGIN] Epoch:[82/100] Loss:[0.0039=0.0000+0.0020+0.0038] Train:[99.98] val:[98.49] Test:[92.81] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000079
BIAS:[0.10] | Model:[CausalGIN] Epoch:[83/100] Loss:[0.0041=0.0000+0.0024+0.0036] Train:[99.95] val:[98.49] Test:[92.56] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000071
BIAS:[0.10] | Model:[CausalGIN] Epoch:[84/100] Loss:[0.0034=0.0000+0.0018+0.0032] Train:[99.96] val:[98.37] Test:[92.62] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000063
BIAS:[0.10] | Model:[CausalGIN] Epoch:[85/100] Loss:[0.0030=0.0000+0.0016+0.0029] Train:[99.98] val:[98.37] Test:[92.69] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000055
BIAS:[0.10] | Model:[CausalGIN] Epoch:[86/100] Loss:[0.0032=0.0000+0.0016+0.0031] Train:[99.96] val:[98.49] Test:[92.56] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000049
BIAS:[0.10] | Model:[CausalGIN] Epoch:[87/100] Loss:[0.0034=0.0000+0.0019+0.0030] Train:[99.98] val:[98.37] Test:[92.75] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000042
BIAS:[0.10] | Model:[CausalGIN] Epoch:[88/100] Loss:[0.0044=0.0000+0.0021+0.0046] Train:[99.95] val:[98.37] Test:[92.69] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000036
BIAS:[0.10] | Model:[CausalGIN] Epoch:[89/100] Loss:[0.0041=0.0000+0.0022+0.0037] Train:[99.98] val:[98.37] Test:[92.81] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000031
BIAS:[0.10] | Model:[CausalGIN] Epoch:[90/100] Loss:[0.0040=0.0000+0.0022+0.0037] Train:[99.95] val:[98.62] Test:[92.62] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000025
BIAS:[0.10] | Model:[CausalGIN] Epoch:[91/100] Loss:[0.0039=0.0000+0.0022+0.0034] Train:[99.96] val:[98.49] Test:[93.06] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000021
BIAS:[0.10] | Model:[CausalGIN] Epoch:[92/100] Loss:[0.0028=0.0000+0.0013+0.0028] Train:[99.98] val:[98.49] Test:[92.81] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000017
BIAS:[0.10] | Model:[CausalGIN] Epoch:[93/100] Loss:[0.0033=0.0000+0.0018+0.0031] Train:[99.96] val:[98.24] Test:[93.06] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000013
BIAS:[0.10] | Model:[CausalGIN] Epoch:[94/100] Loss:[0.0029=0.0000+0.0015+0.0028] Train:[99.98] val:[98.49] Test:[92.75] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000010
BIAS:[0.10] | Model:[CausalGIN] Epoch:[95/100] Loss:[0.0026=0.0000+0.0013+0.0026] Train:[99.98] val:[98.49] Test:[93.00] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000007
BIAS:[0.10] | Model:[CausalGIN] Epoch:[96/100] Loss:[0.0025=0.0000+0.0013+0.0024] Train:[99.98] val:[98.37] Test:[92.62] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000005
BIAS:[0.10] | Model:[CausalGIN] Epoch:[97/100] Loss:[0.0028=0.0000+0.0013+0.0029] Train:[99.98] val:[98.37] Test:[92.81] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000003
BIAS:[0.10] | Model:[CausalGIN] Epoch:[98/100] Loss:[0.0035=0.0000+0.0018+0.0034] Train:[99.96] val:[98.49] Test:[92.81] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000002
BIAS:[0.10] | Model:[CausalGIN] Epoch:[99/100] Loss:[0.0025=0.0000+0.0013+0.0025] Train:[100.00] val:[98.37] Test:[92.69] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000001
BIAS:[0.10] | Model:[CausalGIN] Epoch:[100/100] Loss:[0.0023=0.0000+0.0012+0.0023] Train:[99.98] val:[98.49] Test:[92.69] | Update Test:[co:93.31,c:25.06,o:92.50] at Epoch:[60] | lr:0.000001
syd: BIAS:[0.10] | Val acc:[98.49] Test acc:[co:93.31,c:25.06,o:92.50] at epoch:[60]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.3
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:420  , Cycle:979  , Grids:979  , Diams:979   
| BA  : House:979  , Cycle:420  , Grids:420  , Diams:420   
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:30.0%, Cycle:70.0%, Grids:70.0%, Diams:70.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:60   , Cycle:140  , Grids:140  , Diams:140   
| BA  : House:140  , Cycle:60   , Grids:60   , Diams:60    
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:30.0%, Cycle:70.0%, Grids:70.0%, Diams:70.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.30] | Model:[GIN] Epoch:[1/100] Loss:[1.0543] Train:[57.36] val:[52.50] Test:[43.06] | Best Val:[52.50] Update Test:[43.06] at Epoch:[1] | lr:0.001000
BIAS:[0.30] | Model:[GIN] Epoch:[2/100] Loss:[0.5952] Train:[77.20] val:[71.50] Test:[69.31] | Best Val:[71.50] Update Test:[69.31] at Epoch:[2] | lr:0.000999
BIAS:[0.30] | Model:[GIN] Epoch:[3/100] Loss:[0.4317] Train:[84.19] val:[83.00] Test:[74.12] | Best Val:[83.00] Update Test:[74.12] at Epoch:[3] | lr:0.000998
BIAS:[0.30] | Model:[GIN] Epoch:[4/100] Loss:[0.3909] Train:[86.63] val:[87.25] Test:[81.69] | Best Val:[87.25] Update Test:[81.69] at Epoch:[4] | lr:0.000996
BIAS:[0.30] | Model:[GIN] Epoch:[5/100] Loss:[0.3103] Train:[89.46] val:[84.25] Test:[82.25] | Best Val:[87.25] Update Test:[81.69] at Epoch:[4] | lr:0.000994
BIAS:[0.30] | Model:[GIN] Epoch:[6/100] Loss:[0.2774] Train:[90.58] val:[77.75] Test:[71.06] | Best Val:[87.25] Update Test:[81.69] at Epoch:[4] | lr:0.000991
BIAS:[0.30] | Model:[GIN] Epoch:[7/100] Loss:[0.2593] Train:[91.14] val:[86.25] Test:[81.69] | Best Val:[87.25] Update Test:[81.69] at Epoch:[4] | lr:0.000988
BIAS:[0.30] | Model:[GIN] Epoch:[8/100] Loss:[0.2495] Train:[91.17] val:[87.38] Test:[86.31] | Best Val:[87.38] Update Test:[86.31] at Epoch:[8] | lr:0.000984
BIAS:[0.30] | Model:[GIN] Epoch:[9/100] Loss:[0.2364] Train:[92.19] val:[91.12] Test:[87.81] | Best Val:[91.12] Update Test:[87.81] at Epoch:[9] | lr:0.000980
BIAS:[0.30] | Model:[GIN] Epoch:[10/100] Loss:[0.2347] Train:[91.94] val:[90.88] Test:[89.06] | Best Val:[91.12] Update Test:[87.81] at Epoch:[9] | lr:0.000976
BIAS:[0.30] | Model:[GIN] Epoch:[11/100] Loss:[0.2131] Train:[92.83] val:[85.38] Test:[82.56] | Best Val:[91.12] Update Test:[87.81] at Epoch:[9] | lr:0.000970
BIAS:[0.30] | Model:[GIN] Epoch:[12/100] Loss:[0.2101] Train:[92.69] val:[88.88] Test:[84.81] | Best Val:[91.12] Update Test:[87.81] at Epoch:[9] | lr:0.000965
BIAS:[0.30] | Model:[GIN] Epoch:[13/100] Loss:[0.1862] Train:[93.19] val:[88.75] Test:[83.19] | Best Val:[91.12] Update Test:[87.81] at Epoch:[9] | lr:0.000959
BIAS:[0.30] | Model:[GIN] Epoch:[14/100] Loss:[0.1916] Train:[93.39] val:[91.75] Test:[87.75] | Best Val:[91.75] Update Test:[87.75] at Epoch:[14] | lr:0.000952
BIAS:[0.30] | Model:[GIN] Epoch:[15/100] Loss:[0.2010] Train:[93.26] val:[71.38] Test:[66.69] | Best Val:[91.75] Update Test:[87.75] at Epoch:[14] | lr:0.000946
BIAS:[0.30] | Model:[GIN] Epoch:[16/100] Loss:[0.1915] Train:[93.60] val:[89.75] Test:[87.75] | Best Val:[91.75] Update Test:[87.75] at Epoch:[14] | lr:0.000938
BIAS:[0.30] | Model:[GIN] Epoch:[17/100] Loss:[0.1634] Train:[94.28] val:[92.88] Test:[89.31] | Best Val:[92.88] Update Test:[89.31] at Epoch:[17] | lr:0.000930
BIAS:[0.30] | Model:[GIN] Epoch:[18/100] Loss:[0.1685] Train:[94.32] val:[91.00] Test:[90.38] | Best Val:[92.88] Update Test:[89.31] at Epoch:[17] | lr:0.000922
BIAS:[0.30] | Model:[GIN] Epoch:[19/100] Loss:[0.1646] Train:[94.23] val:[91.12] Test:[89.69] | Best Val:[92.88] Update Test:[89.31] at Epoch:[17] | lr:0.000914
BIAS:[0.30] | Model:[GIN] Epoch:[20/100] Loss:[0.1544] Train:[94.62] val:[93.38] Test:[90.75] | Best Val:[93.38] Update Test:[90.75] at Epoch:[20] | lr:0.000905
BIAS:[0.30] | Model:[GIN] Epoch:[21/100] Loss:[0.1412] Train:[95.07] val:[94.00] Test:[91.19] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000895
BIAS:[0.30] | Model:[GIN] Epoch:[22/100] Loss:[0.1517] Train:[94.64] val:[93.75] Test:[91.88] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000885
BIAS:[0.30] | Model:[GIN] Epoch:[23/100] Loss:[0.1370] Train:[95.34] val:[93.62] Test:[91.12] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000875
BIAS:[0.30] | Model:[GIN] Epoch:[24/100] Loss:[0.1363] Train:[95.60] val:[89.62] Test:[89.12] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000865
BIAS:[0.30] | Model:[GIN] Epoch:[25/100] Loss:[0.1257] Train:[95.64] val:[93.75] Test:[91.88] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000854
BIAS:[0.30] | Model:[GIN] Epoch:[26/100] Loss:[0.1187] Train:[95.98] val:[92.88] Test:[92.81] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000842
BIAS:[0.30] | Model:[GIN] Epoch:[27/100] Loss:[0.1196] Train:[95.89] val:[93.25] Test:[91.81] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000831
BIAS:[0.30] | Model:[GIN] Epoch:[28/100] Loss:[0.1204] Train:[95.75] val:[91.62] Test:[89.25] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000819
BIAS:[0.30] | Model:[GIN] Epoch:[29/100] Loss:[0.1332] Train:[95.28] val:[93.75] Test:[90.50] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000807
BIAS:[0.30] | Model:[GIN] Epoch:[30/100] Loss:[0.1246] Train:[95.59] val:[87.50] Test:[85.06] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000794
BIAS:[0.30] | Model:[GIN] Epoch:[31/100] Loss:[0.1102] Train:[96.32] val:[90.88] Test:[87.56] | Best Val:[94.00] Update Test:[91.19] at Epoch:[21] | lr:0.000781
BIAS:[0.30] | Model:[GIN] Epoch:[32/100] Loss:[0.1150] Train:[96.23] val:[95.25] Test:[92.69] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000768
BIAS:[0.30] | Model:[GIN] Epoch:[33/100] Loss:[0.0917] Train:[97.05] val:[93.38] Test:[89.75] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000755
BIAS:[0.30] | Model:[GIN] Epoch:[34/100] Loss:[0.0868] Train:[96.96] val:[92.25] Test:[91.81] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000741
BIAS:[0.30] | Model:[GIN] Epoch:[35/100] Loss:[0.0906] Train:[97.21] val:[92.12] Test:[87.50] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000727
BIAS:[0.30] | Model:[GIN] Epoch:[36/100] Loss:[0.0780] Train:[97.34] val:[94.38] Test:[93.19] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000713
BIAS:[0.30] | Model:[GIN] Epoch:[37/100] Loss:[0.0775] Train:[97.39] val:[88.25] Test:[88.56] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000699
BIAS:[0.30] | Model:[GIN] Epoch:[38/100] Loss:[0.0988] Train:[96.82] val:[90.50] Test:[91.38] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000684
BIAS:[0.30] | Model:[GIN] Epoch:[39/100] Loss:[0.0806] Train:[97.16] val:[91.00] Test:[89.75] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000670
BIAS:[0.30] | Model:[GIN] Epoch:[40/100] Loss:[0.0838] Train:[97.05] val:[92.75] Test:[93.75] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000655
BIAS:[0.30] | Model:[GIN] Epoch:[41/100] Loss:[0.0676] Train:[97.80] val:[93.00] Test:[92.19] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000640
BIAS:[0.30] | Model:[GIN] Epoch:[42/100] Loss:[0.0682] Train:[97.73] val:[93.62] Test:[91.12] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000625
BIAS:[0.30] | Model:[GIN] Epoch:[43/100] Loss:[0.0736] Train:[97.71] val:[95.00] Test:[93.81] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000609
BIAS:[0.30] | Model:[GIN] Epoch:[44/100] Loss:[0.0660] Train:[97.71] val:[93.62] Test:[88.44] | Best Val:[95.25] Update Test:[92.69] at Epoch:[32] | lr:0.000594
BIAS:[0.30] | Model:[GIN] Epoch:[45/100] Loss:[0.0820] Train:[97.02] val:[95.50] Test:[92.12] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000579
BIAS:[0.30] | Model:[GIN] Epoch:[46/100] Loss:[0.0601] Train:[97.89] val:[93.12] Test:[92.19] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000563
BIAS:[0.30] | Model:[GIN] Epoch:[47/100] Loss:[0.0545] Train:[98.16] val:[94.88] Test:[93.38] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000548
BIAS:[0.30] | Model:[GIN] Epoch:[48/100] Loss:[0.0469] Train:[98.48] val:[93.25] Test:[91.19] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000532
BIAS:[0.30] | Model:[GIN] Epoch:[49/100] Loss:[0.0677] Train:[97.84] val:[94.12] Test:[91.06] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000516
BIAS:[0.30] | Model:[GIN] Epoch:[50/100] Loss:[0.0571] Train:[98.02] val:[93.00] Test:[90.94] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000501
BIAS:[0.30] | Model:[GIN] Epoch:[51/100] Loss:[0.0452] Train:[98.46] val:[93.12] Test:[91.50] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000485
BIAS:[0.30] | Model:[GIN] Epoch:[52/100] Loss:[0.0368] Train:[98.84] val:[94.50] Test:[93.94] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000469
BIAS:[0.30] | Model:[GIN] Epoch:[53/100] Loss:[0.0390] Train:[98.89] val:[94.88] Test:[93.31] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000453
BIAS:[0.30] | Model:[GIN] Epoch:[54/100] Loss:[0.0327] Train:[99.12] val:[92.25] Test:[89.12] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000438
BIAS:[0.30] | Model:[GIN] Epoch:[55/100] Loss:[0.0290] Train:[99.02] val:[94.75] Test:[92.88] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000422
BIAS:[0.30] | Model:[GIN] Epoch:[56/100] Loss:[0.0290] Train:[99.18] val:[95.00] Test:[92.81] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000407
BIAS:[0.30] | Model:[GIN] Epoch:[57/100] Loss:[0.0210] Train:[99.50] val:[95.00] Test:[93.12] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000392
BIAS:[0.30] | Model:[GIN] Epoch:[58/100] Loss:[0.0261] Train:[99.25] val:[94.88] Test:[94.06] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000376
BIAS:[0.30] | Model:[GIN] Epoch:[59/100] Loss:[0.0291] Train:[98.96] val:[94.12] Test:[93.19] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000361
BIAS:[0.30] | Model:[GIN] Epoch:[60/100] Loss:[0.0203] Train:[99.55] val:[94.38] Test:[92.94] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000346
BIAS:[0.30] | Model:[GIN] Epoch:[61/100] Loss:[0.0231] Train:[99.36] val:[94.50] Test:[92.75] | Best Val:[95.50] Update Test:[92.12] at Epoch:[45] | lr:0.000331
BIAS:[0.30] | Model:[GIN] Epoch:[62/100] Loss:[0.0158] Train:[99.62] val:[95.75] Test:[93.88] | Best Val:[95.75] Update Test:[93.88] at Epoch:[62] | lr:0.000317
BIAS:[0.30] | Model:[GIN] Epoch:[63/100] Loss:[0.0189] Train:[99.39] val:[94.62] Test:[93.06] | Best Val:[95.75] Update Test:[93.88] at Epoch:[62] | lr:0.000302
BIAS:[0.30] | Model:[GIN] Epoch:[64/100] Loss:[0.0217] Train:[99.39] val:[95.00] Test:[93.12] | Best Val:[95.75] Update Test:[93.88] at Epoch:[62] | lr:0.000288
BIAS:[0.30] | Model:[GIN] Epoch:[65/100] Loss:[0.0150] Train:[99.62] val:[94.88] Test:[94.31] | Best Val:[95.75] Update Test:[93.88] at Epoch:[62] | lr:0.000274
BIAS:[0.30] | Model:[GIN] Epoch:[66/100] Loss:[0.0093] Train:[99.86] val:[95.62] Test:[94.19] | Best Val:[95.75] Update Test:[93.88] at Epoch:[62] | lr:0.000260
BIAS:[0.30] | Model:[GIN] Epoch:[67/100] Loss:[0.0093] Train:[99.86] val:[95.00] Test:[94.12] | Best Val:[95.75] Update Test:[93.88] at Epoch:[62] | lr:0.000246
BIAS:[0.30] | Model:[GIN] Epoch:[68/100] Loss:[0.0088] Train:[99.82] val:[95.62] Test:[93.62] | Best Val:[95.75] Update Test:[93.88] at Epoch:[62] | lr:0.000233
BIAS:[0.30] | Model:[GIN] Epoch:[69/100] Loss:[0.0073] Train:[99.91] val:[95.25] Test:[94.06] | Best Val:[95.75] Update Test:[93.88] at Epoch:[62] | lr:0.000220
BIAS:[0.30] | Model:[GIN] Epoch:[70/100] Loss:[0.0073] Train:[99.87] val:[95.88] Test:[93.81] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000207
BIAS:[0.30] | Model:[GIN] Epoch:[71/100] Loss:[0.0087] Train:[99.87] val:[95.12] Test:[93.62] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000194
BIAS:[0.30] | Model:[GIN] Epoch:[72/100] Loss:[0.0073] Train:[99.87] val:[95.38] Test:[93.94] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000182
BIAS:[0.30] | Model:[GIN] Epoch:[73/100] Loss:[0.0055] Train:[99.95] val:[95.38] Test:[93.94] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000170
BIAS:[0.30] | Model:[GIN] Epoch:[74/100] Loss:[0.0051] Train:[99.93] val:[95.00] Test:[93.50] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000159
BIAS:[0.30] | Model:[GIN] Epoch:[75/100] Loss:[0.0050] Train:[99.95] val:[95.00] Test:[93.75] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000147
BIAS:[0.30] | Model:[GIN] Epoch:[76/100] Loss:[0.0049] Train:[99.89] val:[95.62] Test:[94.19] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000136
BIAS:[0.30] | Model:[GIN] Epoch:[77/100] Loss:[0.0046] Train:[99.95] val:[95.00] Test:[93.88] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000126
BIAS:[0.30] | Model:[GIN] Epoch:[78/100] Loss:[0.0039] Train:[99.95] val:[95.12] Test:[93.88] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000116
BIAS:[0.30] | Model:[GIN] Epoch:[79/100] Loss:[0.0047] Train:[99.95] val:[94.88] Test:[93.56] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000106
BIAS:[0.30] | Model:[GIN] Epoch:[80/100] Loss:[0.0040] Train:[99.96] val:[95.00] Test:[93.94] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000096
BIAS:[0.30] | Model:[GIN] Epoch:[81/100] Loss:[0.0039] Train:[99.95] val:[95.38] Test:[93.88] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000087
BIAS:[0.30] | Model:[GIN] Epoch:[82/100] Loss:[0.0040] Train:[99.96] val:[95.75] Test:[94.06] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000079
BIAS:[0.30] | Model:[GIN] Epoch:[83/100] Loss:[0.0042] Train:[99.95] val:[95.62] Test:[93.81] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000071
BIAS:[0.30] | Model:[GIN] Epoch:[84/100] Loss:[0.0032] Train:[99.96] val:[95.75] Test:[93.69] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000063
BIAS:[0.30] | Model:[GIN] Epoch:[85/100] Loss:[0.0027] Train:[99.98] val:[95.62] Test:[93.81] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000055
BIAS:[0.30] | Model:[GIN] Epoch:[86/100] Loss:[0.0026] Train:[99.98] val:[95.38] Test:[93.88] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000049
BIAS:[0.30] | Model:[GIN] Epoch:[87/100] Loss:[0.0031] Train:[99.96] val:[95.50] Test:[93.75] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000042
BIAS:[0.30] | Model:[GIN] Epoch:[88/100] Loss:[0.0031] Train:[99.96] val:[95.38] Test:[93.94] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000036
BIAS:[0.30] | Model:[GIN] Epoch:[89/100] Loss:[0.0037] Train:[99.93] val:[95.50] Test:[93.81] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000031
BIAS:[0.30] | Model:[GIN] Epoch:[90/100] Loss:[0.0039] Train:[99.96] val:[95.62] Test:[93.81] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000025
BIAS:[0.30] | Model:[GIN] Epoch:[91/100] Loss:[0.0029] Train:[99.95] val:[95.12] Test:[93.50] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000021
BIAS:[0.30] | Model:[GIN] Epoch:[92/100] Loss:[0.0024] Train:[99.98] val:[95.62] Test:[93.69] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000017
BIAS:[0.30] | Model:[GIN] Epoch:[93/100] Loss:[0.0030] Train:[99.95] val:[95.62] Test:[93.94] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000013
BIAS:[0.30] | Model:[GIN] Epoch:[94/100] Loss:[0.0031] Train:[99.96] val:[95.62] Test:[93.75] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000010
BIAS:[0.30] | Model:[GIN] Epoch:[95/100] Loss:[0.0031] Train:[99.96] val:[95.38] Test:[93.56] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000007
BIAS:[0.30] | Model:[GIN] Epoch:[96/100] Loss:[0.0029] Train:[99.96] val:[95.62] Test:[93.94] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000005
BIAS:[0.30] | Model:[GIN] Epoch:[97/100] Loss:[0.0026] Train:[99.96] val:[95.62] Test:[93.81] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000003
BIAS:[0.30] | Model:[GIN] Epoch:[98/100] Loss:[0.0026] Train:[99.96] val:[95.38] Test:[93.69] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000002
BIAS:[0.30] | Model:[GIN] Epoch:[99/100] Loss:[0.0026] Train:[99.96] val:[95.50] Test:[93.81] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000001
BIAS:[0.30] | Model:[GIN] Epoch:[100/100] Loss:[0.0032] Train:[99.96] val:[95.62] Test:[93.88] | Best Val:[95.88] Update Test:[93.81] at Epoch:[70] | lr:0.000001
syd: BIAS:[0.30] | Best Val acc:[95.88] Test acc:[93.81] at epoch:[70]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.3
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:420  , Cycle:979  , Grids:979  , Diams:979   
| BA  : House:979  , Cycle:420  , Grids:420  , Diams:420   
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:30.0%, Cycle:70.0%, Grids:70.0%, Diams:70.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:60   , Cycle:140  , Grids:140  , Diams:140   
| BA  : House:140  , Cycle:60   , Grids:60   , Diams:60    
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:30.0%, Cycle:70.0%, Grids:70.0%, Diams:70.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.30] | Model:[CausalGIN] Epoch:[1/100] Loss:[1.3148=0.0245+0.7805+1.0442] Train:[69.07] val:[28.00] Test:[27.44] | Update Test:[co:30.81,c:25.19,o:27.44] at Epoch:[1] | lr:0.001000
BIAS:[0.30] | Model:[CausalGIN] Epoch:[2/100] Loss:[0.6787=0.0035+0.4136+0.5267] Train:[84.79] val:[77.50] Test:[68.06] | Update Test:[co:65.19,c:13.00,o:68.06] at Epoch:[2] | lr:0.000999
BIAS:[0.30] | Model:[CausalGIN] Epoch:[3/100] Loss:[0.5254=0.0014+0.3327+0.3841] Train:[88.46] val:[79.00] Test:[69.75] | Update Test:[co:72.25,c:25.81,o:69.75] at Epoch:[3] | lr:0.000998
BIAS:[0.30] | Model:[CausalGIN] Epoch:[4/100] Loss:[0.4617=0.0007+0.2974+0.3279] Train:[89.72] val:[89.75] Test:[85.81] | Update Test:[co:82.44,c:25.56,o:85.81] at Epoch:[4] | lr:0.000996
BIAS:[0.30] | Model:[CausalGIN] Epoch:[5/100] Loss:[0.4397=0.0004+0.2840+0.3111] Train:[90.01] val:[89.88] Test:[87.06] | Update Test:[co:86.56,c:24.25,o:87.06] at Epoch:[5] | lr:0.000994
BIAS:[0.30] | Model:[CausalGIN] Epoch:[6/100] Loss:[0.3816=0.0004+0.2434+0.2759] Train:[91.87] val:[90.62] Test:[87.00] | Update Test:[co:87.19,c:25.06,o:87.00] at Epoch:[6] | lr:0.000991
BIAS:[0.30] | Model:[CausalGIN] Epoch:[7/100] Loss:[0.3824=0.0005+0.2459+0.2726] Train:[91.53] val:[88.25] Test:[88.12] | Update Test:[co:87.19,c:25.06,o:87.00] at Epoch:[6] | lr:0.000988
BIAS:[0.30] | Model:[CausalGIN] Epoch:[8/100] Loss:[0.3981=0.0004+0.2586+0.2785] Train:[90.96] val:[90.00] Test:[86.81] | Update Test:[co:87.19,c:25.06,o:87.00] at Epoch:[6] | lr:0.000984
BIAS:[0.30] | Model:[CausalGIN] Epoch:[9/100] Loss:[0.3363=0.0002+0.2164+0.2396] Train:[92.78] val:[83.50] Test:[82.31] | Update Test:[co:87.19,c:25.06,o:87.00] at Epoch:[6] | lr:0.000980
BIAS:[0.30] | Model:[CausalGIN] Epoch:[10/100] Loss:[0.3287=0.0002+0.2145+0.2281] Train:[92.74] val:[91.12] Test:[87.25] | Update Test:[co:83.44,c:24.88,o:87.25] at Epoch:[10] | lr:0.000976
BIAS:[0.30] | Model:[CausalGIN] Epoch:[11/100] Loss:[0.2884=0.0001+0.1860+0.2047] Train:[93.78] val:[81.75] Test:[77.19] | Update Test:[co:83.44,c:24.88,o:87.25] at Epoch:[10] | lr:0.000970
BIAS:[0.30] | Model:[CausalGIN] Epoch:[12/100] Loss:[0.2940=0.0001+0.1918+0.2043] Train:[93.19] val:[67.75] Test:[64.94] | Update Test:[co:83.44,c:24.88,o:87.25] at Epoch:[10] | lr:0.000965
BIAS:[0.30] | Model:[CausalGIN] Epoch:[13/100] Loss:[0.2940=0.0001+0.1900+0.2078] Train:[93.78] val:[90.62] Test:[90.44] | Update Test:[co:83.44,c:24.88,o:87.25] at Epoch:[10] | lr:0.000959
BIAS:[0.30] | Model:[CausalGIN] Epoch:[14/100] Loss:[0.2729=0.0001+0.1763+0.1932] Train:[93.96] val:[91.75] Test:[88.88] | Update Test:[co:88.06,c:26.38,o:88.88] at Epoch:[14] | lr:0.000952
BIAS:[0.30] | Model:[CausalGIN] Epoch:[15/100] Loss:[0.2422=0.0001+0.1561+0.1721] Train:[94.59] val:[92.25] Test:[88.69] | Update Test:[co:88.44,c:26.62,o:88.69] at Epoch:[15] | lr:0.000946
BIAS:[0.30] | Model:[CausalGIN] Epoch:[16/100] Loss:[0.2345=0.0001+0.1496+0.1698] Train:[94.71] val:[87.00] Test:[80.25] | Update Test:[co:88.44,c:26.62,o:88.69] at Epoch:[15] | lr:0.000938
BIAS:[0.30] | Model:[CausalGIN] Epoch:[17/100] Loss:[0.2261=0.0001+0.1441+0.1640] Train:[95.12] val:[93.62] Test:[89.19] | Update Test:[co:87.94,c:22.81,o:89.19] at Epoch:[17] | lr:0.000930
BIAS:[0.30] | Model:[CausalGIN] Epoch:[18/100] Loss:[0.2233=0.0001+0.1435+0.1596] Train:[95.03] val:[93.88] Test:[90.81] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000922
BIAS:[0.30] | Model:[CausalGIN] Epoch:[19/100] Loss:[0.2309=0.0001+0.1466+0.1684] Train:[95.05] val:[89.50] Test:[88.19] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000914
BIAS:[0.30] | Model:[CausalGIN] Epoch:[20/100] Loss:[0.2187=0.0001+0.1417+0.1541] Train:[95.12] val:[93.12] Test:[91.06] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000905
BIAS:[0.30] | Model:[CausalGIN] Epoch:[21/100] Loss:[0.2051=0.0001+0.1321+0.1459] Train:[95.35] val:[93.75] Test:[92.62] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000895
BIAS:[0.30] | Model:[CausalGIN] Epoch:[22/100] Loss:[0.2014=0.0001+0.1308+0.1411] Train:[95.43] val:[91.88] Test:[87.38] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000885
BIAS:[0.30] | Model:[CausalGIN] Epoch:[23/100] Loss:[0.1769=0.0001+0.1132+0.1274] Train:[96.18] val:[82.88] Test:[81.81] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000875
BIAS:[0.30] | Model:[CausalGIN] Epoch:[24/100] Loss:[0.2036=0.0000+0.1312+0.1446] Train:[95.60] val:[92.62] Test:[89.94] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000865
BIAS:[0.30] | Model:[CausalGIN] Epoch:[25/100] Loss:[0.1633=0.0000+0.1014+0.1239] Train:[96.69] val:[93.00] Test:[91.94] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000854
BIAS:[0.30] | Model:[CausalGIN] Epoch:[26/100] Loss:[0.1861=0.0001+0.1204+0.1314] Train:[95.96] val:[87.25] Test:[87.75] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000842
BIAS:[0.30] | Model:[CausalGIN] Epoch:[27/100] Loss:[0.1607=0.0001+0.1034+0.1147] Train:[96.57] val:[88.38] Test:[81.44] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000831
BIAS:[0.30] | Model:[CausalGIN] Epoch:[28/100] Loss:[0.1540=0.0000+0.0983+0.1115] Train:[96.59] val:[93.88] Test:[89.31] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000819
BIAS:[0.30] | Model:[CausalGIN] Epoch:[29/100] Loss:[0.1615=0.0001+0.1024+0.1181] Train:[96.53] val:[93.62] Test:[90.06] | Update Test:[co:89.75,c:25.44,o:90.81] at Epoch:[18] | lr:0.000807
BIAS:[0.30] | Model:[CausalGIN] Epoch:[30/100] Loss:[0.1425=0.0001+0.0910+0.1028] Train:[96.68] val:[94.25] Test:[92.50] | Update Test:[co:90.62,c:20.75,o:92.50] at Epoch:[30] | lr:0.000794
BIAS:[0.30] | Model:[CausalGIN] Epoch:[31/100] Loss:[0.1336=0.0000+0.0850+0.0972] Train:[97.09] val:[95.00] Test:[93.38] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000781
BIAS:[0.30] | Model:[CausalGIN] Epoch:[32/100] Loss:[0.1365=0.0000+0.0867+0.0996] Train:[96.91] val:[90.88] Test:[87.56] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000768
BIAS:[0.30] | Model:[CausalGIN] Epoch:[33/100] Loss:[0.1263=0.0000+0.0808+0.0910] Train:[97.21] val:[93.50] Test:[90.94] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000755
BIAS:[0.30] | Model:[CausalGIN] Epoch:[34/100] Loss:[0.1817=0.0001+0.1160+0.1312] Train:[95.89] val:[91.12] Test:[90.94] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000741
BIAS:[0.30] | Model:[CausalGIN] Epoch:[35/100] Loss:[0.1351=0.0001+0.0854+0.0994] Train:[97.09] val:[94.50] Test:[93.00] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000727
BIAS:[0.30] | Model:[CausalGIN] Epoch:[36/100] Loss:[0.1246=0.0001+0.0781+0.0929] Train:[97.34] val:[94.25] Test:[93.19] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000713
BIAS:[0.30] | Model:[CausalGIN] Epoch:[37/100] Loss:[0.1058=0.0000+0.0671+0.0774] Train:[97.89] val:[94.75] Test:[93.31] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000699
BIAS:[0.30] | Model:[CausalGIN] Epoch:[38/100] Loss:[0.0907=0.0000+0.0563+0.0686] Train:[98.21] val:[93.50] Test:[91.25] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000684
BIAS:[0.30] | Model:[CausalGIN] Epoch:[39/100] Loss:[0.0867=0.0000+0.0540+0.0653] Train:[98.43] val:[94.00] Test:[93.50] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000670
BIAS:[0.30] | Model:[CausalGIN] Epoch:[40/100] Loss:[0.0950=0.0000+0.0597+0.0705] Train:[97.98] val:[94.62] Test:[91.25] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000655
BIAS:[0.30] | Model:[CausalGIN] Epoch:[41/100] Loss:[0.0820=0.0000+0.0509+0.0622] Train:[98.30] val:[92.88] Test:[88.25] | Update Test:[co:92.00,c:25.25,o:93.38] at Epoch:[31] | lr:0.000640
BIAS:[0.30] | Model:[CausalGIN] Epoch:[42/100] Loss:[0.0890=0.0000+0.0563+0.0654] Train:[98.28] val:[95.38] Test:[93.81] | Update Test:[co:93.12,c:25.50,o:93.81] at Epoch:[42] | lr:0.000625
BIAS:[0.30] | Model:[CausalGIN] Epoch:[43/100] Loss:[0.0740=0.0001+0.0463+0.0553] Train:[98.53] val:[94.88] Test:[93.69] | Update Test:[co:93.12,c:25.50,o:93.81] at Epoch:[42] | lr:0.000609
BIAS:[0.30] | Model:[CausalGIN] Epoch:[44/100] Loss:[0.0824=0.0001+0.0528+0.0593] Train:[98.23] val:[95.38] Test:[92.31] | Update Test:[co:93.12,c:25.50,o:93.81] at Epoch:[42] | lr:0.000594
BIAS:[0.30] | Model:[CausalGIN] Epoch:[45/100] Loss:[0.0683=0.0000+0.0427+0.0511] Train:[98.52] val:[95.12] Test:[92.75] | Update Test:[co:93.12,c:25.50,o:93.81] at Epoch:[42] | lr:0.000579
BIAS:[0.30] | Model:[CausalGIN] Epoch:[46/100] Loss:[0.0585=0.0000+0.0361+0.0448] Train:[98.96] val:[95.62] Test:[93.94] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000563
BIAS:[0.30] | Model:[CausalGIN] Epoch:[47/100] Loss:[0.0570=0.0000+0.0352+0.0435] Train:[98.84] val:[94.75] Test:[93.06] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000548
BIAS:[0.30] | Model:[CausalGIN] Epoch:[48/100] Loss:[0.0693=0.0000+0.0443+0.0500] Train:[98.64] val:[95.12] Test:[92.75] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000532
BIAS:[0.30] | Model:[CausalGIN] Epoch:[49/100] Loss:[0.0447=0.0001+0.0272+0.0350] Train:[99.00] val:[94.62] Test:[93.00] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000516
BIAS:[0.30] | Model:[CausalGIN] Epoch:[50/100] Loss:[0.0519=0.0000+0.0321+0.0397] Train:[98.96] val:[95.38] Test:[93.69] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000501
BIAS:[0.30] | Model:[CausalGIN] Epoch:[51/100] Loss:[0.0564=0.0000+0.0358+0.0412] Train:[98.95] val:[95.50] Test:[94.94] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000485
BIAS:[0.30] | Model:[CausalGIN] Epoch:[52/100] Loss:[0.0491=0.0000+0.0300+0.0380] Train:[99.18] val:[95.00] Test:[93.56] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000469
BIAS:[0.30] | Model:[CausalGIN] Epoch:[53/100] Loss:[0.0509=0.0000+0.0322+0.0374] Train:[98.93] val:[95.12] Test:[93.94] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000453
BIAS:[0.30] | Model:[CausalGIN] Epoch:[54/100] Loss:[0.0353=0.0000+0.0216+0.0274] Train:[99.45] val:[95.25] Test:[93.62] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000438
BIAS:[0.30] | Model:[CausalGIN] Epoch:[55/100] Loss:[0.0248=0.0000+0.0150+0.0196] Train:[99.68] val:[95.25] Test:[94.31] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000422
BIAS:[0.30] | Model:[CausalGIN] Epoch:[56/100] Loss:[0.0258=0.0000+0.0158+0.0201] Train:[99.59] val:[95.38] Test:[93.88] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000407
BIAS:[0.30] | Model:[CausalGIN] Epoch:[57/100] Loss:[0.0194=0.0000+0.0115+0.0158] Train:[99.79] val:[94.88] Test:[94.00] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000392
BIAS:[0.30] | Model:[CausalGIN] Epoch:[58/100] Loss:[0.0227=0.0000+0.0137+0.0180] Train:[99.64] val:[94.75] Test:[93.75] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000376
BIAS:[0.30] | Model:[CausalGIN] Epoch:[59/100] Loss:[0.0214=0.0000+0.0126+0.0176] Train:[99.66] val:[95.25] Test:[93.81] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000361
BIAS:[0.30] | Model:[CausalGIN] Epoch:[60/100] Loss:[0.0185=0.0000+0.0109+0.0152] Train:[99.73] val:[94.38] Test:[94.06] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000346
BIAS:[0.30] | Model:[CausalGIN] Epoch:[61/100] Loss:[0.0125=0.0000+0.0075+0.0100] Train:[99.87] val:[95.00] Test:[93.81] | Update Test:[co:93.19,c:26.75,o:93.94] at Epoch:[46] | lr:0.000331
BIAS:[0.30] | Model:[CausalGIN] Epoch:[62/100] Loss:[0.0122=0.0000+0.0073+0.0099] Train:[99.93] val:[96.00] Test:[93.75] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000317
BIAS:[0.30] | Model:[CausalGIN] Epoch:[63/100] Loss:[0.0143=0.0000+0.0089+0.0107] Train:[99.82] val:[95.00] Test:[93.06] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000302
BIAS:[0.30] | Model:[CausalGIN] Epoch:[64/100] Loss:[0.0109=0.0000+0.0064+0.0088] Train:[99.91] val:[95.25] Test:[94.12] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000288
BIAS:[0.30] | Model:[CausalGIN] Epoch:[65/100] Loss:[0.0078=0.0000+0.0047+0.0062] Train:[99.91] val:[94.62] Test:[93.31] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000274
BIAS:[0.30] | Model:[CausalGIN] Epoch:[66/100] Loss:[0.0084=0.0000+0.0050+0.0067] Train:[99.93] val:[95.62] Test:[94.44] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000260
BIAS:[0.30] | Model:[CausalGIN] Epoch:[67/100] Loss:[0.0073=0.0000+0.0047+0.0052] Train:[99.91] val:[95.50] Test:[93.81] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000246
BIAS:[0.30] | Model:[CausalGIN] Epoch:[68/100] Loss:[0.0075=0.0000+0.0046+0.0058] Train:[99.95] val:[95.38] Test:[93.88] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000233
BIAS:[0.30] | Model:[CausalGIN] Epoch:[69/100] Loss:[0.0109=0.0000+0.0065+0.0089] Train:[99.86] val:[95.12] Test:[93.19] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000220
BIAS:[0.30] | Model:[CausalGIN] Epoch:[70/100] Loss:[0.0113=0.0000+0.0070+0.0086] Train:[99.89] val:[94.50] Test:[94.00] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000207
BIAS:[0.30] | Model:[CausalGIN] Epoch:[71/100] Loss:[0.0071=0.0000+0.0041+0.0061] Train:[99.96] val:[95.38] Test:[94.06] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000194
BIAS:[0.30] | Model:[CausalGIN] Epoch:[72/100] Loss:[0.0063=0.0000+0.0037+0.0051] Train:[99.95] val:[95.50] Test:[93.69] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000182
BIAS:[0.30] | Model:[CausalGIN] Epoch:[73/100] Loss:[0.0044=0.0000+0.0028+0.0032] Train:[99.98] val:[95.62] Test:[93.62] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000170
BIAS:[0.30] | Model:[CausalGIN] Epoch:[74/100] Loss:[0.0047=0.0000+0.0027+0.0040] Train:[99.96] val:[95.88] Test:[94.00] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000159
BIAS:[0.30] | Model:[CausalGIN] Epoch:[75/100] Loss:[0.0060=0.0000+0.0039+0.0043] Train:[99.91] val:[96.00] Test:[93.88] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000147
BIAS:[0.30] | Model:[CausalGIN] Epoch:[76/100] Loss:[0.0068=0.0000+0.0044+0.0048] Train:[99.95] val:[95.62] Test:[93.62] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000136
BIAS:[0.30] | Model:[CausalGIN] Epoch:[77/100] Loss:[0.0058=0.0000+0.0036+0.0045] Train:[99.98] val:[95.38] Test:[93.44] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000126
BIAS:[0.30] | Model:[CausalGIN] Epoch:[78/100] Loss:[0.0063=0.0000+0.0038+0.0050] Train:[99.89] val:[95.50] Test:[93.81] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000116
BIAS:[0.30] | Model:[CausalGIN] Epoch:[79/100] Loss:[0.0047=0.0000+0.0029+0.0036] Train:[99.96] val:[95.75] Test:[93.75] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000106
BIAS:[0.30] | Model:[CausalGIN] Epoch:[80/100] Loss:[0.0042=0.0000+0.0026+0.0031] Train:[99.98] val:[95.62] Test:[93.38] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000096
BIAS:[0.30] | Model:[CausalGIN] Epoch:[81/100] Loss:[0.0044=0.0000+0.0026+0.0035] Train:[99.98] val:[95.62] Test:[93.50] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000087
BIAS:[0.30] | Model:[CausalGIN] Epoch:[82/100] Loss:[0.0029=0.0000+0.0018+0.0021] Train:[99.98] val:[95.50] Test:[93.50] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000079
BIAS:[0.30] | Model:[CausalGIN] Epoch:[83/100] Loss:[0.0050=0.0000+0.0031+0.0039] Train:[99.93] val:[95.75] Test:[93.62] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000071
BIAS:[0.30] | Model:[CausalGIN] Epoch:[84/100] Loss:[0.0037=0.0000+0.0024+0.0027] Train:[99.98] val:[95.62] Test:[93.62] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000063
BIAS:[0.30] | Model:[CausalGIN] Epoch:[85/100] Loss:[0.0031=0.0000+0.0019+0.0024] Train:[99.96] val:[95.62] Test:[93.38] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000055
BIAS:[0.30] | Model:[CausalGIN] Epoch:[86/100] Loss:[0.0040=0.0000+0.0024+0.0033] Train:[99.96] val:[95.88] Test:[93.81] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000049
BIAS:[0.30] | Model:[CausalGIN] Epoch:[87/100] Loss:[0.0036=0.0000+0.0023+0.0027] Train:[99.96] val:[95.38] Test:[93.69] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000042
BIAS:[0.30] | Model:[CausalGIN] Epoch:[88/100] Loss:[0.0035=0.0000+0.0022+0.0027] Train:[99.96] val:[95.88] Test:[93.44] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000036
BIAS:[0.30] | Model:[CausalGIN] Epoch:[89/100] Loss:[0.0036=0.0000+0.0022+0.0026] Train:[99.98] val:[96.00] Test:[93.69] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000031
BIAS:[0.30] | Model:[CausalGIN] Epoch:[90/100] Loss:[0.0036=0.0000+0.0022+0.0028] Train:[99.98] val:[96.00] Test:[93.69] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000025
BIAS:[0.30] | Model:[CausalGIN] Epoch:[91/100] Loss:[0.0034=0.0000+0.0021+0.0025] Train:[99.96] val:[95.50] Test:[93.69] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000021
BIAS:[0.30] | Model:[CausalGIN] Epoch:[92/100] Loss:[0.0036=0.0000+0.0022+0.0028] Train:[99.98] val:[95.75] Test:[93.56] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000017
BIAS:[0.30] | Model:[CausalGIN] Epoch:[93/100] Loss:[0.0029=0.0000+0.0018+0.0023] Train:[99.98] val:[95.62] Test:[93.62] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000013
BIAS:[0.30] | Model:[CausalGIN] Epoch:[94/100] Loss:[0.0036=0.0000+0.0022+0.0027] Train:[99.98] val:[95.75] Test:[93.75] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000010
BIAS:[0.30] | Model:[CausalGIN] Epoch:[95/100] Loss:[0.0030=0.0000+0.0019+0.0023] Train:[99.98] val:[95.88] Test:[93.69] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000007
BIAS:[0.30] | Model:[CausalGIN] Epoch:[96/100] Loss:[0.0030=0.0000+0.0018+0.0022] Train:[99.96] val:[95.75] Test:[93.38] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000005
BIAS:[0.30] | Model:[CausalGIN] Epoch:[97/100] Loss:[0.0032=0.0000+0.0020+0.0024] Train:[99.98] val:[95.88] Test:[93.62] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000003
BIAS:[0.30] | Model:[CausalGIN] Epoch:[98/100] Loss:[0.0042=0.0000+0.0027+0.0030] Train:[99.96] val:[96.00] Test:[93.62] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000002
BIAS:[0.30] | Model:[CausalGIN] Epoch:[99/100] Loss:[0.0034=0.0000+0.0021+0.0027] Train:[99.98] val:[95.88] Test:[93.62] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000001
BIAS:[0.30] | Model:[CausalGIN] Epoch:[100/100] Loss:[0.0032=0.0000+0.0020+0.0024] Train:[99.98] val:[95.75] Test:[93.75] | Update Test:[co:93.06,c:23.88,o:93.75] at Epoch:[62] | lr:0.000001
syd: BIAS:[0.30] | Val acc:[95.75] Test acc:[co:93.06,c:23.88,o:93.75] at epoch:[62]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.5
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5600
| Tree: House:700  , Cycle:700  , Grids:700  , Diams:700   
| BA  : House:700  , Cycle:700  , Grids:700  , Diams:700   
| All : House:1400 , Cycle:1400 , Grids:1400 , Diams:1400  
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:100  , Cycle:100  , Grids:100  , Diams:100   
| BA  : House:100  , Cycle:100  , Grids:100  , Diams:100   
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.50] | Model:[GIN] Epoch:[1/100] Loss:[1.1475] Train:[48.16] val:[48.50] Test:[47.56] | Best Val:[48.50] Update Test:[47.56] at Epoch:[1] | lr:0.001000
BIAS:[0.50] | Model:[GIN] Epoch:[2/100] Loss:[0.6767] Train:[75.05] val:[79.62] Test:[77.88] | Best Val:[79.62] Update Test:[77.88] at Epoch:[2] | lr:0.000999
BIAS:[0.50] | Model:[GIN] Epoch:[3/100] Loss:[0.4532] Train:[83.93] val:[85.00] Test:[84.31] | Best Val:[85.00] Update Test:[84.31] at Epoch:[3] | lr:0.000998
BIAS:[0.50] | Model:[GIN] Epoch:[4/100] Loss:[0.3813] Train:[86.80] val:[85.38] Test:[86.38] | Best Val:[85.38] Update Test:[86.38] at Epoch:[4] | lr:0.000996
BIAS:[0.50] | Model:[GIN] Epoch:[5/100] Loss:[0.3363] Train:[87.96] val:[82.25] Test:[82.38] | Best Val:[85.38] Update Test:[86.38] at Epoch:[4] | lr:0.000994
BIAS:[0.50] | Model:[GIN] Epoch:[6/100] Loss:[0.3427] Train:[87.41] val:[84.88] Test:[84.56] | Best Val:[85.38] Update Test:[86.38] at Epoch:[4] | lr:0.000991
BIAS:[0.50] | Model:[GIN] Epoch:[7/100] Loss:[0.2883] Train:[90.21] val:[90.25] Test:[90.00] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000988
BIAS:[0.50] | Model:[GIN] Epoch:[8/100] Loss:[0.2833] Train:[89.91] val:[87.50] Test:[86.88] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000984
BIAS:[0.50] | Model:[GIN] Epoch:[9/100] Loss:[0.2800] Train:[90.30] val:[87.00] Test:[86.00] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000980
BIAS:[0.50] | Model:[GIN] Epoch:[10/100] Loss:[0.2772] Train:[90.32] val:[88.00] Test:[86.75] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000976
BIAS:[0.50] | Model:[GIN] Epoch:[11/100] Loss:[0.2438] Train:[91.55] val:[89.88] Test:[89.94] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000970
BIAS:[0.50] | Model:[GIN] Epoch:[12/100] Loss:[0.2320] Train:[92.11] val:[89.75] Test:[90.25] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000965
BIAS:[0.50] | Model:[GIN] Epoch:[13/100] Loss:[0.2245] Train:[92.27] val:[90.12] Test:[89.69] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000959
BIAS:[0.50] | Model:[GIN] Epoch:[14/100] Loss:[0.2087] Train:[92.88] val:[87.38] Test:[87.62] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000952
BIAS:[0.50] | Model:[GIN] Epoch:[15/100] Loss:[0.1977] Train:[93.12] val:[87.62] Test:[86.50] | Best Val:[90.25] Update Test:[90.00] at Epoch:[7] | lr:0.000946
BIAS:[0.50] | Model:[GIN] Epoch:[16/100] Loss:[0.1992] Train:[93.04] val:[91.50] Test:[91.19] | Best Val:[91.50] Update Test:[91.19] at Epoch:[16] | lr:0.000938
BIAS:[0.50] | Model:[GIN] Epoch:[17/100] Loss:[0.2005] Train:[93.82] val:[90.38] Test:[90.31] | Best Val:[91.50] Update Test:[91.19] at Epoch:[16] | lr:0.000930
BIAS:[0.50] | Model:[GIN] Epoch:[18/100] Loss:[0.1938] Train:[93.57] val:[75.88] Test:[78.44] | Best Val:[91.50] Update Test:[91.19] at Epoch:[16] | lr:0.000922
BIAS:[0.50] | Model:[GIN] Epoch:[19/100] Loss:[0.1928] Train:[93.30] val:[89.75] Test:[88.88] | Best Val:[91.50] Update Test:[91.19] at Epoch:[16] | lr:0.000914
BIAS:[0.50] | Model:[GIN] Epoch:[20/100] Loss:[0.1885] Train:[93.43] val:[92.38] Test:[90.62] | Best Val:[92.38] Update Test:[90.62] at Epoch:[20] | lr:0.000905
BIAS:[0.50] | Model:[GIN] Epoch:[21/100] Loss:[0.1658] Train:[94.27] val:[78.12] Test:[80.44] | Best Val:[92.38] Update Test:[90.62] at Epoch:[20] | lr:0.000895
BIAS:[0.50] | Model:[GIN] Epoch:[22/100] Loss:[0.1705] Train:[94.75] val:[92.50] Test:[93.38] | Best Val:[92.50] Update Test:[93.38] at Epoch:[22] | lr:0.000885
BIAS:[0.50] | Model:[GIN] Epoch:[23/100] Loss:[0.1523] Train:[94.80] val:[91.00] Test:[90.81] | Best Val:[92.50] Update Test:[93.38] at Epoch:[22] | lr:0.000875
BIAS:[0.50] | Model:[GIN] Epoch:[24/100] Loss:[0.1475] Train:[95.30] val:[93.62] Test:[93.75] | Best Val:[93.62] Update Test:[93.75] at Epoch:[24] | lr:0.000865
BIAS:[0.50] | Model:[GIN] Epoch:[25/100] Loss:[0.1451] Train:[95.05] val:[94.62] Test:[94.31] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000854
BIAS:[0.50] | Model:[GIN] Epoch:[26/100] Loss:[0.1593] Train:[94.52] val:[91.62] Test:[92.00] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000842
BIAS:[0.50] | Model:[GIN] Epoch:[27/100] Loss:[0.1514] Train:[94.86] val:[92.88] Test:[93.69] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000831
BIAS:[0.50] | Model:[GIN] Epoch:[28/100] Loss:[0.1364] Train:[95.20] val:[88.75] Test:[88.50] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000819
BIAS:[0.50] | Model:[GIN] Epoch:[29/100] Loss:[0.1367] Train:[95.32] val:[92.25] Test:[92.00] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000807
BIAS:[0.50] | Model:[GIN] Epoch:[30/100] Loss:[0.1357] Train:[95.09] val:[93.75] Test:[93.00] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000794
BIAS:[0.50] | Model:[GIN] Epoch:[31/100] Loss:[0.1467] Train:[95.18] val:[76.50] Test:[75.44] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000781
BIAS:[0.50] | Model:[GIN] Epoch:[32/100] Loss:[0.1475] Train:[94.84] val:[92.75] Test:[92.62] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000768
BIAS:[0.50] | Model:[GIN] Epoch:[33/100] Loss:[0.1441] Train:[94.62] val:[92.00] Test:[92.25] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000755
BIAS:[0.50] | Model:[GIN] Epoch:[34/100] Loss:[0.1362] Train:[95.18] val:[92.88] Test:[92.62] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000741
BIAS:[0.50] | Model:[GIN] Epoch:[35/100] Loss:[0.1547] Train:[94.96] val:[91.75] Test:[93.31] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000727
BIAS:[0.50] | Model:[GIN] Epoch:[36/100] Loss:[0.1183] Train:[96.00] val:[94.62] Test:[94.75] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000713
BIAS:[0.50] | Model:[GIN] Epoch:[37/100] Loss:[0.1083] Train:[96.41] val:[93.00] Test:[93.56] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000699
BIAS:[0.50] | Model:[GIN] Epoch:[38/100] Loss:[0.1075] Train:[96.52] val:[93.88] Test:[93.69] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000684
BIAS:[0.50] | Model:[GIN] Epoch:[39/100] Loss:[0.1037] Train:[96.25] val:[87.62] Test:[87.44] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000670
BIAS:[0.50] | Model:[GIN] Epoch:[40/100] Loss:[0.1042] Train:[96.34] val:[92.00] Test:[91.56] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000655
BIAS:[0.50] | Model:[GIN] Epoch:[41/100] Loss:[0.0906] Train:[97.18] val:[93.50] Test:[93.62] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000640
BIAS:[0.50] | Model:[GIN] Epoch:[42/100] Loss:[0.0856] Train:[97.36] val:[94.38] Test:[94.06] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000625
BIAS:[0.50] | Model:[GIN] Epoch:[43/100] Loss:[0.1173] Train:[96.16] val:[92.62] Test:[93.25] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000609
BIAS:[0.50] | Model:[GIN] Epoch:[44/100] Loss:[0.0887] Train:[97.20] val:[91.62] Test:[92.12] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000594
BIAS:[0.50] | Model:[GIN] Epoch:[45/100] Loss:[0.0780] Train:[97.43] val:[94.38] Test:[94.50] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000579
BIAS:[0.50] | Model:[GIN] Epoch:[46/100] Loss:[0.0835] Train:[97.45] val:[94.50] Test:[93.69] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000563
BIAS:[0.50] | Model:[GIN] Epoch:[47/100] Loss:[0.0783] Train:[97.34] val:[94.25] Test:[94.44] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000548
BIAS:[0.50] | Model:[GIN] Epoch:[48/100] Loss:[0.0785] Train:[97.50] val:[89.12] Test:[90.62] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000532
BIAS:[0.50] | Model:[GIN] Epoch:[49/100] Loss:[0.0774] Train:[97.36] val:[93.62] Test:[94.19] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000516
BIAS:[0.50] | Model:[GIN] Epoch:[50/100] Loss:[0.0538] Train:[98.43] val:[94.25] Test:[94.38] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000501
BIAS:[0.50] | Model:[GIN] Epoch:[51/100] Loss:[0.0478] Train:[98.50] val:[94.12] Test:[93.81] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000485
BIAS:[0.50] | Model:[GIN] Epoch:[52/100] Loss:[0.0594] Train:[98.09] val:[89.00] Test:[89.56] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000469
BIAS:[0.50] | Model:[GIN] Epoch:[53/100] Loss:[0.0487] Train:[98.46] val:[94.25] Test:[93.88] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000453
BIAS:[0.50] | Model:[GIN] Epoch:[54/100] Loss:[0.0472] Train:[98.54] val:[93.88] Test:[94.00] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000438
BIAS:[0.50] | Model:[GIN] Epoch:[55/100] Loss:[0.0433] Train:[98.84] val:[94.62] Test:[94.50] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000422
BIAS:[0.50] | Model:[GIN] Epoch:[56/100] Loss:[0.0362] Train:[98.98] val:[94.25] Test:[94.06] | Best Val:[94.62] Update Test:[94.31] at Epoch:[25] | lr:0.000407
BIAS:[0.50] | Model:[GIN] Epoch:[57/100] Loss:[0.0407] Train:[98.73] val:[94.88] Test:[94.00] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000392
BIAS:[0.50] | Model:[GIN] Epoch:[58/100] Loss:[0.0459] Train:[98.62] val:[92.75] Test:[93.56] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000376
BIAS:[0.50] | Model:[GIN] Epoch:[59/100] Loss:[0.0335] Train:[99.09] val:[93.88] Test:[94.19] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000361
BIAS:[0.50] | Model:[GIN] Epoch:[60/100] Loss:[0.0294] Train:[99.29] val:[94.50] Test:[94.50] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000346
BIAS:[0.50] | Model:[GIN] Epoch:[61/100] Loss:[0.0225] Train:[99.36] val:[94.62] Test:[95.00] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000331
BIAS:[0.50] | Model:[GIN] Epoch:[62/100] Loss:[0.0262] Train:[99.30] val:[94.38] Test:[93.81] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000317
BIAS:[0.50] | Model:[GIN] Epoch:[63/100] Loss:[0.0292] Train:[99.20] val:[94.12] Test:[94.00] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000302
BIAS:[0.50] | Model:[GIN] Epoch:[64/100] Loss:[0.0241] Train:[99.34] val:[94.88] Test:[94.38] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000288
BIAS:[0.50] | Model:[GIN] Epoch:[65/100] Loss:[0.0217] Train:[99.52] val:[94.75] Test:[94.69] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000274
BIAS:[0.50] | Model:[GIN] Epoch:[66/100] Loss:[0.0150] Train:[99.77] val:[94.50] Test:[94.50] | Best Val:[94.88] Update Test:[94.00] at Epoch:[57] | lr:0.000260
BIAS:[0.50] | Model:[GIN] Epoch:[67/100] Loss:[0.0131] Train:[99.77] val:[95.38] Test:[94.56] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000246
BIAS:[0.50] | Model:[GIN] Epoch:[68/100] Loss:[0.0123] Train:[99.77] val:[94.88] Test:[94.94] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000233
BIAS:[0.50] | Model:[GIN] Epoch:[69/100] Loss:[0.0141] Train:[99.71] val:[94.88] Test:[94.38] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000220
BIAS:[0.50] | Model:[GIN] Epoch:[70/100] Loss:[0.0172] Train:[99.52] val:[94.38] Test:[93.81] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000207
BIAS:[0.50] | Model:[GIN] Epoch:[71/100] Loss:[0.0131] Train:[99.73] val:[94.50] Test:[94.06] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000194
BIAS:[0.50] | Model:[GIN] Epoch:[72/100] Loss:[0.0170] Train:[99.61] val:[94.88] Test:[94.62] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000182
BIAS:[0.50] | Model:[GIN] Epoch:[73/100] Loss:[0.0120] Train:[99.79] val:[94.50] Test:[94.44] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000170
BIAS:[0.50] | Model:[GIN] Epoch:[74/100] Loss:[0.0097] Train:[99.88] val:[94.75] Test:[94.12] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000159
BIAS:[0.50] | Model:[GIN] Epoch:[75/100] Loss:[0.0095] Train:[99.86] val:[94.62] Test:[94.94] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000147
BIAS:[0.50] | Model:[GIN] Epoch:[76/100] Loss:[0.0091] Train:[99.86] val:[94.62] Test:[95.06] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000136
BIAS:[0.50] | Model:[GIN] Epoch:[77/100] Loss:[0.0081] Train:[99.89] val:[94.88] Test:[94.94] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000126
BIAS:[0.50] | Model:[GIN] Epoch:[78/100] Loss:[0.0083] Train:[99.88] val:[94.50] Test:[94.75] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000116
BIAS:[0.50] | Model:[GIN] Epoch:[79/100] Loss:[0.0066] Train:[99.95] val:[94.88] Test:[95.12] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000106
BIAS:[0.50] | Model:[GIN] Epoch:[80/100] Loss:[0.0076] Train:[99.91] val:[94.50] Test:[95.00] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000096
BIAS:[0.50] | Model:[GIN] Epoch:[81/100] Loss:[0.0070] Train:[99.95] val:[95.00] Test:[94.94] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000087
BIAS:[0.50] | Model:[GIN] Epoch:[82/100] Loss:[0.0070] Train:[99.95] val:[95.00] Test:[94.94] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000079
BIAS:[0.50] | Model:[GIN] Epoch:[83/100] Loss:[0.0066] Train:[99.93] val:[95.00] Test:[94.88] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000071
BIAS:[0.50] | Model:[GIN] Epoch:[84/100] Loss:[0.0069] Train:[99.93] val:[95.12] Test:[95.31] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000063
BIAS:[0.50] | Model:[GIN] Epoch:[85/100] Loss:[0.0064] Train:[99.95] val:[94.75] Test:[95.12] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000055
BIAS:[0.50] | Model:[GIN] Epoch:[86/100] Loss:[0.0058] Train:[99.96] val:[95.00] Test:[95.19] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000049
BIAS:[0.50] | Model:[GIN] Epoch:[87/100] Loss:[0.0056] Train:[99.95] val:[95.00] Test:[94.81] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000042
BIAS:[0.50] | Model:[GIN] Epoch:[88/100] Loss:[0.0055] Train:[99.96] val:[94.88] Test:[95.00] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000036
BIAS:[0.50] | Model:[GIN] Epoch:[89/100] Loss:[0.0052] Train:[99.96] val:[94.75] Test:[94.94] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000031
BIAS:[0.50] | Model:[GIN] Epoch:[90/100] Loss:[0.0065] Train:[99.93] val:[95.38] Test:[94.81] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000025
BIAS:[0.50] | Model:[GIN] Epoch:[91/100] Loss:[0.0058] Train:[99.95] val:[94.88] Test:[94.94] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000021
BIAS:[0.50] | Model:[GIN] Epoch:[92/100] Loss:[0.0053] Train:[99.96] val:[94.75] Test:[94.88] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000017
BIAS:[0.50] | Model:[GIN] Epoch:[93/100] Loss:[0.0062] Train:[99.88] val:[94.75] Test:[95.00] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000013
BIAS:[0.50] | Model:[GIN] Epoch:[94/100] Loss:[0.0055] Train:[99.93] val:[95.12] Test:[95.06] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000010
BIAS:[0.50] | Model:[GIN] Epoch:[95/100] Loss:[0.0064] Train:[99.91] val:[95.00] Test:[95.06] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000007
BIAS:[0.50] | Model:[GIN] Epoch:[96/100] Loss:[0.0055] Train:[99.96] val:[95.00] Test:[95.00] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000005
BIAS:[0.50] | Model:[GIN] Epoch:[97/100] Loss:[0.0048] Train:[99.96] val:[95.12] Test:[95.25] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000003
BIAS:[0.50] | Model:[GIN] Epoch:[98/100] Loss:[0.0051] Train:[99.96] val:[94.75] Test:[95.00] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000002
BIAS:[0.50] | Model:[GIN] Epoch:[99/100] Loss:[0.0050] Train:[99.96] val:[94.75] Test:[94.94] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000001
BIAS:[0.50] | Model:[GIN] Epoch:[100/100] Loss:[0.0054] Train:[99.95] val:[94.75] Test:[95.06] | Best Val:[95.38] Update Test:[94.56] at Epoch:[67] | lr:0.000001
syd: BIAS:[0.50] | Best Val acc:[95.38] Test acc:[94.56] at epoch:[67]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.5
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5600
| Tree: House:700  , Cycle:700  , Grids:700  , Diams:700   
| BA  : House:700  , Cycle:700  , Grids:700  , Diams:700   
| All : House:1400 , Cycle:1400 , Grids:1400 , Diams:1400  
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:100  , Cycle:100  , Grids:100  , Diams:100   
| BA  : House:100  , Cycle:100  , Grids:100  , Diams:100   
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.50] | Model:[CausalGIN] Epoch:[1/100] Loss:[1.6226=0.0294+0.9992+1.2175] Train:[58.13] val:[26.62] Test:[26.38] | Update Test:[co:25.69,c:24.38,o:26.38] at Epoch:[1] | lr:0.001000
BIAS:[0.50] | Model:[CausalGIN] Epoch:[2/100] Loss:[0.8101=0.0052+0.4992+0.6166] Train:[81.32] val:[78.38] Test:[77.88] | Update Test:[co:74.88,c:18.38,o:77.88] at Epoch:[2] | lr:0.000999
BIAS:[0.50] | Model:[CausalGIN] Epoch:[3/100] Loss:[0.6212=0.0024+0.3984+0.4433] Train:[85.20] val:[85.00] Test:[87.19] | Update Test:[co:81.12,c:24.50,o:87.19] at Epoch:[3] | lr:0.000998
BIAS:[0.50] | Model:[CausalGIN] Epoch:[4/100] Loss:[0.5324=0.0013+0.3428+0.3780] Train:[87.07] val:[87.00] Test:[85.56] | Update Test:[co:84.31,c:24.75,o:85.56] at Epoch:[4] | lr:0.000996
BIAS:[0.50] | Model:[CausalGIN] Epoch:[5/100] Loss:[0.4835=0.0007+0.3131+0.3401] Train:[88.77] val:[86.88] Test:[87.38] | Update Test:[co:84.31,c:24.75,o:85.56] at Epoch:[4] | lr:0.000994
BIAS:[0.50] | Model:[CausalGIN] Epoch:[6/100] Loss:[0.4479=0.0004+0.2924+0.3106] Train:[88.84] val:[86.62] Test:[85.88] | Update Test:[co:84.31,c:24.75,o:85.56] at Epoch:[4] | lr:0.000991
BIAS:[0.50] | Model:[CausalGIN] Epoch:[7/100] Loss:[0.4043=0.0004+0.2635+0.2812] Train:[90.38] val:[60.62] Test:[61.31] | Update Test:[co:84.31,c:24.75,o:85.56] at Epoch:[4] | lr:0.000988
BIAS:[0.50] | Model:[CausalGIN] Epoch:[8/100] Loss:[0.4330=0.0006+0.2843+0.2967] Train:[89.79] val:[86.38] Test:[87.56] | Update Test:[co:84.31,c:24.75,o:85.56] at Epoch:[4] | lr:0.000984
BIAS:[0.50] | Model:[CausalGIN] Epoch:[9/100] Loss:[0.3741=0.0003+0.2418+0.2643] Train:[91.36] val:[91.38] Test:[90.44] | Update Test:[co:90.62,c:25.06,o:90.44] at Epoch:[9] | lr:0.000980
BIAS:[0.50] | Model:[CausalGIN] Epoch:[10/100] Loss:[0.3458=0.0002+0.2266+0.2384] Train:[91.71] val:[89.00] Test:[89.06] | Update Test:[co:90.62,c:25.06,o:90.44] at Epoch:[9] | lr:0.000976
BIAS:[0.50] | Model:[CausalGIN] Epoch:[11/100] Loss:[0.3376=0.0002+0.2202+0.2345] Train:[92.12] val:[90.38] Test:[90.75] | Update Test:[co:90.62,c:25.06,o:90.44] at Epoch:[9] | lr:0.000970
BIAS:[0.50] | Model:[CausalGIN] Epoch:[12/100] Loss:[0.3227=0.0002+0.2088+0.2275] Train:[92.79] val:[65.00] Test:[63.62] | Update Test:[co:90.62,c:25.06,o:90.44] at Epoch:[9] | lr:0.000965
BIAS:[0.50] | Model:[CausalGIN] Epoch:[13/100] Loss:[0.3110=0.0001+0.2031+0.2157] Train:[93.02] val:[94.38] Test:[93.94] | Update Test:[co:93.06,c:24.19,o:93.94] at Epoch:[13] | lr:0.000959
BIAS:[0.50] | Model:[CausalGIN] Epoch:[14/100] Loss:[0.2786=0.0001+0.1805+0.1960] Train:[93.88] val:[91.88] Test:[92.00] | Update Test:[co:93.06,c:24.19,o:93.94] at Epoch:[13] | lr:0.000952
BIAS:[0.50] | Model:[CausalGIN] Epoch:[15/100] Loss:[0.2485=0.0001+0.1615+0.1737] Train:[94.71] val:[91.38] Test:[92.00] | Update Test:[co:93.06,c:24.19,o:93.94] at Epoch:[13] | lr:0.000946
BIAS:[0.50] | Model:[CausalGIN] Epoch:[16/100] Loss:[0.2478=0.0001+0.1619+0.1717] Train:[93.96] val:[90.62] Test:[91.75] | Update Test:[co:93.06,c:24.19,o:93.94] at Epoch:[13] | lr:0.000938
BIAS:[0.50] | Model:[CausalGIN] Epoch:[17/100] Loss:[0.2498=0.0001+0.1607+0.1781] Train:[94.66] val:[87.88] Test:[87.88] | Update Test:[co:93.06,c:24.19,o:93.94] at Epoch:[13] | lr:0.000930
BIAS:[0.50] | Model:[CausalGIN] Epoch:[18/100] Loss:[0.2587=0.0002+0.1663+0.1845] Train:[94.32] val:[91.75] Test:[92.00] | Update Test:[co:93.06,c:24.19,o:93.94] at Epoch:[13] | lr:0.000922
BIAS:[0.50] | Model:[CausalGIN] Epoch:[19/100] Loss:[0.2701=0.0002+0.1748+0.1904] Train:[94.39] val:[94.00] Test:[93.88] | Update Test:[co:93.06,c:24.19,o:93.94] at Epoch:[13] | lr:0.000914
BIAS:[0.50] | Model:[CausalGIN] Epoch:[20/100] Loss:[0.2328=0.0001+0.1505+0.1644] Train:[95.20] val:[94.25] Test:[94.25] | Update Test:[co:93.06,c:24.19,o:93.94] at Epoch:[13] | lr:0.000905
BIAS:[0.50] | Model:[CausalGIN] Epoch:[21/100] Loss:[0.2062=0.0001+0.1315+0.1493] Train:[95.68] val:[95.62] Test:[94.06] | Update Test:[co:92.06,c:25.00,o:94.06] at Epoch:[21] | lr:0.000895
BIAS:[0.50] | Model:[CausalGIN] Epoch:[22/100] Loss:[0.2220=0.0001+0.1460+0.1519] Train:[95.14] val:[93.12] Test:[93.75] | Update Test:[co:92.06,c:25.00,o:94.06] at Epoch:[21] | lr:0.000885
BIAS:[0.50] | Model:[CausalGIN] Epoch:[23/100] Loss:[0.1885=0.0001+0.1235+0.1297] Train:[96.00] val:[88.88] Test:[89.81] | Update Test:[co:92.06,c:25.00,o:94.06] at Epoch:[21] | lr:0.000875
BIAS:[0.50] | Model:[CausalGIN] Epoch:[24/100] Loss:[0.1927=0.0001+0.1239+0.1375] Train:[95.95] val:[94.00] Test:[93.56] | Update Test:[co:92.06,c:25.00,o:94.06] at Epoch:[21] | lr:0.000865
BIAS:[0.50] | Model:[CausalGIN] Epoch:[25/100] Loss:[0.1847=0.0001+0.1192+0.1308] Train:[96.02] val:[93.88] Test:[93.06] | Update Test:[co:92.06,c:25.00,o:94.06] at Epoch:[21] | lr:0.000854
BIAS:[0.50] | Model:[CausalGIN] Epoch:[26/100] Loss:[0.1716=0.0001+0.1121+0.1190] Train:[96.41] val:[95.25] Test:[95.25] | Update Test:[co:92.06,c:25.00,o:94.06] at Epoch:[21] | lr:0.000842
BIAS:[0.50] | Model:[CausalGIN] Epoch:[27/100] Loss:[0.1579=0.0001+0.1024+0.1109] Train:[96.75] val:[95.00] Test:[95.06] | Update Test:[co:92.06,c:25.00,o:94.06] at Epoch:[21] | lr:0.000831
BIAS:[0.50] | Model:[CausalGIN] Epoch:[28/100] Loss:[0.1719=0.0001+0.1117+0.1203] Train:[96.20] val:[93.38] Test:[93.56] | Update Test:[co:92.06,c:25.00,o:94.06] at Epoch:[21] | lr:0.000819
BIAS:[0.50] | Model:[CausalGIN] Epoch:[29/100] Loss:[0.1578=0.0001+0.1041+0.1071] Train:[96.70] val:[95.75] Test:[95.44] | Update Test:[co:95.31,c:25.87,o:95.44] at Epoch:[29] | lr:0.000807
BIAS:[0.50] | Model:[CausalGIN] Epoch:[30/100] Loss:[0.1392=0.0002+0.0902+0.0978] Train:[97.23] val:[95.75] Test:[95.06] | Update Test:[co:95.31,c:25.87,o:95.44] at Epoch:[29] | lr:0.000794
BIAS:[0.50] | Model:[CausalGIN] Epoch:[31/100] Loss:[0.1377=0.0001+0.0903+0.0947] Train:[97.04] val:[91.38] Test:[90.62] | Update Test:[co:95.31,c:25.87,o:95.44] at Epoch:[29] | lr:0.000781
BIAS:[0.50] | Model:[CausalGIN] Epoch:[32/100] Loss:[0.1322=0.0001+0.0853+0.0938] Train:[97.43] val:[95.75] Test:[95.44] | Update Test:[co:95.31,c:25.87,o:95.44] at Epoch:[29] | lr:0.000768
BIAS:[0.50] | Model:[CausalGIN] Epoch:[33/100] Loss:[0.1135=0.0001+0.0724+0.0821] Train:[97.62] val:[96.75] Test:[95.75] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000755
BIAS:[0.50] | Model:[CausalGIN] Epoch:[34/100] Loss:[0.1486=0.0000+0.0947+0.1078] Train:[97.18] val:[94.25] Test:[93.62] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000741
BIAS:[0.50] | Model:[CausalGIN] Epoch:[35/100] Loss:[0.1151=0.0000+0.0739+0.0822] Train:[97.64] val:[95.50] Test:[94.44] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000727
BIAS:[0.50] | Model:[CausalGIN] Epoch:[36/100] Loss:[0.1126=0.0000+0.0732+0.0787] Train:[97.80] val:[91.25] Test:[91.81] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000713
BIAS:[0.50] | Model:[CausalGIN] Epoch:[37/100] Loss:[0.1106=0.0001+0.0695+0.0821] Train:[97.62] val:[95.00] Test:[94.25] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000699
BIAS:[0.50] | Model:[CausalGIN] Epoch:[38/100] Loss:[0.0994=0.0001+0.0634+0.0718] Train:[98.02] val:[94.88] Test:[94.50] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000684
BIAS:[0.50] | Model:[CausalGIN] Epoch:[39/100] Loss:[0.0959=0.0001+0.0622+0.0672] Train:[97.98] val:[96.75] Test:[96.38] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000670
BIAS:[0.50] | Model:[CausalGIN] Epoch:[40/100] Loss:[0.1160=0.0001+0.0740+0.0838] Train:[97.55] val:[91.88] Test:[91.56] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000655
BIAS:[0.50] | Model:[CausalGIN] Epoch:[41/100] Loss:[0.1035=0.0001+0.0662+0.0745] Train:[97.82] val:[95.62] Test:[95.25] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000640
BIAS:[0.50] | Model:[CausalGIN] Epoch:[42/100] Loss:[0.0856=0.0001+0.0544+0.0622] Train:[98.29] val:[95.38] Test:[94.88] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000625
BIAS:[0.50] | Model:[CausalGIN] Epoch:[43/100] Loss:[0.1172=0.0001+0.0749+0.0845] Train:[97.52] val:[95.50] Test:[96.00] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000609
BIAS:[0.50] | Model:[CausalGIN] Epoch:[44/100] Loss:[0.0722=0.0000+0.0455+0.0535] Train:[98.79] val:[96.62] Test:[96.25] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000594
BIAS:[0.50] | Model:[CausalGIN] Epoch:[45/100] Loss:[0.0688=0.0000+0.0424+0.0528] Train:[98.61] val:[94.50] Test:[94.31] | Update Test:[co:95.56,c:24.81,o:95.75] at Epoch:[33] | lr:0.000579
BIAS:[0.50] | Model:[CausalGIN] Epoch:[46/100] Loss:[0.0599=0.0001+0.0372+0.0454] Train:[98.89] val:[96.88] Test:[96.12] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000563
BIAS:[0.50] | Model:[CausalGIN] Epoch:[47/100] Loss:[0.0576=0.0001+0.0353+0.0446] Train:[98.84] val:[96.25] Test:[95.12] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000548
BIAS:[0.50] | Model:[CausalGIN] Epoch:[48/100] Loss:[0.0607=0.0000+0.0386+0.0443] Train:[98.80] val:[96.75] Test:[95.81] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000532
BIAS:[0.50] | Model:[CausalGIN] Epoch:[49/100] Loss:[0.0454=0.0000+0.0287+0.0332] Train:[99.12] val:[96.75] Test:[96.38] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000516
BIAS:[0.50] | Model:[CausalGIN] Epoch:[50/100] Loss:[0.0528=0.0000+0.0321+0.0413] Train:[99.07] val:[96.75] Test:[95.62] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000501
BIAS:[0.50] | Model:[CausalGIN] Epoch:[51/100] Loss:[0.0533=0.0000+0.0332+0.0401] Train:[98.96] val:[96.38] Test:[96.19] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000485
BIAS:[0.50] | Model:[CausalGIN] Epoch:[52/100] Loss:[0.0463=0.0000+0.0289+0.0349] Train:[99.21] val:[96.12] Test:[95.62] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000469
BIAS:[0.50] | Model:[CausalGIN] Epoch:[53/100] Loss:[0.0361=0.0000+0.0220+0.0283] Train:[99.41] val:[95.88] Test:[96.12] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000453
BIAS:[0.50] | Model:[CausalGIN] Epoch:[54/100] Loss:[0.0410=0.0000+0.0248+0.0322] Train:[99.32] val:[96.88] Test:[96.31] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000438
BIAS:[0.50] | Model:[CausalGIN] Epoch:[55/100] Loss:[0.0493=0.0000+0.0308+0.0370] Train:[98.96] val:[96.75] Test:[95.81] | Update Test:[co:96.19,c:25.56,o:96.12] at Epoch:[46] | lr:0.000422
BIAS:[0.50] | Model:[CausalGIN] Epoch:[56/100] Loss:[0.0368=0.0000+0.0232+0.0272] Train:[99.38] val:[97.62] Test:[96.88] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000407
BIAS:[0.50] | Model:[CausalGIN] Epoch:[57/100] Loss:[0.0266=0.0000+0.0155+0.0222] Train:[99.61] val:[96.75] Test:[96.00] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000392
BIAS:[0.50] | Model:[CausalGIN] Epoch:[58/100] Loss:[0.0276=0.0000+0.0169+0.0213] Train:[99.57] val:[96.75] Test:[96.25] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000376
BIAS:[0.50] | Model:[CausalGIN] Epoch:[59/100] Loss:[0.0247=0.0000+0.0146+0.0200] Train:[99.62] val:[97.38] Test:[96.56] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000361
BIAS:[0.50] | Model:[CausalGIN] Epoch:[60/100] Loss:[0.0176=0.0000+0.0106+0.0139] Train:[99.80] val:[96.75] Test:[96.62] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000346
BIAS:[0.50] | Model:[CausalGIN] Epoch:[61/100] Loss:[0.0258=0.0000+0.0155+0.0206] Train:[99.55] val:[96.38] Test:[96.38] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000331
BIAS:[0.50] | Model:[CausalGIN] Epoch:[62/100] Loss:[0.0192=0.0000+0.0112+0.0160] Train:[99.84] val:[96.25] Test:[96.12] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000317
BIAS:[0.50] | Model:[CausalGIN] Epoch:[63/100] Loss:[0.0218=0.0000+0.0131+0.0174] Train:[99.73] val:[93.12] Test:[93.12] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000302
BIAS:[0.50] | Model:[CausalGIN] Epoch:[64/100] Loss:[0.0257=0.0000+0.0150+0.0214] Train:[99.64] val:[95.75] Test:[96.12] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000288
BIAS:[0.50] | Model:[CausalGIN] Epoch:[65/100] Loss:[0.0203=0.0000+0.0121+0.0164] Train:[99.70] val:[96.38] Test:[95.88] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000274
BIAS:[0.50] | Model:[CausalGIN] Epoch:[66/100] Loss:[0.0247=0.0000+0.0149+0.0197] Train:[99.59] val:[94.88] Test:[95.44] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000260
BIAS:[0.50] | Model:[CausalGIN] Epoch:[67/100] Loss:[0.0223=0.0000+0.0138+0.0169] Train:[99.59] val:[96.88] Test:[97.06] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000246
BIAS:[0.50] | Model:[CausalGIN] Epoch:[68/100] Loss:[0.0163=0.0000+0.0099+0.0128] Train:[99.84] val:[97.00] Test:[96.50] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000233
BIAS:[0.50] | Model:[CausalGIN] Epoch:[69/100] Loss:[0.0103=0.0000+0.0060+0.0087] Train:[99.96] val:[97.25] Test:[96.56] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000220
BIAS:[0.50] | Model:[CausalGIN] Epoch:[70/100] Loss:[0.0126=0.0000+0.0071+0.0111] Train:[99.91] val:[96.88] Test:[96.56] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000207
BIAS:[0.50] | Model:[CausalGIN] Epoch:[71/100] Loss:[0.0102=0.0000+0.0061+0.0082] Train:[99.93] val:[97.25] Test:[96.75] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000194
BIAS:[0.50] | Model:[CausalGIN] Epoch:[72/100] Loss:[0.0081=0.0000+0.0047+0.0069] Train:[99.96] val:[97.00] Test:[96.81] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000182
BIAS:[0.50] | Model:[CausalGIN] Epoch:[73/100] Loss:[0.0087=0.0000+0.0051+0.0070] Train:[99.95] val:[96.88] Test:[96.62] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000170
BIAS:[0.50] | Model:[CausalGIN] Epoch:[74/100] Loss:[0.0067=0.0000+0.0040+0.0054] Train:[99.96] val:[97.00] Test:[96.62] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000159
BIAS:[0.50] | Model:[CausalGIN] Epoch:[75/100] Loss:[0.0069=0.0000+0.0041+0.0056] Train:[99.96] val:[97.12] Test:[96.56] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000147
BIAS:[0.50] | Model:[CausalGIN] Epoch:[76/100] Loss:[0.0069=0.0000+0.0041+0.0056] Train:[99.96] val:[97.12] Test:[96.88] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000136
BIAS:[0.50] | Model:[CausalGIN] Epoch:[77/100] Loss:[0.0068=0.0000+0.0040+0.0056] Train:[99.96] val:[97.12] Test:[96.56] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000126
BIAS:[0.50] | Model:[CausalGIN] Epoch:[78/100] Loss:[0.0061=0.0000+0.0037+0.0047] Train:[99.96] val:[96.88] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000116
BIAS:[0.50] | Model:[CausalGIN] Epoch:[79/100] Loss:[0.0068=0.0000+0.0040+0.0057] Train:[99.96] val:[97.25] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000106
BIAS:[0.50] | Model:[CausalGIN] Epoch:[80/100] Loss:[0.0067=0.0000+0.0039+0.0055] Train:[99.96] val:[97.25] Test:[96.81] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000096
BIAS:[0.50] | Model:[CausalGIN] Epoch:[81/100] Loss:[0.0055=0.0000+0.0032+0.0046] Train:[99.95] val:[97.12] Test:[96.56] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000087
BIAS:[0.50] | Model:[CausalGIN] Epoch:[82/100] Loss:[0.0063=0.0000+0.0038+0.0050] Train:[99.95] val:[97.12] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000079
BIAS:[0.50] | Model:[CausalGIN] Epoch:[83/100] Loss:[0.0075=0.0000+0.0048+0.0054] Train:[99.95] val:[97.38] Test:[96.88] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000071
BIAS:[0.50] | Model:[CausalGIN] Epoch:[84/100] Loss:[0.0063=0.0000+0.0039+0.0049] Train:[99.93] val:[97.12] Test:[96.81] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000063
BIAS:[0.50] | Model:[CausalGIN] Epoch:[85/100] Loss:[0.0055=0.0000+0.0033+0.0045] Train:[99.96] val:[97.50] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000055
BIAS:[0.50] | Model:[CausalGIN] Epoch:[86/100] Loss:[0.0053=0.0000+0.0032+0.0043] Train:[99.96] val:[97.12] Test:[96.81] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000049
BIAS:[0.50] | Model:[CausalGIN] Epoch:[87/100] Loss:[0.0047=0.0000+0.0027+0.0041] Train:[99.95] val:[97.12] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000042
BIAS:[0.50] | Model:[CausalGIN] Epoch:[88/100] Loss:[0.0051=0.0000+0.0031+0.0041] Train:[99.96] val:[97.25] Test:[96.75] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000036
BIAS:[0.50] | Model:[CausalGIN] Epoch:[89/100] Loss:[0.0049=0.0000+0.0029+0.0040] Train:[99.96] val:[97.12] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000031
BIAS:[0.50] | Model:[CausalGIN] Epoch:[90/100] Loss:[0.0052=0.0000+0.0031+0.0041] Train:[99.96] val:[97.00] Test:[96.88] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000025
BIAS:[0.50] | Model:[CausalGIN] Epoch:[91/100] Loss:[0.0052=0.0000+0.0031+0.0042] Train:[99.96] val:[97.12] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000021
BIAS:[0.50] | Model:[CausalGIN] Epoch:[92/100] Loss:[0.0038=0.0000+0.0022+0.0031] Train:[99.96] val:[97.25] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000017
BIAS:[0.50] | Model:[CausalGIN] Epoch:[93/100] Loss:[0.0056=0.0000+0.0034+0.0043] Train:[99.96] val:[97.12] Test:[96.75] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000013
BIAS:[0.50] | Model:[CausalGIN] Epoch:[94/100] Loss:[0.0038=0.0000+0.0022+0.0032] Train:[99.96] val:[97.25] Test:[96.81] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000010
BIAS:[0.50] | Model:[CausalGIN] Epoch:[95/100] Loss:[0.0042=0.0000+0.0026+0.0033] Train:[99.96] val:[97.12] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000007
BIAS:[0.50] | Model:[CausalGIN] Epoch:[96/100] Loss:[0.0040=0.0000+0.0024+0.0033] Train:[99.96] val:[97.12] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000005
BIAS:[0.50] | Model:[CausalGIN] Epoch:[97/100] Loss:[0.0051=0.0000+0.0031+0.0041] Train:[99.96] val:[97.12] Test:[96.69] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000003
BIAS:[0.50] | Model:[CausalGIN] Epoch:[98/100] Loss:[0.0044=0.0000+0.0026+0.0037] Train:[99.96] val:[97.38] Test:[96.62] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000002
BIAS:[0.50] | Model:[CausalGIN] Epoch:[99/100] Loss:[0.0046=0.0000+0.0028+0.0037] Train:[99.96] val:[97.25] Test:[96.88] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000001
BIAS:[0.50] | Model:[CausalGIN] Epoch:[100/100] Loss:[0.0050=0.0000+0.0029+0.0040] Train:[99.95] val:[97.25] Test:[96.56] | Update Test:[co:96.94,c:25.19,o:96.88] at Epoch:[56] | lr:0.000001
syd: BIAS:[0.50] | Val acc:[97.25] Test acc:[co:96.94,c:25.19,o:96.88] at epoch:[56]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.7
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:979  , Cycle:420  , Grids:420  , Diams:420   
| BA  : House:420  , Cycle:979  , Grids:979  , Diams:979   
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:70.0%, Cycle:30.0%, Grids:30.0%, Diams:30.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:140  , Cycle:60   , Grids:60   , Diams:60    
| BA  : House:60   , Cycle:140  , Grids:140  , Diams:140   
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:70.0%, Cycle:30.0%, Grids:30.0%, Diams:30.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.70] | Model:[GIN] Epoch:[1/100] Loss:[1.0436] Train:[55.93] val:[65.00] Test:[55.50] | Best Val:[65.00] Update Test:[55.50] at Epoch:[1] | lr:0.001000
BIAS:[0.70] | Model:[GIN] Epoch:[2/100] Loss:[0.5669] Train:[79.07] val:[78.25] Test:[76.12] | Best Val:[78.25] Update Test:[76.12] at Epoch:[2] | lr:0.000999
BIAS:[0.70] | Model:[GIN] Epoch:[3/100] Loss:[0.4353] Train:[83.56] val:[85.00] Test:[84.38] | Best Val:[85.00] Update Test:[84.38] at Epoch:[3] | lr:0.000998
BIAS:[0.70] | Model:[GIN] Epoch:[4/100] Loss:[0.3858] Train:[85.65] val:[81.25] Test:[80.94] | Best Val:[85.00] Update Test:[84.38] at Epoch:[3] | lr:0.000996
BIAS:[0.70] | Model:[GIN] Epoch:[5/100] Loss:[0.3595] Train:[86.85] val:[84.12] Test:[81.88] | Best Val:[85.00] Update Test:[84.38] at Epoch:[3] | lr:0.000994
BIAS:[0.70] | Model:[GIN] Epoch:[6/100] Loss:[0.3046] Train:[89.56] val:[88.25] Test:[85.62] | Best Val:[88.25] Update Test:[85.62] at Epoch:[6] | lr:0.000991
BIAS:[0.70] | Model:[GIN] Epoch:[7/100] Loss:[0.2810] Train:[89.90] val:[87.12] Test:[82.69] | Best Val:[88.25] Update Test:[85.62] at Epoch:[6] | lr:0.000988
BIAS:[0.70] | Model:[GIN] Epoch:[8/100] Loss:[0.2700] Train:[90.24] val:[69.88] Test:[68.25] | Best Val:[88.25] Update Test:[85.62] at Epoch:[6] | lr:0.000984
BIAS:[0.70] | Model:[GIN] Epoch:[9/100] Loss:[0.2608] Train:[91.15] val:[90.25] Test:[88.06] | Best Val:[90.25] Update Test:[88.06] at Epoch:[9] | lr:0.000980
BIAS:[0.70] | Model:[GIN] Epoch:[10/100] Loss:[0.2509] Train:[90.96] val:[86.75] Test:[84.50] | Best Val:[90.25] Update Test:[88.06] at Epoch:[9] | lr:0.000976
BIAS:[0.70] | Model:[GIN] Epoch:[11/100] Loss:[0.2332] Train:[91.78] val:[67.50] Test:[66.12] | Best Val:[90.25] Update Test:[88.06] at Epoch:[9] | lr:0.000970
BIAS:[0.70] | Model:[GIN] Epoch:[12/100] Loss:[0.2402] Train:[90.94] val:[91.38] Test:[88.44] | Best Val:[91.38] Update Test:[88.44] at Epoch:[12] | lr:0.000965
BIAS:[0.70] | Model:[GIN] Epoch:[13/100] Loss:[0.2118] Train:[92.74] val:[90.38] Test:[86.19] | Best Val:[91.38] Update Test:[88.44] at Epoch:[12] | lr:0.000959
BIAS:[0.70] | Model:[GIN] Epoch:[14/100] Loss:[0.2307] Train:[91.69] val:[87.38] Test:[86.00] | Best Val:[91.38] Update Test:[88.44] at Epoch:[12] | lr:0.000952
BIAS:[0.70] | Model:[GIN] Epoch:[15/100] Loss:[0.2445] Train:[91.40] val:[76.00] Test:[72.44] | Best Val:[91.38] Update Test:[88.44] at Epoch:[12] | lr:0.000946
BIAS:[0.70] | Model:[GIN] Epoch:[16/100] Loss:[0.2243] Train:[92.12] val:[90.38] Test:[91.19] | Best Val:[91.38] Update Test:[88.44] at Epoch:[12] | lr:0.000938
BIAS:[0.70] | Model:[GIN] Epoch:[17/100] Loss:[0.1985] Train:[93.33] val:[88.25] Test:[86.62] | Best Val:[91.38] Update Test:[88.44] at Epoch:[12] | lr:0.000930
BIAS:[0.70] | Model:[GIN] Epoch:[18/100] Loss:[0.1989] Train:[93.19] val:[91.75] Test:[91.12] | Best Val:[91.75] Update Test:[91.12] at Epoch:[18] | lr:0.000922
BIAS:[0.70] | Model:[GIN] Epoch:[19/100] Loss:[0.1872] Train:[93.62] val:[89.12] Test:[86.50] | Best Val:[91.75] Update Test:[91.12] at Epoch:[18] | lr:0.000914
BIAS:[0.70] | Model:[GIN] Epoch:[20/100] Loss:[0.1775] Train:[94.14] val:[87.62] Test:[86.06] | Best Val:[91.75] Update Test:[91.12] at Epoch:[18] | lr:0.000905
BIAS:[0.70] | Model:[GIN] Epoch:[21/100] Loss:[0.1777] Train:[93.42] val:[91.88] Test:[92.50] | Best Val:[91.88] Update Test:[92.50] at Epoch:[21] | lr:0.000895
BIAS:[0.70] | Model:[GIN] Epoch:[22/100] Loss:[0.1702] Train:[93.96] val:[92.75] Test:[92.19] | Best Val:[92.75] Update Test:[92.19] at Epoch:[22] | lr:0.000885
BIAS:[0.70] | Model:[GIN] Epoch:[23/100] Loss:[0.1549] Train:[94.69] val:[74.75] Test:[77.25] | Best Val:[92.75] Update Test:[92.19] at Epoch:[22] | lr:0.000875
BIAS:[0.70] | Model:[GIN] Epoch:[24/100] Loss:[0.1495] Train:[95.00] val:[88.25] Test:[85.44] | Best Val:[92.75] Update Test:[92.19] at Epoch:[22] | lr:0.000865
BIAS:[0.70] | Model:[GIN] Epoch:[25/100] Loss:[0.1540] Train:[94.78] val:[91.50] Test:[91.75] | Best Val:[92.75] Update Test:[92.19] at Epoch:[22] | lr:0.000854
BIAS:[0.70] | Model:[GIN] Epoch:[26/100] Loss:[0.1393] Train:[95.69] val:[78.38] Test:[79.00] | Best Val:[92.75] Update Test:[92.19] at Epoch:[22] | lr:0.000842
BIAS:[0.70] | Model:[GIN] Epoch:[27/100] Loss:[0.1443] Train:[94.96] val:[83.88] Test:[81.06] | Best Val:[92.75] Update Test:[92.19] at Epoch:[22] | lr:0.000831
BIAS:[0.70] | Model:[GIN] Epoch:[28/100] Loss:[0.1362] Train:[95.57] val:[91.50] Test:[88.56] | Best Val:[92.75] Update Test:[92.19] at Epoch:[22] | lr:0.000819
BIAS:[0.70] | Model:[GIN] Epoch:[29/100] Loss:[0.1604] Train:[94.48] val:[83.62] Test:[84.69] | Best Val:[92.75] Update Test:[92.19] at Epoch:[22] | lr:0.000807
BIAS:[0.70] | Model:[GIN] Epoch:[30/100] Loss:[0.1483] Train:[95.18] val:[93.12] Test:[93.81] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000794
BIAS:[0.70] | Model:[GIN] Epoch:[31/100] Loss:[0.1429] Train:[95.07] val:[89.62] Test:[86.25] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000781
BIAS:[0.70] | Model:[GIN] Epoch:[32/100] Loss:[0.1213] Train:[96.03] val:[93.00] Test:[92.75] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000768
BIAS:[0.70] | Model:[GIN] Epoch:[33/100] Loss:[0.1269] Train:[95.53] val:[93.00] Test:[89.75] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000755
BIAS:[0.70] | Model:[GIN] Epoch:[34/100] Loss:[0.1119] Train:[95.98] val:[87.12] Test:[90.00] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000741
BIAS:[0.70] | Model:[GIN] Epoch:[35/100] Loss:[0.1073] Train:[96.39] val:[83.12] Test:[86.06] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000727
BIAS:[0.70] | Model:[GIN] Epoch:[36/100] Loss:[0.0967] Train:[96.73] val:[90.38] Test:[89.44] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000713
BIAS:[0.70] | Model:[GIN] Epoch:[37/100] Loss:[0.1065] Train:[96.64] val:[91.88] Test:[94.44] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000699
BIAS:[0.70] | Model:[GIN] Epoch:[38/100] Loss:[0.1004] Train:[96.57] val:[91.12] Test:[87.06] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000684
BIAS:[0.70] | Model:[GIN] Epoch:[39/100] Loss:[0.1208] Train:[95.87] val:[85.12] Test:[86.25] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000670
BIAS:[0.70] | Model:[GIN] Epoch:[40/100] Loss:[0.1088] Train:[96.30] val:[91.75] Test:[91.69] | Best Val:[93.12] Update Test:[93.81] at Epoch:[30] | lr:0.000655
BIAS:[0.70] | Model:[GIN] Epoch:[41/100] Loss:[0.0979] Train:[96.93] val:[94.62] Test:[94.19] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000640
BIAS:[0.70] | Model:[GIN] Epoch:[42/100] Loss:[0.0872] Train:[97.03] val:[94.62] Test:[94.88] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000625
BIAS:[0.70] | Model:[GIN] Epoch:[43/100] Loss:[0.0686] Train:[97.87] val:[91.50] Test:[92.75] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000609
BIAS:[0.70] | Model:[GIN] Epoch:[44/100] Loss:[0.0799] Train:[97.19] val:[90.50] Test:[90.69] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000594
BIAS:[0.70] | Model:[GIN] Epoch:[45/100] Loss:[0.0655] Train:[97.96] val:[93.62] Test:[91.81] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000579
BIAS:[0.70] | Model:[GIN] Epoch:[46/100] Loss:[0.0607] Train:[98.00] val:[89.50] Test:[90.69] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000563
BIAS:[0.70] | Model:[GIN] Epoch:[47/100] Loss:[0.0715] Train:[97.62] val:[88.62] Test:[86.62] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000548
BIAS:[0.70] | Model:[GIN] Epoch:[48/100] Loss:[0.0733] Train:[97.57] val:[90.75] Test:[92.38] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000532
BIAS:[0.70] | Model:[GIN] Epoch:[49/100] Loss:[0.0626] Train:[97.98] val:[92.50] Test:[92.44] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000516
BIAS:[0.70] | Model:[GIN] Epoch:[50/100] Loss:[0.0703] Train:[97.57] val:[91.38] Test:[91.94] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000501
BIAS:[0.70] | Model:[GIN] Epoch:[51/100] Loss:[0.0679] Train:[97.62] val:[92.00] Test:[89.88] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000485
BIAS:[0.70] | Model:[GIN] Epoch:[52/100] Loss:[0.0522] Train:[98.30] val:[92.75] Test:[93.69] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000469
BIAS:[0.70] | Model:[GIN] Epoch:[53/100] Loss:[0.0559] Train:[98.27] val:[94.25] Test:[94.56] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000453
BIAS:[0.70] | Model:[GIN] Epoch:[54/100] Loss:[0.0477] Train:[98.68] val:[92.50] Test:[94.69] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000438
BIAS:[0.70] | Model:[GIN] Epoch:[55/100] Loss:[0.0705] Train:[97.59] val:[84.75] Test:[88.12] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000422
BIAS:[0.70] | Model:[GIN] Epoch:[56/100] Loss:[0.0775] Train:[97.48] val:[92.25] Test:[91.50] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000407
BIAS:[0.70] | Model:[GIN] Epoch:[57/100] Loss:[0.0470] Train:[98.43] val:[93.75] Test:[94.81] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000392
BIAS:[0.70] | Model:[GIN] Epoch:[58/100] Loss:[0.0498] Train:[98.43] val:[93.00] Test:[94.31] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000376
BIAS:[0.70] | Model:[GIN] Epoch:[59/100] Loss:[0.0354] Train:[98.84] val:[92.12] Test:[94.25] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000361
BIAS:[0.70] | Model:[GIN] Epoch:[60/100] Loss:[0.0356] Train:[98.78] val:[94.38] Test:[94.38] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000346
BIAS:[0.70] | Model:[GIN] Epoch:[61/100] Loss:[0.0251] Train:[99.34] val:[94.12] Test:[95.12] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000331
BIAS:[0.70] | Model:[GIN] Epoch:[62/100] Loss:[0.0188] Train:[99.59] val:[94.62] Test:[95.12] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000317
BIAS:[0.70] | Model:[GIN] Epoch:[63/100] Loss:[0.0190] Train:[99.52] val:[94.38] Test:[94.88] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000302
BIAS:[0.70] | Model:[GIN] Epoch:[64/100] Loss:[0.0226] Train:[99.36] val:[93.75] Test:[93.88] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000288
BIAS:[0.70] | Model:[GIN] Epoch:[65/100] Loss:[0.0180] Train:[99.59] val:[92.88] Test:[94.31] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000274
BIAS:[0.70] | Model:[GIN] Epoch:[66/100] Loss:[0.0132] Train:[99.75] val:[94.38] Test:[94.81] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000260
BIAS:[0.70] | Model:[GIN] Epoch:[67/100] Loss:[0.0137] Train:[99.73] val:[94.00] Test:[95.00] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000246
BIAS:[0.70] | Model:[GIN] Epoch:[68/100] Loss:[0.0155] Train:[99.55] val:[92.00] Test:[93.62] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000233
BIAS:[0.70] | Model:[GIN] Epoch:[69/100] Loss:[0.0115] Train:[99.75] val:[94.00] Test:[94.25] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000220
BIAS:[0.70] | Model:[GIN] Epoch:[70/100] Loss:[0.0109] Train:[99.80] val:[93.62] Test:[94.94] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000207
BIAS:[0.70] | Model:[GIN] Epoch:[71/100] Loss:[0.0110] Train:[99.77] val:[94.12] Test:[95.25] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000194
BIAS:[0.70] | Model:[GIN] Epoch:[72/100] Loss:[0.0092] Train:[99.84] val:[94.12] Test:[94.75] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000182
BIAS:[0.70] | Model:[GIN] Epoch:[73/100] Loss:[0.0081] Train:[99.84] val:[93.88] Test:[95.25] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000170
BIAS:[0.70] | Model:[GIN] Epoch:[74/100] Loss:[0.0083] Train:[99.86] val:[93.75] Test:[94.56] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000159
BIAS:[0.70] | Model:[GIN] Epoch:[75/100] Loss:[0.0073] Train:[99.91] val:[93.88] Test:[95.50] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000147
BIAS:[0.70] | Model:[GIN] Epoch:[76/100] Loss:[0.0074] Train:[99.91] val:[93.62] Test:[94.44] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000136
BIAS:[0.70] | Model:[GIN] Epoch:[77/100] Loss:[0.0083] Train:[99.89] val:[94.50] Test:[94.25] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000126
BIAS:[0.70] | Model:[GIN] Epoch:[78/100] Loss:[0.0065] Train:[99.91] val:[94.25] Test:[95.19] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000116
BIAS:[0.70] | Model:[GIN] Epoch:[79/100] Loss:[0.0053] Train:[99.98] val:[93.88] Test:[95.12] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000106
BIAS:[0.70] | Model:[GIN] Epoch:[80/100] Loss:[0.0048] Train:[99.98] val:[94.00] Test:[94.94] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000096
BIAS:[0.70] | Model:[GIN] Epoch:[81/100] Loss:[0.0052] Train:[99.95] val:[93.75] Test:[95.19] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000087
BIAS:[0.70] | Model:[GIN] Epoch:[82/100] Loss:[0.0052] Train:[99.87] val:[94.00] Test:[94.94] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000079
BIAS:[0.70] | Model:[GIN] Epoch:[83/100] Loss:[0.0061] Train:[99.89] val:[94.62] Test:[95.06] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000071
BIAS:[0.70] | Model:[GIN] Epoch:[84/100] Loss:[0.0058] Train:[99.93] val:[94.12] Test:[94.81] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000063
BIAS:[0.70] | Model:[GIN] Epoch:[85/100] Loss:[0.0043] Train:[99.96] val:[94.12] Test:[94.62] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000055
BIAS:[0.70] | Model:[GIN] Epoch:[86/100] Loss:[0.0041] Train:[99.95] val:[94.62] Test:[95.31] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000049
BIAS:[0.70] | Model:[GIN] Epoch:[87/100] Loss:[0.0045] Train:[99.96] val:[94.25] Test:[95.00] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000042
BIAS:[0.70] | Model:[GIN] Epoch:[88/100] Loss:[0.0035] Train:[99.98] val:[94.38] Test:[94.94] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000036
BIAS:[0.70] | Model:[GIN] Epoch:[89/100] Loss:[0.0042] Train:[99.96] val:[94.12] Test:[94.94] | Best Val:[94.62] Update Test:[94.19] at Epoch:[41] | lr:0.000031
BIAS:[0.70] | Model:[GIN] Epoch:[90/100] Loss:[0.0052] Train:[99.95] val:[94.75] Test:[94.94] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000025
BIAS:[0.70] | Model:[GIN] Epoch:[91/100] Loss:[0.0037] Train:[99.98] val:[94.50] Test:[95.06] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000021
BIAS:[0.70] | Model:[GIN] Epoch:[92/100] Loss:[0.0036] Train:[99.98] val:[94.25] Test:[95.00] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000017
BIAS:[0.70] | Model:[GIN] Epoch:[93/100] Loss:[0.0045] Train:[99.93] val:[94.12] Test:[95.06] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000013
BIAS:[0.70] | Model:[GIN] Epoch:[94/100] Loss:[0.0046] Train:[99.98] val:[94.12] Test:[95.00] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000010
BIAS:[0.70] | Model:[GIN] Epoch:[95/100] Loss:[0.0051] Train:[99.96] val:[94.38] Test:[95.00] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000007
BIAS:[0.70] | Model:[GIN] Epoch:[96/100] Loss:[0.0047] Train:[99.93] val:[94.38] Test:[95.06] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000005
BIAS:[0.70] | Model:[GIN] Epoch:[97/100] Loss:[0.0035] Train:[99.98] val:[94.12] Test:[95.19] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000003
BIAS:[0.70] | Model:[GIN] Epoch:[98/100] Loss:[0.0042] Train:[99.98] val:[94.12] Test:[94.88] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000002
BIAS:[0.70] | Model:[GIN] Epoch:[99/100] Loss:[0.0050] Train:[99.95] val:[94.50] Test:[94.94] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000001
BIAS:[0.70] | Model:[GIN] Epoch:[100/100] Loss:[0.0041] Train:[99.96] val:[94.12] Test:[94.94] | Best Val:[94.75] Update Test:[94.94] at Epoch:[90] | lr:0.000001
syd: BIAS:[0.70] | Best Val acc:[94.75] Test acc:[94.94] at epoch:[90]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.7
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:979  , Cycle:420  , Grids:420  , Diams:420   
| BA  : House:420  , Cycle:979  , Grids:979  , Diams:979   
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:70.0%, Cycle:30.0%, Grids:30.0%, Diams:30.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:800
| Tree: House:140  , Cycle:60   , Grids:60   , Diams:60    
| BA  : House:60   , Cycle:140  , Grids:140  , Diams:140   
| All : House:200  , Cycle:200  , Grids:200  , Diams:200   
| BIAS: House:70.0%, Cycle:30.0%, Grids:30.0%, Diams:30.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.70] | Model:[CausalGIN] Epoch:[1/100] Loss:[1.7292=0.0232+1.0732+1.2888] Train:[52.81] val:[35.12] Test:[27.81] | Update Test:[co:25.69,c:26.06,o:27.81] at Epoch:[1] | lr:0.001000
BIAS:[0.70] | Model:[CausalGIN] Epoch:[2/100] Loss:[0.9045=0.0030+0.5388+0.7283] Train:[79.49] val:[81.50] Test:[83.25] | Update Test:[co:76.19,c:25.00,o:83.25] at Epoch:[2] | lr:0.000999
BIAS:[0.70] | Model:[CausalGIN] Epoch:[3/100] Loss:[0.6597=0.0008+0.4249+0.4687] Train:[84.13] val:[79.88] Test:[77.50] | Update Test:[co:76.19,c:25.00,o:83.25] at Epoch:[2] | lr:0.000998
BIAS:[0.70] | Model:[CausalGIN] Epoch:[4/100] Loss:[0.5479=0.0006+0.3520+0.3912] Train:[86.81] val:[86.88] Test:[86.62] | Update Test:[co:87.00,c:20.56,o:86.62] at Epoch:[4] | lr:0.000996
BIAS:[0.70] | Model:[CausalGIN] Epoch:[5/100] Loss:[0.5381=0.0005+0.3460+0.3838] Train:[86.81] val:[80.38] Test:[77.06] | Update Test:[co:87.00,c:20.56,o:86.62] at Epoch:[4] | lr:0.000994
BIAS:[0.70] | Model:[CausalGIN] Epoch:[6/100] Loss:[0.5094=0.0005+0.3273+0.3637] Train:[88.03] val:[86.75] Test:[85.19] | Update Test:[co:87.00,c:20.56,o:86.62] at Epoch:[4] | lr:0.000991
BIAS:[0.70] | Model:[CausalGIN] Epoch:[7/100] Loss:[0.4306=0.0004+0.2765+0.3077] Train:[89.67] val:[90.62] Test:[88.19] | Update Test:[co:87.94,c:21.12,o:88.19] at Epoch:[7] | lr:0.000988
BIAS:[0.70] | Model:[CausalGIN] Epoch:[8/100] Loss:[0.3792=0.0002+0.2427+0.2729] Train:[91.42] val:[86.75] Test:[84.00] | Update Test:[co:87.94,c:21.12,o:88.19] at Epoch:[7] | lr:0.000984
BIAS:[0.70] | Model:[CausalGIN] Epoch:[9/100] Loss:[0.3681=0.0001+0.2367+0.2627] Train:[92.05] val:[91.62] Test:[90.62] | Update Test:[co:90.12,c:27.44,o:90.62] at Epoch:[9] | lr:0.000980
BIAS:[0.70] | Model:[CausalGIN] Epoch:[10/100] Loss:[0.3377=0.0001+0.2183+0.2387] Train:[92.55] val:[83.25] Test:[74.75] | Update Test:[co:90.12,c:27.44,o:90.62] at Epoch:[9] | lr:0.000976
BIAS:[0.70] | Model:[CausalGIN] Epoch:[11/100] Loss:[0.3253=0.0002+0.2089+0.2326] Train:[92.96] val:[88.38] Test:[85.31] | Update Test:[co:90.12,c:27.44,o:90.62] at Epoch:[9] | lr:0.000970
BIAS:[0.70] | Model:[CausalGIN] Epoch:[12/100] Loss:[0.2931=0.0002+0.1888+0.2086] Train:[93.37] val:[74.25] Test:[76.88] | Update Test:[co:90.12,c:27.44,o:90.62] at Epoch:[9] | lr:0.000965
BIAS:[0.70] | Model:[CausalGIN] Epoch:[13/100] Loss:[0.2890=0.0001+0.1856+0.2067] Train:[93.73] val:[93.25] Test:[92.50] | Update Test:[co:91.25,c:25.56,o:92.50] at Epoch:[13] | lr:0.000959
BIAS:[0.70] | Model:[CausalGIN] Epoch:[14/100] Loss:[0.2884=0.0002+0.1858+0.2049] Train:[93.57] val:[90.12] Test:[90.25] | Update Test:[co:91.25,c:25.56,o:92.50] at Epoch:[13] | lr:0.000952
BIAS:[0.70] | Model:[CausalGIN] Epoch:[15/100] Loss:[0.2549=0.0001+0.1631+0.1835] Train:[94.39] val:[91.62] Test:[91.75] | Update Test:[co:91.25,c:25.56,o:92.50] at Epoch:[13] | lr:0.000946
BIAS:[0.70] | Model:[CausalGIN] Epoch:[16/100] Loss:[0.2543=0.0002+0.1635+0.1814] Train:[94.80] val:[94.12] Test:[94.94] | Update Test:[co:94.44,c:25.44,o:94.94] at Epoch:[16] | lr:0.000938
BIAS:[0.70] | Model:[CausalGIN] Epoch:[17/100] Loss:[0.2214=0.0001+0.1421+0.1586] Train:[95.25] val:[86.25] Test:[87.50] | Update Test:[co:94.44,c:25.44,o:94.94] at Epoch:[16] | lr:0.000930
BIAS:[0.70] | Model:[CausalGIN] Epoch:[18/100] Loss:[0.2295=0.0001+0.1448+0.1692] Train:[95.21] val:[92.38] Test:[94.44] | Update Test:[co:94.44,c:25.44,o:94.94] at Epoch:[16] | lr:0.000922
BIAS:[0.70] | Model:[CausalGIN] Epoch:[19/100] Loss:[0.2211=0.0001+0.1430+0.1561] Train:[95.32] val:[94.12] Test:[95.25] | Update Test:[co:94.44,c:25.44,o:94.94] at Epoch:[16] | lr:0.000914
BIAS:[0.70] | Model:[CausalGIN] Epoch:[20/100] Loss:[0.2150=0.0001+0.1372+0.1554] Train:[95.80] val:[94.62] Test:[93.69] | Update Test:[co:92.62,c:23.94,o:93.69] at Epoch:[20] | lr:0.000905
BIAS:[0.70] | Model:[CausalGIN] Epoch:[21/100] Loss:[0.1970=0.0001+0.1245+0.1450] Train:[95.91] val:[92.88] Test:[93.38] | Update Test:[co:92.62,c:23.94,o:93.69] at Epoch:[20] | lr:0.000895
BIAS:[0.70] | Model:[CausalGIN] Epoch:[22/100] Loss:[0.1868=0.0001+0.1182+0.1370] Train:[96.05] val:[95.38] Test:[96.06] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000885
BIAS:[0.70] | Model:[CausalGIN] Epoch:[23/100] Loss:[0.1500=0.0001+0.0935+0.1130] Train:[97.21] val:[94.75] Test:[94.62] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000875
BIAS:[0.70] | Model:[CausalGIN] Epoch:[24/100] Loss:[0.1811=0.0000+0.1146+0.1330] Train:[96.64] val:[91.75] Test:[92.94] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000865
BIAS:[0.70] | Model:[CausalGIN] Epoch:[25/100] Loss:[0.1949=0.0001+0.1236+0.1425] Train:[96.21] val:[69.88] Test:[71.56] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000854
BIAS:[0.70] | Model:[CausalGIN] Epoch:[26/100] Loss:[0.1802=0.0001+0.1149+0.1304] Train:[96.34] val:[94.62] Test:[93.31] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000842
BIAS:[0.70] | Model:[CausalGIN] Epoch:[27/100] Loss:[0.1956=0.0001+0.1277+0.1359] Train:[95.87] val:[89.88] Test:[90.50] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000831
BIAS:[0.70] | Model:[CausalGIN] Epoch:[28/100] Loss:[0.1631=0.0001+0.1016+0.1230] Train:[96.75] val:[93.88] Test:[92.56] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000819
BIAS:[0.70] | Model:[CausalGIN] Epoch:[29/100] Loss:[0.1475=0.0001+0.0906+0.1138] Train:[96.93] val:[68.38] Test:[70.62] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000807
BIAS:[0.70] | Model:[CausalGIN] Epoch:[30/100] Loss:[0.1698=0.0001+0.1066+0.1263] Train:[96.59] val:[94.38] Test:[94.50] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000794
BIAS:[0.70] | Model:[CausalGIN] Epoch:[31/100] Loss:[0.1583=0.0001+0.0982+0.1201] Train:[96.46] val:[94.00] Test:[95.12] | Update Test:[co:94.81,c:26.56,o:96.06] at Epoch:[22] | lr:0.000781
BIAS:[0.70] | Model:[CausalGIN] Epoch:[32/100] Loss:[0.1322=0.0001+0.0840+0.0964] Train:[97.32] val:[96.75] Test:[96.50] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000768
BIAS:[0.70] | Model:[CausalGIN] Epoch:[33/100] Loss:[0.1102=0.0001+0.0683+0.0839] Train:[97.78] val:[94.75] Test:[94.50] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000755
BIAS:[0.70] | Model:[CausalGIN] Epoch:[34/100] Loss:[0.1123=0.0001+0.0689+0.0868] Train:[97.77] val:[95.25] Test:[94.44] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000741
BIAS:[0.70] | Model:[CausalGIN] Epoch:[35/100] Loss:[0.1249=0.0001+0.0776+0.0947] Train:[97.68] val:[92.38] Test:[94.19] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000727
BIAS:[0.70] | Model:[CausalGIN] Epoch:[36/100] Loss:[0.1333=0.0001+0.0820+0.1025] Train:[97.59] val:[93.00] Test:[91.94] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000713
BIAS:[0.70] | Model:[CausalGIN] Epoch:[37/100] Loss:[0.0853=0.0000+0.0530+0.0644] Train:[98.53] val:[95.00] Test:[95.75] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000699
BIAS:[0.70] | Model:[CausalGIN] Epoch:[38/100] Loss:[0.0993=0.0000+0.0610+0.0765] Train:[98.11] val:[91.88] Test:[87.38] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000684
BIAS:[0.70] | Model:[CausalGIN] Epoch:[39/100] Loss:[0.0995=0.0000+0.0610+0.0768] Train:[98.12] val:[90.25] Test:[92.88] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000670
BIAS:[0.70] | Model:[CausalGIN] Epoch:[40/100] Loss:[0.0864=0.0001+0.0524+0.0679] Train:[98.59] val:[94.38] Test:[92.31] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000655
BIAS:[0.70] | Model:[CausalGIN] Epoch:[41/100] Loss:[0.0839=0.0001+0.0518+0.0641] Train:[98.28] val:[95.25] Test:[94.31] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000640
BIAS:[0.70] | Model:[CausalGIN] Epoch:[42/100] Loss:[0.0880=0.0000+0.0545+0.0669] Train:[98.27] val:[94.00] Test:[91.62] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000625
BIAS:[0.70] | Model:[CausalGIN] Epoch:[43/100] Loss:[0.1019=0.0000+0.0636+0.0765] Train:[97.94] val:[96.25] Test:[96.38] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000609
BIAS:[0.70] | Model:[CausalGIN] Epoch:[44/100] Loss:[0.0951=0.0001+0.0587+0.0728] Train:[97.98] val:[95.62] Test:[95.00] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000594
BIAS:[0.70] | Model:[CausalGIN] Epoch:[45/100] Loss:[0.0745=0.0000+0.0452+0.0585] Train:[98.53] val:[94.75] Test:[92.38] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000579
BIAS:[0.70] | Model:[CausalGIN] Epoch:[46/100] Loss:[0.0651=0.0000+0.0397+0.0508] Train:[98.59] val:[94.00] Test:[95.06] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000563
BIAS:[0.70] | Model:[CausalGIN] Epoch:[47/100] Loss:[0.0502=0.0000+0.0304+0.0394] Train:[99.16] val:[96.75] Test:[96.62] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000548
BIAS:[0.70] | Model:[CausalGIN] Epoch:[48/100] Loss:[0.0600=0.0000+0.0368+0.0465] Train:[98.71] val:[96.38] Test:[95.69] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000532
BIAS:[0.70] | Model:[CausalGIN] Epoch:[49/100] Loss:[0.0582=0.0000+0.0346+0.0472] Train:[99.09] val:[96.50] Test:[95.75] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000516
BIAS:[0.70] | Model:[CausalGIN] Epoch:[50/100] Loss:[0.0669=0.0000+0.0409+0.0521] Train:[98.77] val:[96.00] Test:[95.56] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000501
BIAS:[0.70] | Model:[CausalGIN] Epoch:[51/100] Loss:[0.0529=0.0000+0.0314+0.0430] Train:[99.02] val:[93.88] Test:[94.19] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000485
BIAS:[0.70] | Model:[CausalGIN] Epoch:[52/100] Loss:[0.0537=0.0000+0.0316+0.0443] Train:[99.09] val:[96.62] Test:[96.38] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000469
BIAS:[0.70] | Model:[CausalGIN] Epoch:[53/100] Loss:[0.0424=0.0000+0.0249+0.0351] Train:[99.20] val:[94.88] Test:[95.31] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000453
BIAS:[0.70] | Model:[CausalGIN] Epoch:[54/100] Loss:[0.0438=0.0000+0.0260+0.0356] Train:[99.12] val:[96.75] Test:[95.69] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000438
BIAS:[0.70] | Model:[CausalGIN] Epoch:[55/100] Loss:[0.0415=0.0000+0.0240+0.0350] Train:[99.16] val:[96.00] Test:[95.44] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000422
BIAS:[0.70] | Model:[CausalGIN] Epoch:[56/100] Loss:[0.0452=0.0000+0.0259+0.0386] Train:[99.20] val:[94.50] Test:[95.06] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000407
BIAS:[0.70] | Model:[CausalGIN] Epoch:[57/100] Loss:[0.0599=0.0000+0.0353+0.0492] Train:[98.80] val:[92.88] Test:[93.44] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000392
BIAS:[0.70] | Model:[CausalGIN] Epoch:[58/100] Loss:[0.0682=0.0000+0.0411+0.0543] Train:[98.53] val:[96.62] Test:[96.19] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000376
BIAS:[0.70] | Model:[CausalGIN] Epoch:[59/100] Loss:[0.0363=0.0000+0.0216+0.0295] Train:[99.41] val:[94.50] Test:[95.38] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000361
BIAS:[0.70] | Model:[CausalGIN] Epoch:[60/100] Loss:[0.0253=0.0000+0.0146+0.0213] Train:[99.66] val:[96.50] Test:[96.44] | Update Test:[co:96.19,c:24.12,o:96.50] at Epoch:[32] | lr:0.000346
BIAS:[0.70] | Model:[CausalGIN] Epoch:[61/100] Loss:[0.0211=0.0000+0.0118+0.0186] Train:[99.75] val:[97.12] Test:[96.44] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000331
BIAS:[0.70] | Model:[CausalGIN] Epoch:[62/100] Loss:[0.0193=0.0000+0.0107+0.0172] Train:[99.84] val:[95.88] Test:[96.12] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000317
BIAS:[0.70] | Model:[CausalGIN] Epoch:[63/100] Loss:[0.0500=0.0000+0.0297+0.0407] Train:[99.02] val:[95.12] Test:[95.31] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000302
BIAS:[0.70] | Model:[CausalGIN] Epoch:[64/100] Loss:[0.0282=0.0000+0.0163+0.0237] Train:[99.54] val:[96.38] Test:[96.25] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000288
BIAS:[0.70] | Model:[CausalGIN] Epoch:[65/100] Loss:[0.0191=0.0000+0.0107+0.0168] Train:[99.75] val:[96.88] Test:[96.44] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000274
BIAS:[0.70] | Model:[CausalGIN] Epoch:[66/100] Loss:[0.0142=0.0000+0.0076+0.0132] Train:[99.87] val:[96.38] Test:[96.44] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000260
BIAS:[0.70] | Model:[CausalGIN] Epoch:[67/100] Loss:[0.0142=0.0000+0.0078+0.0128] Train:[99.84] val:[96.25] Test:[96.19] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000246
BIAS:[0.70] | Model:[CausalGIN] Epoch:[68/100] Loss:[0.0126=0.0000+0.0071+0.0110] Train:[99.87] val:[96.50] Test:[96.69] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000233
BIAS:[0.70] | Model:[CausalGIN] Epoch:[69/100] Loss:[0.0119=0.0000+0.0062+0.0114] Train:[99.96] val:[96.62] Test:[96.50] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000220
BIAS:[0.70] | Model:[CausalGIN] Epoch:[70/100] Loss:[0.0144=0.0000+0.0080+0.0129] Train:[99.89] val:[96.50] Test:[96.50] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000207
BIAS:[0.70] | Model:[CausalGIN] Epoch:[71/100] Loss:[0.0088=0.0000+0.0047+0.0081] Train:[99.95] val:[96.75] Test:[96.56] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000194
BIAS:[0.70] | Model:[CausalGIN] Epoch:[72/100] Loss:[0.0086=0.0000+0.0050+0.0073] Train:[99.93] val:[96.50] Test:[96.31] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000182
BIAS:[0.70] | Model:[CausalGIN] Epoch:[73/100] Loss:[0.0071=0.0000+0.0040+0.0062] Train:[99.96] val:[96.75] Test:[96.75] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000170
BIAS:[0.70] | Model:[CausalGIN] Epoch:[74/100] Loss:[0.0078=0.0000+0.0043+0.0070] Train:[99.95] val:[96.38] Test:[96.38] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000159
BIAS:[0.70] | Model:[CausalGIN] Epoch:[75/100] Loss:[0.0077=0.0000+0.0042+0.0069] Train:[99.95] val:[96.50] Test:[96.69] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000147
BIAS:[0.70] | Model:[CausalGIN] Epoch:[76/100] Loss:[0.0082=0.0000+0.0043+0.0076] Train:[99.96] val:[96.38] Test:[96.81] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000136
BIAS:[0.70] | Model:[CausalGIN] Epoch:[77/100] Loss:[0.0084=0.0000+0.0045+0.0078] Train:[99.96] val:[96.25] Test:[96.56] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000126
BIAS:[0.70] | Model:[CausalGIN] Epoch:[78/100] Loss:[0.0087=0.0000+0.0047+0.0079] Train:[99.95] val:[96.62] Test:[96.25] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000116
BIAS:[0.70] | Model:[CausalGIN] Epoch:[79/100] Loss:[0.0064=0.0000+0.0036+0.0056] Train:[99.98] val:[96.38] Test:[96.75] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000106
BIAS:[0.70] | Model:[CausalGIN] Epoch:[80/100] Loss:[0.0057=0.0000+0.0031+0.0052] Train:[99.98] val:[96.62] Test:[96.69] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000096
BIAS:[0.70] | Model:[CausalGIN] Epoch:[81/100] Loss:[0.0051=0.0000+0.0029+0.0045] Train:[99.98] val:[96.38] Test:[96.50] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000087
BIAS:[0.70] | Model:[CausalGIN] Epoch:[82/100] Loss:[0.0051=0.0000+0.0028+0.0046] Train:[99.98] val:[96.62] Test:[96.56] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000079
BIAS:[0.70] | Model:[CausalGIN] Epoch:[83/100] Loss:[0.0062=0.0000+0.0036+0.0052] Train:[99.96] val:[96.62] Test:[96.62] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000071
BIAS:[0.70] | Model:[CausalGIN] Epoch:[84/100] Loss:[0.0063=0.0000+0.0035+0.0055] Train:[99.98] val:[96.38] Test:[96.81] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000063
BIAS:[0.70] | Model:[CausalGIN] Epoch:[85/100] Loss:[0.0050=0.0000+0.0029+0.0043] Train:[99.96] val:[96.38] Test:[96.44] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000055
BIAS:[0.70] | Model:[CausalGIN] Epoch:[86/100] Loss:[0.0059=0.0000+0.0032+0.0053] Train:[99.98] val:[96.50] Test:[96.81] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000049
BIAS:[0.70] | Model:[CausalGIN] Epoch:[87/100] Loss:[0.0051=0.0000+0.0030+0.0043] Train:[99.96] val:[96.38] Test:[96.62] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000042
BIAS:[0.70] | Model:[CausalGIN] Epoch:[88/100] Loss:[0.0057=0.0000+0.0033+0.0047] Train:[99.98] val:[96.38] Test:[96.62] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000036
BIAS:[0.70] | Model:[CausalGIN] Epoch:[89/100] Loss:[0.0058=0.0000+0.0034+0.0048] Train:[99.96] val:[96.50] Test:[96.75] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000031
BIAS:[0.70] | Model:[CausalGIN] Epoch:[90/100] Loss:[0.0054=0.0000+0.0027+0.0053] Train:[99.98] val:[96.62] Test:[96.44] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000025
BIAS:[0.70] | Model:[CausalGIN] Epoch:[91/100] Loss:[0.0062=0.0000+0.0035+0.0054] Train:[99.96] val:[96.62] Test:[96.56] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000021
BIAS:[0.70] | Model:[CausalGIN] Epoch:[92/100] Loss:[0.0058=0.0000+0.0032+0.0052] Train:[99.96] val:[96.62] Test:[96.44] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000017
BIAS:[0.70] | Model:[CausalGIN] Epoch:[93/100] Loss:[0.0049=0.0000+0.0027+0.0045] Train:[99.98] val:[96.25] Test:[96.69] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000013
BIAS:[0.70] | Model:[CausalGIN] Epoch:[94/100] Loss:[0.0050=0.0000+0.0029+0.0043] Train:[99.96] val:[96.62] Test:[96.69] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000010
BIAS:[0.70] | Model:[CausalGIN] Epoch:[95/100] Loss:[0.0043=0.0000+0.0024+0.0038] Train:[99.98] val:[96.50] Test:[96.75] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000007
BIAS:[0.70] | Model:[CausalGIN] Epoch:[96/100] Loss:[0.0050=0.0000+0.0028+0.0044] Train:[99.98] val:[96.62] Test:[96.75] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000005
BIAS:[0.70] | Model:[CausalGIN] Epoch:[97/100] Loss:[0.0055=0.0000+0.0031+0.0047] Train:[99.96] val:[96.38] Test:[96.56] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000003
BIAS:[0.70] | Model:[CausalGIN] Epoch:[98/100] Loss:[0.0056=0.0000+0.0031+0.0050] Train:[99.98] val:[96.50] Test:[96.62] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000002
BIAS:[0.70] | Model:[CausalGIN] Epoch:[99/100] Loss:[0.0049=0.0000+0.0027+0.0043] Train:[99.96] val:[96.50] Test:[96.56] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000001
BIAS:[0.70] | Model:[CausalGIN] Epoch:[100/100] Loss:[0.0039=0.0000+0.0021+0.0035] Train:[99.98] val:[96.50] Test:[96.62] | Update Test:[co:96.50,c:26.44,o:96.44] at Epoch:[61] | lr:0.000001
syd: BIAS:[0.70] | Val acc:[96.50] Test acc:[co:96.50,c:26.44,o:96.44] at epoch:[61]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.9
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model........................................................................GIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:1260 , Cycle:139  , Grids:139  , Diams:139   
| BA  : House:139  , Cycle:1260 , Grids:1260 , Diams:1260  
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:90.1%, Cycle:9.9%, Grids:9.9%, Diams:9.9%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:796
| Tree: House:180  , Cycle:19   , Grids:19   , Diams:19    
| BA  : House:19   , Cycle:180  , Grids:180  , Diams:180   
| All : House:199  , Cycle:199  , Grids:199  , Diams:199   
| BIAS: House:90.5%, Cycle:9.5%, Grids:9.5%, Diams:9.5%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.90] | Model:[GIN] Epoch:[1/100] Loss:[0.9356] Train:[61.45] val:[61.93] Test:[44.75] | Best Val:[61.93] Update Test:[44.75] at Epoch:[1] | lr:0.001000
BIAS:[0.90] | Model:[GIN] Epoch:[2/100] Loss:[0.5692] Train:[81.11] val:[83.79] Test:[67.75] | Best Val:[83.79] Update Test:[67.75] at Epoch:[2] | lr:0.000999
BIAS:[0.90] | Model:[GIN] Epoch:[3/100] Loss:[0.4943] Train:[82.95] val:[78.39] Test:[58.38] | Best Val:[83.79] Update Test:[67.75] at Epoch:[2] | lr:0.000998
BIAS:[0.90] | Model:[GIN] Epoch:[4/100] Loss:[0.3741] Train:[86.99] val:[85.55] Test:[67.50] | Best Val:[85.55] Update Test:[67.50] at Epoch:[4] | lr:0.000996
BIAS:[0.90] | Model:[GIN] Epoch:[5/100] Loss:[0.3106] Train:[89.12] val:[89.45] Test:[74.12] | Best Val:[89.45] Update Test:[74.12] at Epoch:[5] | lr:0.000994
BIAS:[0.90] | Model:[GIN] Epoch:[6/100] Loss:[0.2858] Train:[90.80] val:[87.56] Test:[79.81] | Best Val:[89.45] Update Test:[74.12] at Epoch:[5] | lr:0.000991
BIAS:[0.90] | Model:[GIN] Epoch:[7/100] Loss:[0.2375] Train:[91.87] val:[87.56] Test:[80.00] | Best Val:[89.45] Update Test:[74.12] at Epoch:[5] | lr:0.000988
BIAS:[0.90] | Model:[GIN] Epoch:[8/100] Loss:[0.2287] Train:[91.82] val:[90.20] Test:[81.00] | Best Val:[90.20] Update Test:[81.00] at Epoch:[8] | lr:0.000984
BIAS:[0.90] | Model:[GIN] Epoch:[9/100] Loss:[0.2463] Train:[91.14] val:[85.43] Test:[77.44] | Best Val:[90.20] Update Test:[81.00] at Epoch:[8] | lr:0.000980
BIAS:[0.90] | Model:[GIN] Epoch:[10/100] Loss:[0.2274] Train:[91.76] val:[82.91] Test:[74.19] | Best Val:[90.20] Update Test:[81.00] at Epoch:[8] | lr:0.000976
BIAS:[0.90] | Model:[GIN] Epoch:[11/100] Loss:[0.1936] Train:[93.32] val:[91.71] Test:[83.44] | Best Val:[91.71] Update Test:[83.44] at Epoch:[11] | lr:0.000970
BIAS:[0.90] | Model:[GIN] Epoch:[12/100] Loss:[0.2185] Train:[92.03] val:[79.40] Test:[76.69] | Best Val:[91.71] Update Test:[83.44] at Epoch:[11] | lr:0.000965
BIAS:[0.90] | Model:[GIN] Epoch:[13/100] Loss:[0.1873] Train:[93.41] val:[92.21] Test:[88.25] | Best Val:[92.21] Update Test:[88.25] at Epoch:[13] | lr:0.000959
BIAS:[0.90] | Model:[GIN] Epoch:[14/100] Loss:[0.1821] Train:[93.87] val:[85.68] Test:[75.50] | Best Val:[92.21] Update Test:[88.25] at Epoch:[13] | lr:0.000952
BIAS:[0.90] | Model:[GIN] Epoch:[15/100] Loss:[0.1867] Train:[93.30] val:[90.95] Test:[86.44] | Best Val:[92.21] Update Test:[88.25] at Epoch:[13] | lr:0.000946
BIAS:[0.90] | Model:[GIN] Epoch:[16/100] Loss:[0.1730] Train:[93.91] val:[93.34] Test:[86.94] | Best Val:[93.34] Update Test:[86.94] at Epoch:[16] | lr:0.000938
BIAS:[0.90] | Model:[GIN] Epoch:[17/100] Loss:[0.1668] Train:[94.42] val:[84.92] Test:[83.19] | Best Val:[93.34] Update Test:[86.94] at Epoch:[16] | lr:0.000930
BIAS:[0.90] | Model:[GIN] Epoch:[18/100] Loss:[0.1849] Train:[93.71] val:[90.83] Test:[87.56] | Best Val:[93.34] Update Test:[86.94] at Epoch:[16] | lr:0.000922
BIAS:[0.90] | Model:[GIN] Epoch:[19/100] Loss:[0.1579] Train:[94.73] val:[91.46] Test:[85.44] | Best Val:[93.34] Update Test:[86.94] at Epoch:[16] | lr:0.000914
BIAS:[0.90] | Model:[GIN] Epoch:[20/100] Loss:[0.1537] Train:[94.91] val:[91.58] Test:[84.50] | Best Val:[93.34] Update Test:[86.94] at Epoch:[16] | lr:0.000905
BIAS:[0.90] | Model:[GIN] Epoch:[21/100] Loss:[0.1540] Train:[95.05] val:[93.09] Test:[89.06] | Best Val:[93.34] Update Test:[86.94] at Epoch:[16] | lr:0.000895
BIAS:[0.90] | Model:[GIN] Epoch:[22/100] Loss:[0.1531] Train:[94.92] val:[92.59] Test:[82.38] | Best Val:[93.34] Update Test:[86.94] at Epoch:[16] | lr:0.000885
BIAS:[0.90] | Model:[GIN] Epoch:[23/100] Loss:[0.1510] Train:[94.53] val:[91.21] Test:[85.50] | Best Val:[93.34] Update Test:[86.94] at Epoch:[16] | lr:0.000875
BIAS:[0.90] | Model:[GIN] Epoch:[24/100] Loss:[0.1301] Train:[95.82] val:[93.97] Test:[90.50] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000865
BIAS:[0.90] | Model:[GIN] Epoch:[25/100] Loss:[0.1244] Train:[96.03] val:[88.19] Test:[81.56] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000854
BIAS:[0.90] | Model:[GIN] Epoch:[26/100] Loss:[0.1237] Train:[95.82] val:[92.71] Test:[89.25] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000842
BIAS:[0.90] | Model:[GIN] Epoch:[27/100] Loss:[0.1274] Train:[95.96] val:[88.82] Test:[81.75] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000831
BIAS:[0.90] | Model:[GIN] Epoch:[28/100] Loss:[0.1240] Train:[96.02] val:[92.59] Test:[88.62] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000819
BIAS:[0.90] | Model:[GIN] Epoch:[29/100] Loss:[0.1300] Train:[95.43] val:[92.71] Test:[87.44] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000807
BIAS:[0.90] | Model:[GIN] Epoch:[30/100] Loss:[0.1091] Train:[96.59] val:[90.95] Test:[87.75] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000794
BIAS:[0.90] | Model:[GIN] Epoch:[31/100] Loss:[0.1094] Train:[96.43] val:[92.96] Test:[91.94] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000781
BIAS:[0.90] | Model:[GIN] Epoch:[32/100] Loss:[0.1000] Train:[96.80] val:[86.18] Test:[86.31] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000768
BIAS:[0.90] | Model:[GIN] Epoch:[33/100] Loss:[0.1052] Train:[96.77] val:[93.47] Test:[90.19] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000755
BIAS:[0.90] | Model:[GIN] Epoch:[34/100] Loss:[0.1011] Train:[96.91] val:[93.22] Test:[91.12] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000741
BIAS:[0.90] | Model:[GIN] Epoch:[35/100] Loss:[0.1043] Train:[96.50] val:[92.34] Test:[88.94] | Best Val:[93.97] Update Test:[90.50] at Epoch:[24] | lr:0.000727
BIAS:[0.90] | Model:[GIN] Epoch:[36/100] Loss:[0.0948] Train:[96.82] val:[94.22] Test:[89.38] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000713
BIAS:[0.90] | Model:[GIN] Epoch:[37/100] Loss:[0.1061] Train:[96.28] val:[86.43] Test:[81.12] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000699
BIAS:[0.90] | Model:[GIN] Epoch:[38/100] Loss:[0.0919] Train:[97.02] val:[92.34] Test:[90.81] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000684
BIAS:[0.90] | Model:[GIN] Epoch:[39/100] Loss:[0.0739] Train:[97.55] val:[91.08] Test:[92.19] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000670
BIAS:[0.90] | Model:[GIN] Epoch:[40/100] Loss:[0.0737] Train:[97.64] val:[93.84] Test:[90.69] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000655
BIAS:[0.90] | Model:[GIN] Epoch:[41/100] Loss:[0.0749] Train:[97.34] val:[93.59] Test:[88.75] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000640
BIAS:[0.90] | Model:[GIN] Epoch:[42/100] Loss:[0.0943] Train:[96.93] val:[91.21] Test:[87.00] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000625
BIAS:[0.90] | Model:[GIN] Epoch:[43/100] Loss:[0.0701] Train:[97.91] val:[92.59] Test:[89.38] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000609
BIAS:[0.90] | Model:[GIN] Epoch:[44/100] Loss:[0.0842] Train:[97.16] val:[94.22] Test:[89.94] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000594
BIAS:[0.90] | Model:[GIN] Epoch:[45/100] Loss:[0.0633] Train:[97.93] val:[92.09] Test:[92.50] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000579
BIAS:[0.90] | Model:[GIN] Epoch:[46/100] Loss:[0.0555] Train:[98.30] val:[93.09] Test:[89.56] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000563
BIAS:[0.90] | Model:[GIN] Epoch:[47/100] Loss:[0.0540] Train:[98.23] val:[91.83] Test:[91.75] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000548
BIAS:[0.90] | Model:[GIN] Epoch:[48/100] Loss:[0.0441] Train:[98.75] val:[93.59] Test:[90.44] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000532
BIAS:[0.90] | Model:[GIN] Epoch:[49/100] Loss:[0.0511] Train:[98.27] val:[91.58] Test:[85.75] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000516
BIAS:[0.90] | Model:[GIN] Epoch:[50/100] Loss:[0.0580] Train:[98.23] val:[93.34] Test:[92.94] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000501
BIAS:[0.90] | Model:[GIN] Epoch:[51/100] Loss:[0.0397] Train:[98.73] val:[88.07] Test:[86.31] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000485
BIAS:[0.90] | Model:[GIN] Epoch:[52/100] Loss:[0.0424] Train:[98.53] val:[91.46] Test:[92.12] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000469
BIAS:[0.90] | Model:[GIN] Epoch:[53/100] Loss:[0.0445] Train:[98.52] val:[92.59] Test:[92.00] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000453
BIAS:[0.90] | Model:[GIN] Epoch:[54/100] Loss:[0.0334] Train:[99.09] val:[93.84] Test:[90.56] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000438
BIAS:[0.90] | Model:[GIN] Epoch:[55/100] Loss:[0.0362] Train:[98.78] val:[93.09] Test:[92.31] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000422
BIAS:[0.90] | Model:[GIN] Epoch:[56/100] Loss:[0.0256] Train:[99.21] val:[92.21] Test:[91.69] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000407
BIAS:[0.90] | Model:[GIN] Epoch:[57/100] Loss:[0.0202] Train:[99.45] val:[92.09] Test:[91.56] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000392
BIAS:[0.90] | Model:[GIN] Epoch:[58/100] Loss:[0.0204] Train:[99.45] val:[91.71] Test:[89.25] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000376
BIAS:[0.90] | Model:[GIN] Epoch:[59/100] Loss:[0.0307] Train:[99.14] val:[92.96] Test:[90.62] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000361
BIAS:[0.90] | Model:[GIN] Epoch:[60/100] Loss:[0.0302] Train:[99.07] val:[93.09] Test:[90.88] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000346
BIAS:[0.90] | Model:[GIN] Epoch:[61/100] Loss:[0.0186] Train:[99.34] val:[92.96] Test:[92.06] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000331
BIAS:[0.90] | Model:[GIN] Epoch:[62/100] Loss:[0.0157] Train:[99.59] val:[92.21] Test:[92.56] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000317
BIAS:[0.90] | Model:[GIN] Epoch:[63/100] Loss:[0.0115] Train:[99.77] val:[92.59] Test:[90.50] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000302
BIAS:[0.90] | Model:[GIN] Epoch:[64/100] Loss:[0.0114] Train:[99.75] val:[92.96] Test:[92.62] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000288
BIAS:[0.90] | Model:[GIN] Epoch:[65/100] Loss:[0.0102] Train:[99.73] val:[92.59] Test:[90.50] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000274
BIAS:[0.90] | Model:[GIN] Epoch:[66/100] Loss:[0.0097] Train:[99.75] val:[93.72] Test:[91.69] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000260
BIAS:[0.90] | Model:[GIN] Epoch:[67/100] Loss:[0.0106] Train:[99.70] val:[92.71] Test:[92.12] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000246
BIAS:[0.90] | Model:[GIN] Epoch:[68/100] Loss:[0.0113] Train:[99.68] val:[93.34] Test:[91.62] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000233
BIAS:[0.90] | Model:[GIN] Epoch:[69/100] Loss:[0.0070] Train:[99.95] val:[93.34] Test:[91.06] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000220
BIAS:[0.90] | Model:[GIN] Epoch:[70/100] Loss:[0.0060] Train:[99.93] val:[93.84] Test:[91.06] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000207
BIAS:[0.90] | Model:[GIN] Epoch:[71/100] Loss:[0.0057] Train:[99.96] val:[93.34] Test:[91.75] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000194
BIAS:[0.90] | Model:[GIN] Epoch:[72/100] Loss:[0.0045] Train:[99.96] val:[93.09] Test:[92.19] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000182
BIAS:[0.90] | Model:[GIN] Epoch:[73/100] Loss:[0.0052] Train:[99.95] val:[92.96] Test:[91.88] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000170
BIAS:[0.90] | Model:[GIN] Epoch:[74/100] Loss:[0.0046] Train:[99.98] val:[93.22] Test:[92.00] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000159
BIAS:[0.90] | Model:[GIN] Epoch:[75/100] Loss:[0.0039] Train:[99.96] val:[93.34] Test:[92.56] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000147
BIAS:[0.90] | Model:[GIN] Epoch:[76/100] Loss:[0.0041] Train:[99.96] val:[93.59] Test:[91.94] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000136
BIAS:[0.90] | Model:[GIN] Epoch:[77/100] Loss:[0.0051] Train:[99.96] val:[93.59] Test:[91.12] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000126
BIAS:[0.90] | Model:[GIN] Epoch:[78/100] Loss:[0.0029] Train:[100.00] val:[93.34] Test:[92.31] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000116
BIAS:[0.90] | Model:[GIN] Epoch:[79/100] Loss:[0.0041] Train:[99.98] val:[93.09] Test:[92.44] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000106
BIAS:[0.90] | Model:[GIN] Epoch:[80/100] Loss:[0.0032] Train:[99.98] val:[93.34] Test:[92.31] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000096
BIAS:[0.90] | Model:[GIN] Epoch:[81/100] Loss:[0.0031] Train:[100.00] val:[94.10] Test:[91.94] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000087
BIAS:[0.90] | Model:[GIN] Epoch:[82/100] Loss:[0.0030] Train:[100.00] val:[93.59] Test:[92.25] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000079
BIAS:[0.90] | Model:[GIN] Epoch:[83/100] Loss:[0.0030] Train:[100.00] val:[93.22] Test:[92.25] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000071
BIAS:[0.90] | Model:[GIN] Epoch:[84/100] Loss:[0.0036] Train:[99.98] val:[93.59] Test:[91.69] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000063
BIAS:[0.90] | Model:[GIN] Epoch:[85/100] Loss:[0.0023] Train:[100.00] val:[93.34] Test:[92.19] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000055
BIAS:[0.90] | Model:[GIN] Epoch:[86/100] Loss:[0.0026] Train:[100.00] val:[93.47] Test:[92.38] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000049
BIAS:[0.90] | Model:[GIN] Epoch:[87/100] Loss:[0.0032] Train:[99.96] val:[93.22] Test:[92.25] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000042
BIAS:[0.90] | Model:[GIN] Epoch:[88/100] Loss:[0.0018] Train:[100.00] val:[93.09] Test:[92.00] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000036
BIAS:[0.90] | Model:[GIN] Epoch:[89/100] Loss:[0.0027] Train:[99.98] val:[93.22] Test:[92.38] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000031
BIAS:[0.90] | Model:[GIN] Epoch:[90/100] Loss:[0.0034] Train:[99.96] val:[93.59] Test:[92.25] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000025
BIAS:[0.90] | Model:[GIN] Epoch:[91/100] Loss:[0.0024] Train:[99.98] val:[93.09] Test:[91.88] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000021
BIAS:[0.90] | Model:[GIN] Epoch:[92/100] Loss:[0.0024] Train:[100.00] val:[92.96] Test:[91.94] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000017
BIAS:[0.90] | Model:[GIN] Epoch:[93/100] Loss:[0.0031] Train:[100.00] val:[93.34] Test:[92.50] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000013
BIAS:[0.90] | Model:[GIN] Epoch:[94/100] Loss:[0.0021] Train:[100.00] val:[93.09] Test:[92.31] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000010
BIAS:[0.90] | Model:[GIN] Epoch:[95/100] Loss:[0.0028] Train:[100.00] val:[93.22] Test:[92.12] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000007
BIAS:[0.90] | Model:[GIN] Epoch:[96/100] Loss:[0.0024] Train:[99.98] val:[93.34] Test:[92.12] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000005
BIAS:[0.90] | Model:[GIN] Epoch:[97/100] Loss:[0.0024] Train:[100.00] val:[93.34] Test:[91.94] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000003
BIAS:[0.90] | Model:[GIN] Epoch:[98/100] Loss:[0.0020] Train:[100.00] val:[93.22] Test:[92.25] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000002
BIAS:[0.90] | Model:[GIN] Epoch:[99/100] Loss:[0.0023] Train:[100.00] val:[93.59] Test:[92.38] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000001
BIAS:[0.90] | Model:[GIN] Epoch:[100/100] Loss:[0.0022] Train:[100.00] val:[93.72] Test:[92.12] | Best Val:[94.22] Update Test:[89.38] at Epoch:[36] | lr:0.000001
syd: BIAS:[0.90] | Best Val acc:[94.22] Test acc:[89.38] at epoch:[36]
step_size..................................................................0.001
min_lr.....................................................................1e-06
pretrain......................................................................30
data_num....................................................................2000
node_num......................................................................15
max_degree....................................................................10
feature_dim...................................................................-1
noise........................................................................0.1
num_classes....................................................................4
shape_num......................................................................1
bias.........................................................................0.9
penalty_weight...............................................................0.1
train_type..................................................................base
epochs.......................................................................100
batch_size...................................................................128
the............................................................................0
with_random.................................................................True
eval_random................................................................False
normalize..................................................................False
save_model.................................................................False
inference..................................................................False
without_node_attention.....................................................False
without_edge_attention.....................................................False
k..............................................................................3
layers.........................................................................3
c............................................................................0.5
o............................................................................1.0
co...........................................................................0.5
harf_hidden..................................................................0.5
cat_or_add...................................................................add
num_layers.....................................................................3
folds.........................................................................10
fc_num.......................................................................222
data_root...................................................................data
save_dir...................................................................debug
dataset.....................................................................NCI1
epoch_select............................................................test_max
model..................................................................CausalGIN
hidden.......................................................................128
seed.........................................................................555
lr.........................................................................0.001
lr_decay_factor..............................................................0.5
lr_decay_step_size...........................................................500
weight_decay...................................................................0
global_pool..................................................................sum

----------------------------------------------------------------------------------------------------
| graph: Tree-house | nodes num:246 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-house | nodes num:230 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-cycle | nodes num:247 | edges num:542 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-cycle | nodes num:231 | edges num:996 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-grid | nodes num:247 | edges num:544 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-grid | nodes num:231 | edges num:998 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: Tree-diamond | nodes num:247 | edges num:546 |
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
| graph: BA-diamond | nodes num:231 | edges num:1000 |
----------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Train Total:5596
| Tree: House:1260 , Cycle:139  , Grids:139  , Diams:139   
| BA  : House:139  , Cycle:1260 , Grids:1260 , Diams:1260  
| All : House:1399 , Cycle:1399 , Grids:1399 , Diams:1399  
| BIAS: House:90.1%, Cycle:9.9%, Grids:9.9%, Diams:9.9%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Val    Total:796
| Tree: House:180  , Cycle:19   , Grids:19   , Diams:19    
| BA  : House:19   , Cycle:180  , Grids:180  , Diams:180   
| All : House:199  , Cycle:199  , Grids:199  , Diams:199   
| BIAS: House:90.5%, Cycle:9.5%, Grids:9.5%, Diams:9.5%
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Test   Total:1600
| Tree: House:200  , Cycle:200  , Grids:200  , Diams:200   
| BA  : House:200  , Cycle:200  , Grids:200  , Diams:200   
| All : House:400  , Cycle:400  , Grids:400  , Diams:400   
| BIAS: House:50.0%, Cycle:50.0%, Grids:50.0%, Diams:50.0%
------------------------------------------------------------------------------------------------------------------------------------------------------
BIAS:[0.90] | Model:[CausalGIN] Epoch:[1/100] Loss:[1.4361=0.0235+0.9107+1.0274] Train:[61.87] val:[47.24] Test:[27.25] | Update Test:[co:25.69,c:25.31,o:27.25] at Epoch:[1] | lr:0.001000
BIAS:[0.90] | Model:[CausalGIN] Epoch:[2/100] Loss:[0.7364=0.0038+0.4394+0.5904] Train:[84.81] val:[70.48] Test:[68.44] | Update Test:[co:54.06,c:24.62,o:68.44] at Epoch:[2] | lr:0.000999
BIAS:[0.90] | Model:[CausalGIN] Epoch:[3/100] Loss:[0.5378=0.0016+0.3355+0.4030] Train:[88.42] val:[87.56] Test:[72.31] | Update Test:[co:71.62,c:26.69,o:72.31] at Epoch:[3] | lr:0.000998
BIAS:[0.90] | Model:[CausalGIN] Epoch:[4/100] Loss:[0.4532=0.0011+0.2886+0.3282] Train:[89.97] val:[90.45] Test:[76.94] | Update Test:[co:73.25,c:24.44,o:76.94] at Epoch:[4] | lr:0.000996
BIAS:[0.90] | Model:[CausalGIN] Epoch:[5/100] Loss:[0.4084=0.0007+0.2620+0.2922] Train:[90.99] val:[87.44] Test:[78.62] | Update Test:[co:73.25,c:24.44,o:76.94] at Epoch:[4] | lr:0.000994
BIAS:[0.90] | Model:[CausalGIN] Epoch:[6/100] Loss:[0.4029=0.0006+0.2572+0.2908] Train:[91.26] val:[84.67] Test:[76.31] | Update Test:[co:73.25,c:24.44,o:76.94] at Epoch:[4] | lr:0.000991
BIAS:[0.90] | Model:[CausalGIN] Epoch:[7/100] Loss:[0.3624=0.0006+0.2329+0.2585] Train:[91.90] val:[90.70] Test:[80.81] | Update Test:[co:79.56,c:26.38,o:80.81] at Epoch:[7] | lr:0.000988
BIAS:[0.90] | Model:[CausalGIN] Epoch:[8/100] Loss:[0.3583=0.0004+0.2276+0.2610] Train:[91.96] val:[88.94] Test:[81.44] | Update Test:[co:79.56,c:26.38,o:80.81] at Epoch:[7] | lr:0.000984
BIAS:[0.90] | Model:[CausalGIN] Epoch:[9/100] Loss:[0.3289=0.0004+0.2096+0.2382] Train:[92.76] val:[84.17] Test:[76.50] | Update Test:[co:79.56,c:26.38,o:80.81] at Epoch:[7] | lr:0.000980
BIAS:[0.90] | Model:[CausalGIN] Epoch:[10/100] Loss:[0.3212=0.0003+0.2059+0.2305] Train:[92.69] val:[89.57] Test:[86.88] | Update Test:[co:79.56,c:26.38,o:80.81] at Epoch:[7] | lr:0.000976
BIAS:[0.90] | Model:[CausalGIN] Epoch:[11/100] Loss:[0.3273=0.0003+0.2117+0.2311] Train:[92.53] val:[75.63] Test:[76.38] | Update Test:[co:79.56,c:26.38,o:80.81] at Epoch:[7] | lr:0.000970
BIAS:[0.90] | Model:[CausalGIN] Epoch:[12/100] Loss:[0.2836=0.0002+0.1820+0.2030] Train:[93.51] val:[92.09] Test:[82.31] | Update Test:[co:78.38,c:26.19,o:82.31] at Epoch:[12] | lr:0.000965
BIAS:[0.90] | Model:[CausalGIN] Epoch:[13/100] Loss:[0.2725=0.0003+0.1753+0.1940] Train:[93.98] val:[86.56] Test:[84.69] | Update Test:[co:78.38,c:26.19,o:82.31] at Epoch:[12] | lr:0.000959
BIAS:[0.90] | Model:[CausalGIN] Epoch:[14/100] Loss:[0.2722=0.0002+0.1752+0.1938] Train:[94.09] val:[88.44] Test:[83.06] | Update Test:[co:78.38,c:26.19,o:82.31] at Epoch:[12] | lr:0.000952
BIAS:[0.90] | Model:[CausalGIN] Epoch:[15/100] Loss:[0.2444=0.0001+0.1570+0.1746] Train:[94.64] val:[93.34] Test:[87.00] | Update Test:[co:85.06,c:24.31,o:87.00] at Epoch:[15] | lr:0.000946
BIAS:[0.90] | Model:[CausalGIN] Epoch:[16/100] Loss:[0.2388=0.0001+0.1539+0.1696] Train:[94.80] val:[87.81] Test:[86.38] | Update Test:[co:85.06,c:24.31,o:87.00] at Epoch:[15] | lr:0.000938
BIAS:[0.90] | Model:[CausalGIN] Epoch:[17/100] Loss:[0.2301=0.0001+0.1466+0.1670] Train:[95.09] val:[93.34] Test:[83.50] | Update Test:[co:85.06,c:24.31,o:87.00] at Epoch:[15] | lr:0.000930
BIAS:[0.90] | Model:[CausalGIN] Epoch:[18/100] Loss:[0.2039=0.0001+0.1295+0.1488] Train:[95.57] val:[94.47] Test:[90.19] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000922
BIAS:[0.90] | Model:[CausalGIN] Epoch:[19/100] Loss:[0.2264=0.0001+0.1430+0.1666] Train:[95.37] val:[90.58] Test:[80.69] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000914
BIAS:[0.90] | Model:[CausalGIN] Epoch:[20/100] Loss:[0.2012=0.0002+0.1268+0.1485] Train:[95.89] val:[83.04] Test:[82.44] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000905
BIAS:[0.90] | Model:[CausalGIN] Epoch:[21/100] Loss:[0.1966=0.0001+0.1231+0.1468] Train:[95.84] val:[93.47] Test:[88.56] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000895
BIAS:[0.90] | Model:[CausalGIN] Epoch:[22/100] Loss:[0.1921=0.0001+0.1224+0.1395] Train:[96.02] val:[93.72] Test:[91.88] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000885
BIAS:[0.90] | Model:[CausalGIN] Epoch:[23/100] Loss:[0.1776=0.0002+0.1122+0.1306] Train:[96.03] val:[84.05] Test:[74.38] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000875
BIAS:[0.90] | Model:[CausalGIN] Epoch:[24/100] Loss:[0.1932=0.0001+0.1210+0.1443] Train:[96.03] val:[93.59] Test:[90.69] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000865
BIAS:[0.90] | Model:[CausalGIN] Epoch:[25/100] Loss:[0.1618=0.0001+0.1012+0.1212] Train:[96.52] val:[93.22] Test:[93.25] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000854
BIAS:[0.90] | Model:[CausalGIN] Epoch:[26/100] Loss:[0.1625=0.0001+0.1028+0.1193] Train:[96.75] val:[93.22] Test:[87.50] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000842
BIAS:[0.90] | Model:[CausalGIN] Epoch:[27/100] Loss:[0.1564=0.0001+0.0983+0.1162] Train:[96.64] val:[78.77] Test:[70.06] | Update Test:[co:89.75,c:25.44,o:90.19] at Epoch:[18] | lr:0.000831
BIAS:[0.90] | Model:[CausalGIN] Epoch:[28/100] Loss:[0.1490=0.0001+0.0930+0.1117] Train:[96.80] val:[94.60] Test:[88.06] | Update Test:[co:87.12,c:25.12,o:88.06] at Epoch:[28] | lr:0.000819
BIAS:[0.90] | Model:[CausalGIN] Epoch:[29/100] Loss:[0.1457=0.0001+0.0914+0.1085] Train:[96.98] val:[92.09] Test:[90.56] | Update Test:[co:87.12,c:25.12,o:88.06] at Epoch:[28] | lr:0.000807
BIAS:[0.90] | Model:[CausalGIN] Epoch:[30/100] Loss:[0.1548=0.0001+0.0980+0.1136] Train:[96.60] val:[92.21] Test:[89.38] | Update Test:[co:87.12,c:25.12,o:88.06] at Epoch:[28] | lr:0.000794
BIAS:[0.90] | Model:[CausalGIN] Epoch:[31/100] Loss:[0.1673=0.0001+0.1058+0.1228] Train:[96.62] val:[84.92] Test:[82.81] | Update Test:[co:87.12,c:25.12,o:88.06] at Epoch:[28] | lr:0.000781
BIAS:[0.90] | Model:[CausalGIN] Epoch:[32/100] Loss:[0.1143=0.0001+0.0711+0.0863] Train:[97.82] val:[91.33] Test:[81.44] | Update Test:[co:87.12,c:25.12,o:88.06] at Epoch:[28] | lr:0.000768
BIAS:[0.90] | Model:[CausalGIN] Epoch:[33/100] Loss:[0.1094=0.0001+0.0686+0.0816] Train:[97.89] val:[94.35] Test:[91.56] | Update Test:[co:87.12,c:25.12,o:88.06] at Epoch:[28] | lr:0.000755
BIAS:[0.90] | Model:[CausalGIN] Epoch:[34/100] Loss:[0.1144=0.0001+0.0712+0.0863] Train:[98.03] val:[87.06] Test:[90.62] | Update Test:[co:87.12,c:25.12,o:88.06] at Epoch:[28] | lr:0.000741
BIAS:[0.90] | Model:[CausalGIN] Epoch:[35/100] Loss:[0.1280=0.0001+0.0809+0.0943] Train:[97.39] val:[88.44] Test:[72.62] | Update Test:[co:87.12,c:25.12,o:88.06] at Epoch:[28] | lr:0.000727
BIAS:[0.90] | Model:[CausalGIN] Epoch:[36/100] Loss:[0.1035=0.0001+0.0651+0.0768] Train:[97.91] val:[95.10] Test:[90.50] | Update Test:[co:92.62,c:24.31,o:90.50] at Epoch:[36] | lr:0.000713
BIAS:[0.90] | Model:[CausalGIN] Epoch:[37/100] Loss:[0.1054=0.0001+0.0651+0.0806] Train:[97.84] val:[92.71] Test:[85.94] | Update Test:[co:92.62,c:24.31,o:90.50] at Epoch:[36] | lr:0.000699
BIAS:[0.90] | Model:[CausalGIN] Epoch:[38/100] Loss:[0.0828=0.0001+0.0505+0.0645] Train:[98.46] val:[95.48] Test:[93.75] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000684
BIAS:[0.90] | Model:[CausalGIN] Epoch:[39/100] Loss:[0.0901=0.0001+0.0567+0.0667] Train:[97.94] val:[88.44] Test:[81.56] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000670
BIAS:[0.90] | Model:[CausalGIN] Epoch:[40/100] Loss:[0.0857=0.0001+0.0519+0.0675] Train:[98.28] val:[91.46] Test:[84.44] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000655
BIAS:[0.90] | Model:[CausalGIN] Epoch:[41/100] Loss:[0.0752=0.0001+0.0459+0.0584] Train:[98.37] val:[93.72] Test:[88.69] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000640
BIAS:[0.90] | Model:[CausalGIN] Epoch:[42/100] Loss:[0.0692=0.0001+0.0421+0.0542] Train:[98.73] val:[92.96] Test:[86.69] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000625
BIAS:[0.90] | Model:[CausalGIN] Epoch:[43/100] Loss:[0.0659=0.0001+0.0391+0.0536] Train:[98.78] val:[92.09] Test:[87.81] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000609
BIAS:[0.90] | Model:[CausalGIN] Epoch:[44/100] Loss:[0.0614=0.0001+0.0372+0.0482] Train:[98.82] val:[94.85] Test:[91.25] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000594
BIAS:[0.90] | Model:[CausalGIN] Epoch:[45/100] Loss:[0.0496=0.0000+0.0292+0.0408] Train:[99.14] val:[92.09] Test:[89.00] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000579
BIAS:[0.90] | Model:[CausalGIN] Epoch:[46/100] Loss:[0.0501=0.0000+0.0293+0.0416] Train:[99.11] val:[94.85] Test:[94.88] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000563
BIAS:[0.90] | Model:[CausalGIN] Epoch:[47/100] Loss:[0.0448=0.0000+0.0267+0.0360] Train:[99.16] val:[93.97] Test:[92.94] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000548
BIAS:[0.90] | Model:[CausalGIN] Epoch:[48/100] Loss:[0.0424=0.0000+0.0257+0.0334] Train:[99.23] val:[94.85] Test:[94.19] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000532
BIAS:[0.90] | Model:[CausalGIN] Epoch:[49/100] Loss:[0.0340=0.0000+0.0196+0.0289] Train:[99.43] val:[92.46] Test:[88.38] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000516
BIAS:[0.90] | Model:[CausalGIN] Epoch:[50/100] Loss:[0.0406=0.0001+0.0240+0.0332] Train:[99.25] val:[93.34] Test:[92.81] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000501
BIAS:[0.90] | Model:[CausalGIN] Epoch:[51/100] Loss:[0.0396=0.0001+0.0229+0.0333] Train:[99.25] val:[94.60] Test:[91.88] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000485
BIAS:[0.90] | Model:[CausalGIN] Epoch:[52/100] Loss:[0.0357=0.0001+0.0216+0.0283] Train:[99.29] val:[94.47] Test:[94.88] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000469
BIAS:[0.90] | Model:[CausalGIN] Epoch:[53/100] Loss:[0.0301=0.0000+0.0181+0.0241] Train:[99.54] val:[94.97] Test:[93.38] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000453
BIAS:[0.90] | Model:[CausalGIN] Epoch:[54/100] Loss:[0.0323=0.0000+0.0198+0.0250] Train:[99.36] val:[94.60] Test:[93.75] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000438
BIAS:[0.90] | Model:[CausalGIN] Epoch:[55/100] Loss:[0.0385=0.0001+0.0233+0.0304] Train:[99.23] val:[93.22] Test:[93.62] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000422
BIAS:[0.90] | Model:[CausalGIN] Epoch:[56/100] Loss:[0.0244=0.0000+0.0141+0.0207] Train:[99.66] val:[94.22] Test:[95.38] | Update Test:[co:94.06,c:22.19,o:93.75] at Epoch:[38] | lr:0.000407
BIAS:[0.90] | Model:[CausalGIN] Epoch:[57/100] Loss:[0.0193=0.0001+0.0104+0.0177] Train:[99.82] val:[96.11] Test:[94.12] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000392
BIAS:[0.90] | Model:[CausalGIN] Epoch:[58/100] Loss:[0.0163=0.0000+0.0089+0.0147] Train:[99.80] val:[94.97] Test:[95.75] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000376
BIAS:[0.90] | Model:[CausalGIN] Epoch:[59/100] Loss:[0.0226=0.0000+0.0135+0.0183] Train:[99.70] val:[95.73] Test:[94.25] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000361
BIAS:[0.90] | Model:[CausalGIN] Epoch:[60/100] Loss:[0.0149=0.0000+0.0082+0.0135] Train:[99.80] val:[95.60] Test:[92.94] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000346
BIAS:[0.90] | Model:[CausalGIN] Epoch:[61/100] Loss:[0.0102=0.0000+0.0055+0.0092] Train:[99.91] val:[95.60] Test:[94.94] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000331
BIAS:[0.90] | Model:[CausalGIN] Epoch:[62/100] Loss:[0.0130=0.0000+0.0073+0.0115] Train:[99.82] val:[94.97] Test:[94.50] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000317
BIAS:[0.90] | Model:[CausalGIN] Epoch:[63/100] Loss:[0.0136=0.0000+0.0071+0.0130] Train:[99.82] val:[96.11] Test:[92.88] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000302
BIAS:[0.90] | Model:[CausalGIN] Epoch:[64/100] Loss:[0.0079=0.0000+0.0044+0.0069] Train:[99.91] val:[95.23] Test:[94.56] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000288
BIAS:[0.90] | Model:[CausalGIN] Epoch:[65/100] Loss:[0.0056=0.0000+0.0030+0.0053] Train:[99.98] val:[95.98] Test:[95.25] | Update Test:[co:93.94,c:25.69,o:94.12] at Epoch:[57] | lr:0.000274
BIAS:[0.90] | Model:[CausalGIN] Epoch:[66/100] Loss:[0.0050=0.0000+0.0027+0.0045] Train:[99.96] val:[96.61] Test:[94.19] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000260
BIAS:[0.90] | Model:[CausalGIN] Epoch:[67/100] Loss:[0.0042=0.0000+0.0023+0.0037] Train:[99.96] val:[95.48] Test:[95.19] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000246
BIAS:[0.90] | Model:[CausalGIN] Epoch:[68/100] Loss:[0.0036=0.0000+0.0018+0.0035] Train:[100.00] val:[95.73] Test:[94.88] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000233
BIAS:[0.90] | Model:[CausalGIN] Epoch:[69/100] Loss:[0.0059=0.0000+0.0032+0.0054] Train:[99.95] val:[95.10] Test:[95.81] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000220
BIAS:[0.90] | Model:[CausalGIN] Epoch:[70/100] Loss:[0.0063=0.0000+0.0035+0.0055] Train:[99.95] val:[96.23] Test:[94.88] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000207
BIAS:[0.90] | Model:[CausalGIN] Epoch:[71/100] Loss:[0.0046=0.0000+0.0025+0.0042] Train:[99.98] val:[96.48] Test:[94.81] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000194
BIAS:[0.90] | Model:[CausalGIN] Epoch:[72/100] Loss:[0.0042=0.0000+0.0022+0.0039] Train:[99.96] val:[96.23] Test:[94.94] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000182
BIAS:[0.90] | Model:[CausalGIN] Epoch:[73/100] Loss:[0.0026=0.0000+0.0013+0.0026] Train:[100.00] val:[95.73] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000170
BIAS:[0.90] | Model:[CausalGIN] Epoch:[74/100] Loss:[0.0033=0.0000+0.0018+0.0030] Train:[99.98] val:[95.98] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000159
BIAS:[0.90] | Model:[CausalGIN] Epoch:[75/100] Loss:[0.0038=0.0000+0.0022+0.0032] Train:[99.98] val:[95.98] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000147
BIAS:[0.90] | Model:[CausalGIN] Epoch:[76/100] Loss:[0.0029=0.0000+0.0015+0.0026] Train:[100.00] val:[95.73] Test:[95.25] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000136
BIAS:[0.90] | Model:[CausalGIN] Epoch:[77/100] Loss:[0.0027=0.0000+0.0015+0.0023] Train:[100.00] val:[95.98] Test:[95.12] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000126
BIAS:[0.90] | Model:[CausalGIN] Epoch:[78/100] Loss:[0.0031=0.0000+0.0017+0.0028] Train:[99.98] val:[95.85] Test:[94.62] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000116
BIAS:[0.90] | Model:[CausalGIN] Epoch:[79/100] Loss:[0.0026=0.0000+0.0013+0.0026] Train:[100.00] val:[95.85] Test:[94.88] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000106
BIAS:[0.90] | Model:[CausalGIN] Epoch:[80/100] Loss:[0.0023=0.0000+0.0013+0.0021] Train:[100.00] val:[95.98] Test:[95.19] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000096
BIAS:[0.90] | Model:[CausalGIN] Epoch:[81/100] Loss:[0.0026=0.0000+0.0014+0.0022] Train:[99.98] val:[95.85] Test:[95.00] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000087
BIAS:[0.90] | Model:[CausalGIN] Epoch:[82/100] Loss:[0.0020=0.0000+0.0011+0.0017] Train:[100.00] val:[96.11] Test:[95.19] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000079
BIAS:[0.90] | Model:[CausalGIN] Epoch:[83/100] Loss:[0.0045=0.0000+0.0024+0.0042] Train:[99.95] val:[96.36] Test:[94.88] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000071
BIAS:[0.90] | Model:[CausalGIN] Epoch:[84/100] Loss:[0.0025=0.0000+0.0014+0.0023] Train:[100.00] val:[96.11] Test:[94.94] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000063
BIAS:[0.90] | Model:[CausalGIN] Epoch:[85/100] Loss:[0.0019=0.0000+0.0011+0.0017] Train:[100.00] val:[96.23] Test:[94.94] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000055
BIAS:[0.90] | Model:[CausalGIN] Epoch:[86/100] Loss:[0.0023=0.0000+0.0013+0.0020] Train:[100.00] val:[95.85] Test:[95.19] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000049
BIAS:[0.90] | Model:[CausalGIN] Epoch:[87/100] Loss:[0.0020=0.0000+0.0011+0.0018] Train:[100.00] val:[95.85] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000042
BIAS:[0.90] | Model:[CausalGIN] Epoch:[88/100] Loss:[0.0022=0.0000+0.0012+0.0020] Train:[100.00] val:[95.98] Test:[95.25] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000036
BIAS:[0.90] | Model:[CausalGIN] Epoch:[89/100] Loss:[0.0023=0.0000+0.0013+0.0021] Train:[100.00] val:[95.98] Test:[94.88] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000031
BIAS:[0.90] | Model:[CausalGIN] Epoch:[90/100] Loss:[0.0028=0.0000+0.0015+0.0025] Train:[99.98] val:[96.11] Test:[94.69] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000025
BIAS:[0.90] | Model:[CausalGIN] Epoch:[91/100] Loss:[0.0017=0.0000+0.0010+0.0015] Train:[100.00] val:[96.23] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000021
BIAS:[0.90] | Model:[CausalGIN] Epoch:[92/100] Loss:[0.0019=0.0000+0.0011+0.0016] Train:[100.00] val:[96.23] Test:[95.00] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000017
BIAS:[0.90] | Model:[CausalGIN] Epoch:[93/100] Loss:[0.0022=0.0000+0.0013+0.0018] Train:[100.00] val:[96.11] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000013
BIAS:[0.90] | Model:[CausalGIN] Epoch:[94/100] Loss:[0.0019=0.0000+0.0010+0.0017] Train:[100.00] val:[96.23] Test:[95.12] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000010
BIAS:[0.90] | Model:[CausalGIN] Epoch:[95/100] Loss:[0.0018=0.0000+0.0010+0.0017] Train:[100.00] val:[95.73] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000007
BIAS:[0.90] | Model:[CausalGIN] Epoch:[96/100] Loss:[0.0018=0.0000+0.0010+0.0016] Train:[100.00] val:[96.36] Test:[95.19] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000005
BIAS:[0.90] | Model:[CausalGIN] Epoch:[97/100] Loss:[0.0027=0.0000+0.0015+0.0024] Train:[99.98] val:[95.85] Test:[95.25] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000003
BIAS:[0.90] | Model:[CausalGIN] Epoch:[98/100] Loss:[0.0021=0.0000+0.0011+0.0019] Train:[100.00] val:[95.73] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000002
BIAS:[0.90] | Model:[CausalGIN] Epoch:[99/100] Loss:[0.0025=0.0000+0.0015+0.0020] Train:[99.96] val:[95.85] Test:[95.00] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000001
BIAS:[0.90] | Model:[CausalGIN] Epoch:[100/100] Loss:[0.0019=0.0000+0.0011+0.0014] Train:[99.98] val:[95.60] Test:[95.06] | Update Test:[co:94.00,c:24.19,o:94.19] at Epoch:[66] | lr:0.000001
syd: BIAS:[0.90] | Val acc:[95.60] Test acc:[co:94.00,c:24.19,o:94.19] at epoch:[66]
